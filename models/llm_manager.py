"""Flexible LLM manager supporting multiple backends."""

import os
import logging
from typing import Optional, Dict, Any
from abc import ABC, abstractmethod
from config.settings import settings

logger = logging.getLogger(__name__)

class LLMBackend(ABC):
    """Abstract base class for LLM backends."""
    
    @abstractmethod
    def load_model(self) -> bool:
        pass
    
    @abstractmethod
    def generate(self, prompt: str, max_tokens: int = 512, **kwargs) -> str:
        pass
    
    @abstractmethod
    def unload_model(self):
        pass
    
    @abstractmethod
    def is_model_loaded(self) -> bool:
        pass

class MockBackend(LLMBackend):
    """Mock backend for testing without actual models."""
    
    def __init__(self):
        self._loaded = True
        self.response_templates = {
            "azure virtual network": "Azure Virtual Network (VNet) is a representation of your own network in the cloud. It is a logical isolation of the Azure cloud dedicated to your subscription. You can fully control the IP address ranges, DNS settings, security policies, and route tables within this network.",
            "vpn gateway": "Azure VPN Gateway is a specific type of virtual network gateway that is used to send encrypted traffic between an Azure virtual network and an on-premises location over the public Internet. It provides secure cross-premises connectivity.",
            "load balancer": "Azure Load Balancer operates at layer 4 (TCP, UDP) and provides high availability and network performance to your applications. It distributes incoming traffic among healthy virtual machine instances in a load balancer set.",
            "expressroute": "Azure ExpressRoute lets you extend your on-premises networks into the Microsoft cloud over a private connection facilitated by a connectivity provider. With ExpressRoute, you can establish connections to Microsoft cloud services."
        }
    
    def load_model(self) -> bool:
        logger.info("✅ Mock LLM backend loaded")
        return True
    
    def generate(self, prompt: str, max_tokens: int = 512, **kwargs) -> str:
        # Simple keyword matching for realistic responses
        prompt_lower = prompt.lower()
        
        for keyword, response in self.response_templates.items():
            if keyword in prompt_lower:
                return f"{response}\n\nThis is a mock response demonstrating the system functionality. In production, this would be generated by a real LLM based on your Azure documentation."
        
        return f"This is a mock response to your query about Azure networking. The system successfully retrieved relevant context and would normally generate a detailed answer using a real language model. Your query was: '{prompt[:100]}...'"
    
    def unload_model(self):
        pass
    
    def is_model_loaded(self) -> bool:
        return self._loaded

class OllamaBackend(LLMBackend):
    """Ollama backend for LLM inference."""
    
    def __init__(self, model_name: str = None, base_url: str = None):
        self.model_name = model_name or settings.OLLAMA_MODEL
        self.base_url = base_url or settings.OLLAMA_URL
        self._loaded = False
    
    def load_model(self) -> bool:
        try:
            import requests
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                if any(model["name"] == self.model_name for model in models):
                    self._loaded = True
                    logger.info(f"✅ Ollama model {self.model_name} available")
                    return True
                else:
                    logger.error(f"Model {self.model_name} not found in Ollama")
                    return False
        except Exception as e:
            logger.error(f"Ollama not available: {e}")
            return False
    
    def generate(self, prompt: str, max_tokens: int = 512, **kwargs) -> str:
        try:
            import requests
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "num_predict": max_tokens,
                        "temperature": kwargs.get("temperature", 0.1)
                    }
                },
                timeout=30
            )
            
            if response.status_code == 200:
                return response.json()["response"]
            else:
                return f"Error: {response.status_code}"
                
        except Exception as e:
            return f"Generation error: {e}"
    
    def unload_model(self):
        self._loaded = False
    
    def is_model_loaded(self) -> bool:
        return self._loaded

class LlamaCppBackend(LLMBackend):
    """llama-cpp-python backend."""
    
    def __init__(self, model_path: str = None):
        self.model_path = model_path or settings.LLAMA_MODEL_PATH
        self.llm = None
        self._loaded = False
    
    def load_model(self) -> bool:
        try:
            from llama_cpp import Llama
            
            if not os.path.exists(self.model_path):
                logger.error(f"Model file not found: {self.model_path}")
                return False
            
            self.llm = Llama(
                model_path=self.model_path,
                n_ctx=settings.LLAMA_CONTEXT_SIZE,
                n_threads=settings.LLAMA_THREADS,
                verbose=False
            )
            self._loaded = True
            logger.info("✅ llama-cpp-python model loaded")
            return True
            
        except ImportError:
            logger.error("llama-cpp-python not installed")
            return False
        except Exception as e:
            logger.error(f"Failed to load model: {e}")
            return False
    
    def generate(self, prompt: str, max_tokens: int = 512, **kwargs) -> str:
        if not self._loaded:
            return "Error: Model not loaded"
        
        try:
            response = self.llm(
                prompt,
                max_tokens=max_tokens,
                temperature=kwargs.get("temperature", 0.1),
                stop=["</s>", "Human:", "Assistant:", "\n\n"]
            )
            return response['choices'][0]['text'].strip()
        except Exception as e:
            return f"Generation error: {e}"
    
    def unload_model(self):
        if self.llm:
            del self.llm
            self.llm = None
            self._loaded = False
    
    def is_model_loaded(self) -> bool:
        return self._loaded

class FlexibleLLMManager:
    """LLM manager that can use different backends."""
    
    def __init__(self):
        self.backend: Optional[LLMBackend] = None
        self.backend_type = None
        self.setup_backend(settings.LLM_BACKEND)
    
    def setup_backend(self, backend_type: str = "auto", **kwargs) -> bool:
        """Setup the LLM backend."""
        
        if backend_type == "auto":
            # Try backends in order of preference
            for bt in ["ollama", "llama-cpp", "mock"]:
                if self.setup_backend(bt, **kwargs):
                    return True
            return False
        
        elif backend_type == "ollama":
            self.backend = OllamaBackend()
            
        elif backend_type == "llama-cpp":
            self.backend = LlamaCppBackend()
            
        elif backend_type == "mock":
            self.backend = MockBackend()
            
        else:
            logger.error(f"Unknown backend type: {backend_type}")
            return False
        
        success = self.backend.load_model()
        if success:
            self.backend_type = backend_type
            logger.info(f"✅ Using {backend_type} backend")
        else:
            # Fallback to mock if preferred backend fails
            if backend_type != "mock":
                logger.warning(f"Falling back to mock backend")
                return self.setup_backend("mock")
        
        return success
    
    def generate(self, prompt: str, max_tokens: int = 512, **kwargs) -> str:
        """Generate response using current backend."""
        if not self.backend:
            return "Error: No backend loaded"
        
        return self.backend.generate(prompt, max_tokens, **kwargs)
    
    def is_model_loaded(self) -> bool:
        """Check if backend is loaded."""
        return self.backend and self.backend.is_model_loaded()
    
    def get_backend_info(self) -> Dict[str, Any]:
        """Get information about current backend."""
        return {
            "backend_type": self.backend_type,
            "is_loaded": self.is_model_loaded(),
            "available_backends": ["mock", "ollama", "llama-cpp"]
        }
    
    def switch_backend(self, backend_type: str) -> bool:
        """Switch to a different backend."""
        if self.backend:
            self.backend.unload_model()
        
        return self.setup_backend(backend_type)

# Global instance
llm_manager = FlexibleLLMManager()
