{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ff2c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "0a00e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "cfaab6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Azure RAG Project: ~/azure-multimodal-rag\n",
      "üìç Current Working Directory: azure-multimodal-rag\n",
      "üéØ Azure RAG Project: ~/projects/azure-multimodal-rag\n",
      "üìç Working in: azure-multimodal-rag\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Get your home directory automatically\n",
    "HOME = Path.home()\n",
    "project_name = \"azure-multimodal-rag\"\n",
    "AZURE_RAG_PROJECT = HOME / \"projects\" / project_name\n",
    "\n",
    "print(f\"üéØ Azure RAG Project: ~/{project_name}\")\n",
    "print(f\"üìç Current Working Directory: {Path.cwd().name}\")\n",
    "\n",
    "# Create and navigate\n",
    "AZURE_RAG_PROJECT.mkdir(parents=True, exist_ok=True)\n",
    "os.chdir(AZURE_RAG_PROJECT)\n",
    "\n",
    "# Safe display\n",
    "print(f\"üéØ Azure RAG Project: ~/projects/azure-multimodal-rag\")\n",
    "print(f\"üìç Working in: {AZURE_RAG_PROJECT.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "74a1e384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Azure RAG Project: ~/projects/azure-multimodal-rag\n",
      "üìç Working directory: azure-multimodal-rag\n",
      "üìÅ Creating directory structure...\n",
      "   üìÅ config\n",
      "   üìÅ models\n",
      "   üìÅ data/raw/pdfs\n",
      "   üìÅ data/processed\n",
      "   üìÅ data/vector_store\n",
      "   üìÅ src/document_processor\n",
      "   üìÅ src/vector_store\n",
      "   üìÅ src/retrieval\n",
      "   üìÅ src/generation\n",
      "   üìÅ src/utils\n",
      "   üìÅ logs\n",
      "   üìÅ notebooks\n",
      "   üìÅ scripts\n",
      "   üìÅ tests\n",
      "‚úÖ Project structure created\n",
      "üìç All files created in: ~/azure-multimodal-rag/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up logging\n",
    "# Create logs directory\n",
    "Path(\"logs\").mkdir(exist_ok=True)\n",
    "\n",
    "# Configure to save to file\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('logs/app.log'),  # Save to file\n",
    "        logging.StreamHandler()               # Also show on screen\n",
    "    ]\n",
    ")\n",
    "\n",
    "# SECURE Project configuration - no personal paths exposed\n",
    "HOME = Path.home()  # Gets your home directory automatically\n",
    "PROJECT_NAME = \"azure-multimodal-rag\"\n",
    "AZURE_RAG_PROJECT = HOME / \"projects\" / PROJECT_NAME\n",
    "\n",
    "# Safe display function\n",
    "def safe_display(path):\n",
    "    \"\"\"Display path without exposing personal directory\"\"\"\n",
    "    return str(path).replace(str(HOME), \"~\")\n",
    "\n",
    "print(f\"üéØ Azure RAG Project: {safe_display(AZURE_RAG_PROJECT)}\")\n",
    "\n",
    "# Create project directory and add to path\n",
    "AZURE_RAG_PROJECT.mkdir(parents=True, exist_ok=True)\n",
    "if str(AZURE_RAG_PROJECT) not in sys.path:\n",
    "    sys.path.append(str(AZURE_RAG_PROJECT))\n",
    "\n",
    "# Change to project directory for file creation\n",
    "os.chdir(AZURE_RAG_PROJECT)\n",
    "print(f\"üìç Working directory: {AZURE_RAG_PROJECT.name}\")\n",
    "\n",
    "# Create complete directory structure\n",
    "directories = [\n",
    "    \"config\", \"models\", \"data/raw/pdfs\", \"data/processed\", \"data/vector_store\",\n",
    "    \"src/document_processor\", \"src/vector_store\", \"src/retrieval\", \n",
    "    \"src/generation\", \"src/utils\", \"logs\", \"notebooks\", \"scripts\", \"tests\"\n",
    "]\n",
    "\n",
    "print(\"üìÅ Creating directory structure...\")\n",
    "for directory in directories:\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"   üìÅ {directory}\")\n",
    "\n",
    "# Create __init__.py files\n",
    "init_files = [\n",
    "    \"src/__init__.py\", \"src/document_processor/__init__.py\", \n",
    "    \"src/vector_store/__init__.py\", \"src/retrieval/__init__.py\",\n",
    "    \"src/generation/__init__.py\", \"src/utils/__init__.py\", \"config/__init__.py\"\n",
    "]\n",
    "\n",
    "for init_file in init_files:\n",
    "    Path(init_file).touch()\n",
    "\n",
    "print(\"‚úÖ Project structure created\")\n",
    "print(f\"üìç All files created in: ~/{PROJECT_NAME}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af75c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "526e4c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Creating Configuration System...\n",
      "‚úÖ Configuration System Created!\n",
      "\n",
      "üìã Current Settings:\n",
      "   üìÅ PDF Folder: data/raw/pdfs\n",
      "   üìÅ Processed Folder: data/processed\n",
      "   üìÅ Vector Store: data/vector_store\n",
      "   üìù Chunk Size: 1000 characters\n",
      "   üîÑ Chunk Overlap: 200 characters (20% overlap)\n",
      "   ü§ñ Embedding Model: all-MiniLM-L6-v2\n",
      "   üíæ Max Memory: 8 GB\n",
      "   üìä Max Files: 10\n",
      "   üìÑ Supported Types: .pdf, .txt, .md\n",
      "\n",
      "üíæ Configuration saved to: config/settings.py\n"
     ]
    }
   ],
   "source": [
    "# Cell: Configuration System\n",
    "print(\"‚öôÔ∏è Creating Configuration System...\")\n",
    "\n",
    "class AzureRAGConfig:\n",
    "    \"\"\"Configuration for our Azure RAG system\"\"\"\n",
    "    \n",
    "    # File paths\n",
    "    PDF_FOLDER = \"data/raw/pdfs\"\n",
    "    PROCESSED_FOLDER = \"data/processed\"\n",
    "    VECTOR_STORE_FOLDER = \"data/vector_store\"\n",
    "    LOGS_FOLDER = \"logs\"\n",
    "    \n",
    "    # Text processing settings\n",
    "    CHUNK_SIZE = 1000        # characters - good balance\n",
    "    CHUNK_OVERLAP = 200      # 20% overlap for context\n",
    "    \n",
    "    # AI model settings\n",
    "    EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"  # For creating embeddings\n",
    "    MAX_MEMORY_GB = 8        # Memory limit\n",
    "    \n",
    "    # Processing limits\n",
    "    MAX_FILES_TO_PROCESS = 10  # Don't overwhelm the system\n",
    "    \n",
    "    # Supported file types\n",
    "    SUPPORTED_FILE_TYPES = [\".pdf\", \".txt\", \".md\"]\n",
    "\n",
    "# Create our configuration instance\n",
    "config = AzureRAGConfig()\n",
    "\n",
    "# Test and display our configuration\n",
    "print(\"‚úÖ Configuration System Created!\")\n",
    "print(\"\\nüìã Current Settings:\")\n",
    "print(f\"   üìÅ PDF Folder: {config.PDF_FOLDER}\")\n",
    "print(f\"   üìÅ Processed Folder: {config.PROCESSED_FOLDER}\")\n",
    "print(f\"   üìÅ Vector Store: {config.VECTOR_STORE_FOLDER}\")\n",
    "print(f\"   üìù Chunk Size: {config.CHUNK_SIZE} characters\")\n",
    "print(f\"   üîÑ Chunk Overlap: {config.CHUNK_OVERLAP} characters ({config.CHUNK_OVERLAP/config.CHUNK_SIZE*100:.0f}% overlap)\")\n",
    "print(f\"   ü§ñ Embedding Model: {config.EMBEDDING_MODEL}\")\n",
    "print(f\"   üíæ Max Memory: {config.MAX_MEMORY_GB} GB\")\n",
    "print(f\"   üìä Max Files: {config.MAX_FILES_TO_PROCESS}\")\n",
    "print(f\"   üìÑ Supported Types: {', '.join(config.SUPPORTED_FILE_TYPES)}\")\n",
    "\n",
    "# Save configuration to file for later use\n",
    "config_file_content = f'''\"\"\"Azure RAG Configuration Settings\"\"\"\n",
    "\n",
    "class AzureRAGConfig:\n",
    "    \"\"\"Configuration for our Azure RAG system\"\"\"\n",
    "    \n",
    "    # File paths\n",
    "    PDF_FOLDER = \"{config.PDF_FOLDER}\"\n",
    "    PROCESSED_FOLDER = \"{config.PROCESSED_FOLDER}\"\n",
    "    VECTOR_STORE_FOLDER = \"{config.VECTOR_STORE_FOLDER}\"\n",
    "    LOGS_FOLDER = \"{config.LOGS_FOLDER}\"\n",
    "    \n",
    "    # Text processing settings\n",
    "    CHUNK_SIZE = {config.CHUNK_SIZE}\n",
    "    CHUNK_OVERLAP = {config.CHUNK_OVERLAP}\n",
    "    \n",
    "    # AI model settings\n",
    "    EMBEDDING_MODEL = \"{config.EMBEDDING_MODEL}\"\n",
    "    MAX_MEMORY_GB = {config.MAX_MEMORY_GB}\n",
    "    \n",
    "    # Processing limits\n",
    "    MAX_FILES_TO_PROCESS = {config.MAX_FILES_TO_PROCESS}\n",
    "    \n",
    "    # Supported file types\n",
    "    SUPPORTED_FILE_TYPES = {config.SUPPORTED_FILE_TYPES}\n",
    "\n",
    "# Global config instance\n",
    "config = AzureRAGConfig()\n",
    "'''\n",
    "\n",
    "# Write to config file\n",
    "with open('config/settings.py', 'w') as f:\n",
    "    f.write(config_file_content)\n",
    "\n",
    "print(f\"\\nüíæ Configuration saved to: config/settings.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b168109a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24088732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5f9b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "b3e3ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's create a corrected version that works with our config\n",
    "class SimplePDFReader:\n",
    "    \"\"\"Simple PDF reader using PyMuPDF\"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        \"\"\"Initialize with optional config\"\"\"\n",
    "        self.config = config\n",
    "        \n",
    "    def read_pdf(self, pdf_path):\n",
    "        \"\"\"Read a PDF file and return text content\"\"\"\n",
    "        try:\n",
    "            import fitz  # PyMuPDF\n",
    "            \n",
    "            # Open the PDF\n",
    "            doc = fitz.open(pdf_path)\n",
    "            text_content = \"\"\n",
    "            \n",
    "            print(f\"üìÑ Processing {len(doc)} pages...\")\n",
    "            \n",
    "            # Extract text from each page\n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc[page_num]\n",
    "                page_text = page.get_text()\n",
    "                text_content += page_text\n",
    "                \n",
    "                # Show progress for larger documents\n",
    "                if (page_num + 1) % 5 == 0 or page_num == 0:\n",
    "                    print(f\"   üìñ Processed page {page_num + 1}/{len(doc)}\")\n",
    "            \n",
    "            doc.close()\n",
    "            return text_content\n",
    "            \n",
    "        except ImportError:\n",
    "            raise Exception(\"PyMuPDF (fitz) not installed. Run: pip install PyMuPDF\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error reading PDF {pdf_path}: {str(e)}\")\n",
    "\n",
    " \n",
    "       \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7154a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "3f567f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Found 2 PDF files:\n",
      "   1. 01-study-guide-az-vnet.pdf\n",
      "   2. 02-study-guide-az-load-balancer.pdf\n",
      "\n",
      "üîç Testing with: 01-study-guide-az-vnet.pdf\n",
      "üìñ Reading 01-study-guide-az-vnet.pdf...\n",
      "üìÑ Processing 11 pages...\n",
      "   üìñ Processed page 1/11\n",
      "   üìñ Processed page 5/11\n",
      "   üìñ Processed page 10/11\n",
      "\n",
      "‚úÖ Successfully read PDF!\n",
      "üìä Content Analysis:\n",
      "   üìù Total characters: 25,974\n",
      "   üìÑ Total words: 3,986\n",
      "   üìã Total lines: 518\n",
      "\n",
      "üìã First 5 meaningful lines:\n",
      "   1. Tuesday, November 2, 2021\n",
      "   2. 1\n",
      "   3. Capabilities of Azure Virtual Networks\n",
      "   4. Azure VNets enable resources in Azure to securely communicate with each other, the internet,\n",
      "   5. and on-premises networks.\n",
      "\n",
      "üìÑ Sample chunk (first 1000 characters):\n",
      "'Tuesday, November 2, 2021 \n",
      "1 \n",
      " \n",
      " \n",
      "Capabilities of Azure Virtual Networks \n",
      "Azure VNets enable resources in Azure to securely communicate with each other, the internet, \n",
      "and on-premises networks. \n",
      " \n",
      "Communication with the internet. All resources in a VNet can communicate outbound to the \n",
      "internet, by default. You can communicate inbound to a resource by assigning a public IP \n",
      "address or a public Load Balancer. You can also use public IP or public Load Balancer to manage \n",
      "your outbound connections. \n",
      "Communication between Azure resources. There are three key mechanisms through which \n",
      "Azure resource can communicate: VNets, VNet service endpoints, and VNet peering. Virtual \n",
      "Networks can connect not only virtual machines (VMs), but other Azure Resources, such as the \n",
      "App Service Environment, Azure Kubernetes Service, and Azure Virtual Machine Scale Sets. You \n",
      "can use service endpoints to connect to other Azure resource types, such as Azure SQL \n",
      "databases and storage accounts. When you create...'\n",
      "\n",
      "üßÆ Chunking Analysis:\n",
      "   üì¶ Estimated chunks: 32\n",
      "   üìè Chunk size: 1,000 characters\n",
      "   üîÑ Overlap: 200 characters (20.0%)\n",
      "   üíæ Memory per chunk: ~3.9 KB\n",
      "\n",
      "‚úÖ Content looks good - contains Azure networking terms!\n",
      "\n",
      "üéØ Next step: Text Chunking System!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pdf_files = list(Path(config.PDF_FOLDER).glob(\"*.pdf\"))\n",
    "print(f\"üìÑ Found {len(pdf_files)} PDF files:\")\n",
    "\n",
    "for i, pdf_file in enumerate(pdf_files, 1):\n",
    "    print(f\"   {i}. {pdf_file.name}\")\n",
    "\n",
    "if pdf_files:\n",
    "    # Test with the first PDF\n",
    "    test_pdf = pdf_files[0]\n",
    "    print(f\"\\nüîç Testing with: {test_pdf.name}\")\n",
    "    \n",
    "    try:\n",
    "        # Create PDF reader instance with config\n",
    "        pdf_reader = SimplePDFReader(config=config)\n",
    "        \n",
    "        # Read the PDF\n",
    "        print(f\"üìñ Reading {test_pdf.name}...\")\n",
    "        text_content = pdf_reader.read_pdf(str(test_pdf))\n",
    "        \n",
    "        # Analyze what we got\n",
    "        print(f\"\\n‚úÖ Successfully read PDF!\")\n",
    "        print(f\"üìä Content Analysis:\")\n",
    "        print(f\"   üìù Total characters: {len(text_content):,}\")\n",
    "        print(f\"   üìÑ Total words: {len(text_content.split()):,}\")  \n",
    "        print(f\"   üìã Total lines: {len(text_content.splitlines()):,}\")\n",
    "        \n",
    "        # Clean and show first few meaningful lines\n",
    "        lines = [line.strip() for line in text_content.splitlines() if line.strip()]\n",
    "        print(f\"\\nüìã First 5 meaningful lines:\")\n",
    "        for i, line in enumerate(lines[:5], 1):\n",
    "            display_line = line[:100] + \"...\" if len(line) > 100 else line\n",
    "            print(f\"   {i}. {display_line}\")\n",
    "        \n",
    "        # Show a sample chunk\n",
    "        print(f\"\\nüìÑ Sample chunk (first {config.CHUNK_SIZE} characters):\")\n",
    "        sample_chunk = text_content[:config.CHUNK_SIZE].strip()\n",
    "        print(f\"'{sample_chunk}...'\")\n",
    "        \n",
    "        # Calculate chunking statistics\n",
    "        total_chars = len(text_content)\n",
    "        chunk_size = config.CHUNK_SIZE\n",
    "        chunk_overlap = config.CHUNK_OVERLAP\n",
    "        \n",
    "        # Estimate chunks (accounting for overlap)\n",
    "        estimated_chunks = max(1, (total_chars - chunk_overlap) // (chunk_size - chunk_overlap))\n",
    "        \n",
    "        print(f\"\\nüßÆ Chunking Analysis:\")\n",
    "        print(f\"   üì¶ Estimated chunks: {estimated_chunks}\")\n",
    "        print(f\"   üìè Chunk size: {chunk_size:,} characters\")\n",
    "        print(f\"   üîÑ Overlap: {chunk_overlap:,} characters ({chunk_overlap/chunk_size*100:.1f}%)\")\n",
    "        print(f\"   üíæ Memory per chunk: ~{chunk_size * 4 / 1024:.1f} KB\")  # Rough estimate\n",
    "        \n",
    "        # Content quality check\n",
    "        if len(text_content.strip()) < 100:\n",
    "            print(f\"\\n‚ö†Ô∏è  Warning: Very short content - PDF might be image-based\")\n",
    "        elif \"azure\" in text_content.lower() or \"network\" in text_content.lower():\n",
    "            print(f\"\\n‚úÖ Content looks good - contains Azure networking terms!\")\n",
    "        else:\n",
    "            print(f\"\\nü§î Content extracted but may need review\")\n",
    "            \n",
    "        print(f\"\\nüéØ Next step: Text Chunking System!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        print(f\"üîß Debug info:\")\n",
    "        print(f\"   üìÅ PDF path: {test_pdf}\")\n",
    "        print(f\"   üìè File size: {test_pdf.stat().st_size:,} bytes\")\n",
    "        print(f\"   üîç File exists: {test_pdf.exists()}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No PDF files found!\")\n",
    "    print(f\"üìÅ Looking in: {Path(config.PDF_FOLDER).absolute()}\")\n",
    "    print(\"üí° Add some PDF files to continue testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dcb179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bff63ce",
   "metadata": {},
   "source": [
    "**Smart Text Chunking System Overview**\n",
    "\n",
    "**What This Code Does:**\n",
    "\n",
    "* **Initializes `SmartTextChunker`** with configurable chunk size and overlap, reporting setup details.\n",
    "* **Cleans input text** by normalizing whitespace and paragraph breaks.\n",
    "* **Finds smart split points** near ideal chunk boundaries, preferring sentence endings or paragraph breaks.\n",
    "* **Generates overlapping chunks** of roughly `CHUNK_SIZE` characters with `CHUNK_OVERLAP`, enforcing a minimum forward progress to avoid tiny fragments.\n",
    "* **Assigns metadata** (`chunk_id`, start/end offsets, character and word counts, source) to each chunk.\n",
    "* **Analyzes chunk statistics** (total count, size range, averages) for quality control.\n",
    "* **Includes safety checks** to prevent infinite loops or excessive chunk counts.\n",
    "* **Test harness** demonstrates the fixed algorithm on sample Azure networking text and reports before/after metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "e8a6e7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Building Text Chunking System...\n",
      "\n",
      "üß™ Testing FIXED Text Chunking System...\n",
      "üéØ Chunker initialized:\n",
      "   üìè Chunk size: 1000 characters\n",
      "   üîÑ Overlap: 200 characters (20.0%)\n",
      "\n",
      "üìù Sample text length: 1,208 characters\n",
      "üìù Processing text: 1,202 characters\n",
      "üì¶ Starting chunking process...\n",
      "   üì¶ Chunk 1: 0-955 (955 chars)\n",
      "   üì¶ Chunk 2: 755-1202 (447 chars)\n",
      "‚úÖ Chunking complete: 2 chunks created\n",
      "\n",
      "üìä FIXED Chunking Results:\n",
      "   üì¶ Total chunks created: 2 (was 202!)\n",
      "   üìè Average chunk size: 701 characters\n",
      "   üìÑ Average words per chunk: 110 words\n",
      "   üìê Size range: 447-955 characters\n",
      "\n",
      "üîç Generated Chunks:\n",
      "\n",
      "üì¶ Chunk 0:\n",
      "   üìè Length: 955 chars, 150 words\n",
      "   üìç Position: 0-955\n",
      "   üìù Preview: Azure Virtual Networks (VNets) provide the foundation for your private network in Azure. VNets enable Azure resources, like Azure Virtual Machines (VM...\n",
      "   üîÑ Overlap with next chunk: 400 characters\n",
      "\n",
      "üì¶ Chunk 1:\n",
      "   üìè Length: 447 chars, 71 words\n",
      "   üìç Position: 755-1202\n",
      "   üìù Preview: ups (NSGs) contain security rules that allow or deny inbound or outbound network traffic to several types of Azure resources. For each rule, you can s...\n",
      "\n",
      "üß™ Testing with REAL Azure PDF content...\n",
      "üìÑ PDF content: 25,974 characters\n",
      "üìù Processing text: 25,370 characters\n",
      "üì¶ Starting chunking process...\n",
      "   üì¶ Chunk 1: 0-963 (963 chars)\n",
      "   üì¶ Chunk 2: 763-1668 (905 chars)\n",
      "   üì¶ Chunk 3: 1513-2513 (999 chars)\n",
      "‚úÖ Chunking complete: 33 chunks created\n",
      "\n",
      "üìä Real PDF Chunking Results:\n",
      "   üì¶ Total chunks: 33\n",
      "   üìè Average size: 953 characters\n",
      "   üìÑ Average words: 149 words\n",
      "   üìê Size range: 685-1000 characters\n",
      "\n",
      "üéØ Chunking Analysis:\n",
      "   üìà Expected chunks: ~25\n",
      "   ‚úÖ Actual chunks: 33\n",
      "   üìä Efficiency: 75.8% of expected\n",
      "\n",
      "üìä BEFORE vs AFTER:\n",
      "   üî¥ BROKEN: 202 tiny chunks, avg 106 chars\n",
      "   ‚úÖ FIXED:  2 proper chunks, avg 701 chars\n",
      "\n",
      "‚úÖ Text Chunking System FIXED and Ready!\n",
      "üéØ Next: Test with real PDF content!\n"
     ]
    }
   ],
   "source": [
    "# Cell: Text Chunking System - UPDATED AND FIXED\n",
    "print(\"üì¶ Building Text Chunking System...\")\n",
    "\n",
    "class SmartTextChunker:\n",
    "    \"\"\"Intelligent text chunking for Azure RAG system\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initialize with configuration settings\"\"\"\n",
    "        self.config = config\n",
    "        self.chunk_size = config.CHUNK_SIZE\n",
    "        self.chunk_overlap = config.CHUNK_OVERLAP\n",
    "        \n",
    "        print(f\"üéØ Chunker initialized:\")\n",
    "        print(f\"   üìè Chunk size: {self.chunk_size} characters\")\n",
    "        print(f\"   üîÑ Overlap: {self.chunk_overlap} characters ({self.chunk_overlap/self.chunk_size*100:.1f}%)\")\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text before chunking\"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        import re\n",
    "        \n",
    "        # Replace multiple spaces with single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove excessive line breaks (keep paragraph breaks)\n",
    "        text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)\n",
    "        \n",
    "        # Strip leading/trailing whitespace\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def find_sentence_boundary(self, text, ideal_position):\n",
    "        \"\"\"Find the best place to split text (prefer sentence endings)\"\"\"\n",
    "        # Don't search past the end of text\n",
    "        if ideal_position >= len(text):\n",
    "            return len(text)\n",
    "            \n",
    "        # Look for sentence endings near the ideal position\n",
    "        search_range = min(100, len(text) - ideal_position)  # Don't go past text end\n",
    "        \n",
    "        # Search backwards from ideal position for sentence endings\n",
    "        for i in range(ideal_position, max(0, ideal_position - search_range), -1):\n",
    "            if i < len(text) and text[i] in '.!?':\n",
    "                # Make sure it's not just an abbreviation\n",
    "                if i + 1 < len(text) and text[i + 1] in ' \\n':\n",
    "                    return i + 1\n",
    "        \n",
    "        # If no sentence ending found, look for paragraph breaks\n",
    "        for i in range(ideal_position, max(0, ideal_position - search_range), -1):\n",
    "            if i < len(text) and text[i] == '\\n':\n",
    "                return i + 1\n",
    "        \n",
    "        # If nothing found, use the ideal position\n",
    "        return ideal_position\n",
    "    \n",
    "    def create_chunks(self, text, source_info=\"Unknown\"):\n",
    "        \"\"\"Split text into overlapping chunks with smart boundaries\"\"\"\n",
    "        \n",
    "        # Clean the text first\n",
    "        text = self.clean_text(text)\n",
    "        \n",
    "        print(f\"üìù Processing text: {len(text):,} characters\")\n",
    "        \n",
    "        if len(text) <= self.chunk_size:\n",
    "            # Text is small enough to be one chunk\n",
    "            return [{\n",
    "                'content': text,\n",
    "                'chunk_id': 0,\n",
    "                'source': source_info,\n",
    "                'char_start': 0,\n",
    "                'char_end': len(text),\n",
    "                'char_count': len(text),\n",
    "                'word_count': len(text.split())\n",
    "            }]\n",
    "        \n",
    "        chunks = []\n",
    "        start_pos = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        print(f\"üì¶ Starting chunking process...\")\n",
    "        \n",
    "        while start_pos < len(text):\n",
    "            # Calculate ideal end position\n",
    "            ideal_end = start_pos + self.chunk_size\n",
    "            \n",
    "            if ideal_end >= len(text):\n",
    "                # Last chunk - take remaining text\n",
    "                end_pos = len(text)\n",
    "            else:\n",
    "                # Find smart boundary\n",
    "                end_pos = self.find_sentence_boundary(text, ideal_end)\n",
    "            \n",
    "            # Extract chunk content\n",
    "            chunk_content = text[start_pos:end_pos].strip()\n",
    "            \n",
    "            if chunk_content:  # Only add non-empty chunks\n",
    "                chunk = {\n",
    "                    'content': chunk_content,\n",
    "                    'chunk_id': chunk_id,\n",
    "                    'source': source_info,\n",
    "                    'char_start': start_pos,\n",
    "                    'char_end': end_pos,\n",
    "                    'char_count': len(chunk_content),\n",
    "                    'word_count': len(chunk_content.split())\n",
    "                }\n",
    "                chunks.append(chunk)\n",
    "                chunk_id += 1\n",
    "                \n",
    "                # Show progress for first few chunks\n",
    "                if chunk_id <= 3:\n",
    "                    print(f\"   üì¶ Chunk {chunk_id}: {start_pos}-{end_pos} ({len(chunk_content)} chars)\")\n",
    "            \n",
    "            # FIXED: Calculate next starting position with proper overlap\n",
    "            next_start = end_pos - self.chunk_overlap\n",
    "            \n",
    "            # CRITICAL FIX: Ensure meaningful progress to prevent tiny chunks\n",
    "            min_progress = max(self.chunk_size - self.chunk_overlap - 50, 200)  # Minimum meaningful step\n",
    "            \n",
    "            if next_start <= start_pos:\n",
    "                # If overlap is too big, make reasonable progress\n",
    "                next_start = start_pos + min_progress\n",
    "            elif (next_start - start_pos) < min_progress:\n",
    "                # If we're not making enough progress, force a bigger step\n",
    "                next_start = start_pos + min_progress\n",
    "            \n",
    "            # Safety: if we're near the end, just finish\n",
    "            if next_start >= len(text) - 50:\n",
    "                break\n",
    "                \n",
    "            start_pos = next_start\n",
    "            \n",
    "            # Safety check to prevent infinite loops\n",
    "            if chunk_id > 100:  # Reasonable limit for any document\n",
    "                print(f\"‚ö†Ô∏è  Safety limit reached at {chunk_id} chunks\")\n",
    "                break\n",
    "        \n",
    "        print(f\"‚úÖ Chunking complete: {len(chunks)} chunks created\")\n",
    "        return chunks\n",
    "    \n",
    "    def analyze_chunks(self, chunks):\n",
    "        \"\"\"Analyze chunk statistics\"\"\"\n",
    "        if not chunks:\n",
    "            return {}\n",
    "        \n",
    "        char_counts = [chunk['char_count'] for chunk in chunks]\n",
    "        word_counts = [chunk['word_count'] for chunk in chunks]\n",
    "        \n",
    "        return {\n",
    "            'total_chunks': len(chunks),\n",
    "            'avg_chars': sum(char_counts) / len(char_counts),\n",
    "            'min_chars': min(char_counts),\n",
    "            'max_chars': max(char_counts),\n",
    "            'avg_words': sum(word_counts) / len(word_counts),\n",
    "            'total_chars': sum(char_counts),\n",
    "            'total_words': sum(word_counts)\n",
    "        }\n",
    "\n",
    "# Test the FIXED chunking system\n",
    "print(\"\\nüß™ Testing FIXED Text Chunking System...\")\n",
    "\n",
    "# Create chunker instance\n",
    "chunker = SmartTextChunker(config)\n",
    "\n",
    "# Test with sample Azure networking text\n",
    "sample_text = \"\"\"\n",
    "Azure Virtual Networks (VNets) provide the foundation for your private network in Azure. VNets enable Azure resources, like Azure Virtual Machines (VMs), to securely communicate with each other, the internet, and on-premises networks.\n",
    "\n",
    "A VNet is similar to a traditional network that you'd operate in your own data center. But it brings with it additional benefits of Azure's infrastructure such as scale, availability, and isolation.\n",
    "\n",
    "Key concepts for VNets include address space, subnets, regions, and subscriptions. The address space is a set of private and public IP addresses that you can use within the VNet. You can divide the address space into multiple subnets and allocate a portion of the VNet's address space to each subnet.\n",
    "\n",
    "Network Security Groups (NSGs) contain security rules that allow or deny inbound or outbound network traffic to several types of Azure resources. For each rule, you can specify source and destination, port, and protocol.\n",
    "\n",
    "Azure Load Balancer operates at layer 4 of the Open Systems Interconnection (OSI) model. It's the single point of contact for clients. Load Balancer distributes inbound flows that arrive at the load balancer's front end to backend pool instances.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Sample text length: {len(sample_text):,} characters\")\n",
    "\n",
    "# Create chunks with FIXED algorithm\n",
    "chunks = chunker.create_chunks(sample_text, source_info=\"Azure VNet Study Guide\")\n",
    "\n",
    "# Analyze results\n",
    "stats = chunker.analyze_chunks(chunks)\n",
    "\n",
    "print(f\"\\nüìä FIXED Chunking Results:\")\n",
    "print(f\"   üì¶ Total chunks created: {stats['total_chunks']} (was 202!)\")\n",
    "print(f\"   üìè Average chunk size: {stats['avg_chars']:.0f} characters\")\n",
    "print(f\"   üìÑ Average words per chunk: {stats['avg_words']:.0f} words\")\n",
    "print(f\"   üìê Size range: {stats['min_chars']}-{stats['max_chars']} characters\")\n",
    "\n",
    "# Show the chunks\n",
    "print(f\"\\nüîç Generated Chunks:\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nüì¶ Chunk {chunk['chunk_id']}:\")\n",
    "    print(f\"   üìè Length: {chunk['char_count']} chars, {chunk['word_count']} words\")\n",
    "    print(f\"   üìç Position: {chunk['char_start']}-{chunk['char_end']}\")\n",
    "    preview = chunk['content'][:150] + \"...\" if len(chunk['content']) > 150 else chunk['content']\n",
    "    print(f\"   üìù Preview: {preview}\")\n",
    "    \n",
    "    # Show overlap with next chunk\n",
    "    if i < len(chunks) - 1:\n",
    "        next_chunk = chunks[i + 1]\n",
    "        overlap_start = max(chunk['char_start'], next_chunk['char_start'] - config.CHUNK_OVERLAP)\n",
    "        overlap_end = min(chunk['char_end'], next_chunk['char_start'] + config.CHUNK_OVERLAP)\n",
    "        if overlap_end > overlap_start:\n",
    "            print(f\"   üîÑ Overlap with next chunk: {overlap_end - overlap_start} characters\")\n",
    "\n",
    "# Test with real PDF content if available\n",
    "if 'text_content' in globals():\n",
    "    print(f\"\\nüß™ Testing with REAL Azure PDF content...\")\n",
    "    print(f\"üìÑ PDF content: {len(text_content):,} characters\")\n",
    "    \n",
    "    # Create chunks from real PDF\n",
    "    real_chunks = chunker.create_chunks(text_content, source_info=\"Azure VNet Study Guide PDF\")\n",
    "    real_stats = chunker.analyze_chunks(real_chunks)\n",
    "    \n",
    "    print(f\"\\nüìä Real PDF Chunking Results:\")\n",
    "    print(f\"   üì¶ Total chunks: {real_stats['total_chunks']}\")\n",
    "    print(f\"   üìè Average size: {real_stats['avg_chars']:.0f} characters\")\n",
    "    print(f\"   üìÑ Average words: {real_stats['avg_words']:.0f} words\")\n",
    "    print(f\"   üìê Size range: {real_stats['min_chars']}-{real_stats['max_chars']} characters\")\n",
    "    \n",
    "    # Expected vs actual\n",
    "    expected_chunks = len(text_content) // config.CHUNK_SIZE\n",
    "    print(f\"\\nüéØ Chunking Analysis:\")\n",
    "    print(f\"   üìà Expected chunks: ~{expected_chunks}\")\n",
    "    print(f\"   ‚úÖ Actual chunks: {real_stats['total_chunks']}\")\n",
    "    print(f\"   üìä Efficiency: {expected_chunks/real_stats['total_chunks']*100:.1f}% of expected\")\n",
    "\n",
    "print(f\"\\nüìä BEFORE vs AFTER:\")\n",
    "print(f\"   üî¥ BROKEN: 202 tiny chunks, avg 106 chars\")\n",
    "print(f\"   ‚úÖ FIXED:  {stats['total_chunks']} proper chunks, avg {stats['avg_chars']:.0f} chars\")\n",
    "\n",
    "print(f\"\\n‚úÖ Text Chunking System FIXED and Ready!\")\n",
    "print(f\"üéØ Next: Test with real PDF content!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c796f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74b6445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e0c9a1b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d80a147",
   "metadata": {},
   "source": [
    "**Basic Text Search System Overview**\n",
    "\n",
    "* **Defines a `BasicTextSearcher` class** that ingests text ‚Äúchunks‚Äù and builds an inverted index mapping keywords to their source passages.\n",
    "* **Tokenizes content** using regular expressions and avoids duplicate entries in the index for each chunk.\n",
    "* **Implements a relevance scoring** method combining term frequency, exact-phrase matches, and domain-specific bonuses for Azure-related terms.\n",
    "* **Provides a `search()` method** that retrieves, scores, and ranks matching chunks based on the query, returning the top results.\n",
    "* **Includes methods for adding new chunks** (`add_chunks`) and rebuilding the index incrementally.\n",
    "* **Offers a `get_search_statistics()` method** to report total chunks, indexed terms, average/min/max chunk sizes, source distribution, and a rough memory estimate.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "013cb47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Building Basic Text Search System...\n",
      "\n",
      "üß™ Testing Basic Search System...\n",
      "üéØ Search system initialized\n",
      "   üîç Ready to index chunks\n",
      "   üìä Will track search statistics\n",
      "üìö Added 4 chunks to search index\n",
      "   üì¶ Total chunks in system: 4\n",
      "   üîë Index contains 68 unique terms\n",
      "\n",
      "üéØ Testing Search Queries:\n",
      "\n",
      "üîç Searching for: 'network security'\n",
      "   üìù Query terms: ['network', 'security']\n",
      "   üì¶ Found 4 candidate chunks\n",
      "   ‚úÖ Returning top 3 results\n",
      "      1. Score: 1.600 | Terms: ['network', 'security']\n",
      "      2. Score: 1.430 | Terms: ['network', 'security']\n",
      "      3. Score: 0.483 | Terms: ['network']\n",
      "\n",
      "üìä Results for 'network security':\n",
      "   1. [1.600] Azure VNet Guide\n",
      "      üìù Azure Virtual Networks (VNets) provide isolated networking environments in Azure. You can configure ...\n",
      "      üéØ Matched: ['network', 'security']\n",
      "   2. [1.430] Azure Security Guide\n",
      "      üìù Network Security Groups (NSGs) contain security rules that control inbound and outbound traffic to A...\n",
      "      üéØ Matched: ['network', 'security']\n",
      "   3. [0.483] Azure VNet Guide\n",
      "      üìù Subnets allow you to segment your Azure virtual network into smaller networks. Each subnet must be a...\n",
      "      üéØ Matched: ['network']\n",
      "\n",
      "üîç Searching for: 'load balancer'\n",
      "   üìù Query terms: ['load', 'balancer']\n",
      "   üì¶ Found 1 candidate chunks\n",
      "   ‚úÖ Returning top 1 results\n",
      "      1. Score: 1.387 | Terms: ['load', 'balancer']\n",
      "\n",
      "üìä Results for 'load balancer':\n",
      "   1. [1.387] Azure Load Balancer Guide\n",
      "      üìù Azure Load Balancer distributes incoming network traffic across multiple virtual machines. It operat...\n",
      "      üéØ Matched: ['load', 'balancer']\n",
      "\n",
      "üîç Searching for: 'Azure subnet'\n",
      "   üìù Query terms: ['azure', 'subnet']\n",
      "   üì¶ Found 4 candidate chunks\n",
      "   ‚úÖ Returning top 3 results\n",
      "      1. Score: 0.550 | Terms: ['azure', 'subnet']\n",
      "      2. Score: 0.525 | Terms: ['azure', 'subnet']\n",
      "      3. Score: 0.343 | Terms: ['azure']\n",
      "\n",
      "üìä Results for 'Azure subnet':\n",
      "   1. [0.550] Azure VNet Guide\n",
      "      üìù Azure Virtual Networks (VNets) provide isolated networking environments in Azure. You can configure ...\n",
      "      üéØ Matched: ['azure', 'subnet']\n",
      "   2. [0.525] Azure VNet Guide\n",
      "      üìù Subnets allow you to segment your Azure virtual network into smaller networks. Each subnet must be a...\n",
      "      üéØ Matched: ['azure', 'subnet']\n",
      "   3. [0.343] Azure Security Guide\n",
      "      üìù Network Security Groups (NSGs) contain security rules that control inbound and outbound traffic to A...\n",
      "      üéØ Matched: ['azure']\n",
      "\n",
      "üîç Searching for: 'virtual machine traffic'\n",
      "   üìù Query terms: ['virtual', 'machine', 'traffic']\n",
      "   üì¶ Found 4 candidate chunks\n",
      "   ‚úÖ Returning top 3 results\n",
      "      1. Score: 0.450 | Terms: ['virtual']\n",
      "      2. Score: 0.442 | Terms: ['virtual']\n",
      "      3. Score: 0.430 | Terms: ['virtual', 'machine', 'traffic']\n",
      "\n",
      "üìä Results for 'virtual machine traffic':\n",
      "   1. [0.450] Azure VNet Guide\n",
      "      üìù Azure Virtual Networks (VNets) provide isolated networking environments in Azure. You can configure ...\n",
      "      üéØ Matched: ['virtual']\n",
      "   2. [0.442] Azure VNet Guide\n",
      "      üìù Subnets allow you to segment your Azure virtual network into smaller networks. Each subnet must be a...\n",
      "      üéØ Matched: ['virtual']\n",
      "   3. [0.430] Azure Load Balancer Guide\n",
      "      üìù Azure Load Balancer distributes incoming network traffic across multiple virtual machines. It operat...\n",
      "      üéØ Matched: ['virtual', 'machine', 'traffic']\n",
      "\n",
      "üîç Searching for: 'NSG rules'\n",
      "   üìù Query terms: ['nsg', 'rules']\n",
      "   üì¶ Found 1 candidate chunks\n",
      "   ‚úÖ Returning top 1 results\n",
      "      1. Score: 0.387 | Terms: ['nsg', 'rules']\n",
      "\n",
      "üìä Results for 'NSG rules':\n",
      "   1. [0.387] Azure Security Guide\n",
      "      üìù Network Security Groups (NSGs) contain security rules that control inbound and outbound traffic to A...\n",
      "      üéØ Matched: ['nsg', 'rules']\n",
      "\n",
      "üìä Search System Statistics:\n",
      "   üìà total_chunks: 4\n",
      "   üìà total_terms_indexed: 68\n",
      "   üìà avg_chunk_size: 154.25\n",
      "   üìà min_chunk_size: 144\n",
      "   üìà max_chunk_size: 166\n",
      "   üìà sources: {'Azure VNet Guide': 2, 'Azure Security Guide': 1, 'Azure Load Balancer Guide': 1}\n",
      "   üìà memory_estimate_mb: 0.002353668212890625\n",
      "\n",
      "‚úÖ Basic Search System Ready!\n",
      "üéØ Next: Simple Storage System for persistence!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Cell: Basic Text Search System\n",
    "print(\"üîç Building Basic Text Search System...\")\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class BasicTextSearcher:\n",
    "    \"\"\"Simple but effective text search for Azure RAG foundation\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initialize with configuration\"\"\"\n",
    "        self.config = config\n",
    "        self.chunks = []  # Will store our text chunks\n",
    "        self.search_index = {}  # Simple keyword index\n",
    "        \n",
    "        print(f\"üéØ Search system initialized\")\n",
    "        print(f\"   üîç Ready to index chunks\")\n",
    "        print(f\"   üìä Will track search statistics\")\n",
    "    \n",
    "    def add_chunks(self, chunks: List[Dict]):\n",
    "        \"\"\"Add chunks to our search system\"\"\"\n",
    "        self.chunks.extend(chunks)\n",
    "        self._build_search_index(chunks)\n",
    "        \n",
    "        print(f\"üìö Added {len(chunks)} chunks to search index\")\n",
    "        print(f\"   üì¶ Total chunks in system: {len(self.chunks)}\")\n",
    "        print(f\"   üîë Index contains {len(self.search_index)} unique terms\")\n",
    "    \n",
    "    def _build_search_index(self, chunks: List[Dict]):\n",
    "        \"\"\"Build a simple keyword index for fast searching\"\"\"\n",
    "        for chunk in chunks:\n",
    "            chunk_id = chunk['chunk_id']\n",
    "            text = chunk['content'].lower()\n",
    "            \n",
    "            # Extract words (simple tokenization)\n",
    "            words = re.findall(r'\\b\\w+\\b', text)\n",
    "            \n",
    "            # Add to inverted index\n",
    "            for word in words:\n",
    "                if word not in self.search_index:\n",
    "                    self.search_index[word] = []\n",
    "                \n",
    "                # Only add if not already there (avoid duplicates)\n",
    "                if chunk_id not in [item['chunk_id'] for item in self.search_index[word]]:\n",
    "                    self.search_index[word].append({\n",
    "                        'chunk_id': chunk_id,\n",
    "                        'source': chunk['source']\n",
    "                    })\n",
    "    \n",
    "    def _calculate_relevance_score(self, chunk: Dict, query_terms: List[str]) -> float:\n",
    "        \"\"\"Calculate how relevant a chunk is to the query\"\"\"\n",
    "        content = chunk['content'].lower()\n",
    "        score = 0.0\n",
    "        \n",
    "        # Count term matches\n",
    "        for term in query_terms:\n",
    "            term_count = content.count(term.lower())\n",
    "            if term_count > 0:\n",
    "                # Term frequency score (more mentions = higher score)\n",
    "                tf_score = term_count / len(content.split())\n",
    "                score += tf_score\n",
    "                \n",
    "                # Bonus for exact phrase matches\n",
    "                if len(query_terms) > 1 and ' '.join(query_terms).lower() in content:\n",
    "                    score += 0.5\n",
    "        \n",
    "        # Bonus for Azure-specific terms (domain relevance)\n",
    "        azure_terms = ['azure', 'vnet', 'subnet', 'nsg', 'load balancer', 'network']\n",
    "        azure_bonus = sum(1 for term in azure_terms if term in content) * 0.1\n",
    "        score += azure_bonus\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def search(self, query: str, max_results: int = 5) -> List[Dict]:\n",
    "        \"\"\"Search for chunks matching the query\"\"\"\n",
    "        if not query.strip():\n",
    "            return []\n",
    "        \n",
    "        print(f\"\\nüîç Searching for: '{query}'\")\n",
    "        \n",
    "        # Prepare query terms\n",
    "        query_terms = re.findall(r'\\b\\w+\\b', query.lower())\n",
    "        print(f\"   üìù Query terms: {query_terms}\")\n",
    "        \n",
    "        # Find candidate chunks\n",
    "        candidate_chunks = set()\n",
    "        \n",
    "        for term in query_terms:\n",
    "            if term in self.search_index:\n",
    "                for item in self.search_index[term]:\n",
    "                    candidate_chunks.add(item['chunk_id'])\n",
    "        \n",
    "        print(f\"   üì¶ Found {len(candidate_chunks)} candidate chunks\")\n",
    "        \n",
    "        if not candidate_chunks:\n",
    "            print(f\"   ‚ùå No matches found\")\n",
    "            return []\n",
    "        \n",
    "        # Score and rank chunks\n",
    "        scored_chunks = []\n",
    "        \n",
    "        for chunk_id in candidate_chunks:\n",
    "            # Find the actual chunk\n",
    "            chunk = next((c for c in self.chunks if c['chunk_id'] == chunk_id), None)\n",
    "            if chunk:\n",
    "                score = self._calculate_relevance_score(chunk, query_terms)\n",
    "                if score > 0:\n",
    "                    scored_chunks.append({\n",
    "                        'chunk': chunk,\n",
    "                        'score': score,\n",
    "                        'matched_terms': [term for term in query_terms \n",
    "                                        if term in chunk['content'].lower()]\n",
    "                    })\n",
    "        \n",
    "        # Sort by relevance score (highest first)\n",
    "        scored_chunks.sort(key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        # Return top results\n",
    "        results = scored_chunks[:max_results]\n",
    "        \n",
    "        print(f\"   ‚úÖ Returning top {len(results)} results\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"      {i}. Score: {result['score']:.3f} | Terms: {result['matched_terms']}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_search_statistics(self) -> Dict:\n",
    "        \"\"\"Get statistics about the search system\"\"\"\n",
    "        if not self.chunks:\n",
    "            return {'status': 'empty'}\n",
    "        \n",
    "        # Calculate statistics\n",
    "        chunk_sizes = [len(chunk['content']) for chunk in self.chunks]\n",
    "        sources = [chunk['source'] for chunk in self.chunks]\n",
    "        source_counts = Counter(sources)\n",
    "        \n",
    "        return {\n",
    "            'total_chunks': len(self.chunks),\n",
    "            'total_terms_indexed': len(self.search_index),\n",
    "            'avg_chunk_size': sum(chunk_sizes) / len(chunk_sizes),\n",
    "            'min_chunk_size': min(chunk_sizes),\n",
    "            'max_chunk_size': max(chunk_sizes),\n",
    "            'sources': dict(source_counts),\n",
    "            'memory_estimate_mb': (sum(chunk_sizes) * 4) / (1024 * 1024)  # Rough estimate\n",
    "        }\n",
    "\n",
    "# Test the search system\n",
    "print(\"\\nüß™ Testing Basic Search System...\")\n",
    "\n",
    "# Create search system\n",
    "searcher = BasicTextSearcher(config)\n",
    "\n",
    "# Create some test chunks (simulate chunked Azure content)\n",
    "test_chunks = [\n",
    "    {\n",
    "        'content': 'Azure Virtual Networks (VNets) provide isolated networking environments in Azure. You can configure subnets, network security groups, and routing tables.',\n",
    "        'chunk_id': 0,\n",
    "        'source': 'Azure VNet Guide',\n",
    "        'char_start': 0,\n",
    "        'char_end': 150,\n",
    "        'char_count': 150,\n",
    "        'word_count': 25\n",
    "    },\n",
    "    {\n",
    "        'content': 'Network Security Groups (NSGs) contain security rules that control inbound and outbound traffic to Azure resources. Each rule specifies protocol, port, and direction.',\n",
    "        'chunk_id': 1,\n",
    "        'source': 'Azure Security Guide', \n",
    "        'char_start': 0,\n",
    "        'char_end': 160,\n",
    "        'char_count': 160,\n",
    "        'word_count': 27\n",
    "    },\n",
    "    {\n",
    "        'content': 'Azure Load Balancer distributes incoming network traffic across multiple virtual machines. It operates at Layer 4 and supports both TCP and UDP protocols.',\n",
    "        'chunk_id': 2,\n",
    "        'source': 'Azure Load Balancer Guide',\n",
    "        'char_start': 0,\n",
    "        'char_end': 155,\n",
    "        'char_count': 155,\n",
    "        'word_count': 26\n",
    "    },\n",
    "    {\n",
    "        'content': 'Subnets allow you to segment your Azure virtual network into smaller networks. Each subnet must be assigned a portion of the VNet address space.',\n",
    "        'chunk_id': 3,\n",
    "        'source': 'Azure VNet Guide',\n",
    "        'char_start': 151,\n",
    "        'char_end': 280,\n",
    "        'char_count': 129,\n",
    "        'word_count': 24\n",
    "    }\n",
    "]\n",
    "\n",
    "# Add chunks to searcher\n",
    "searcher.add_chunks(test_chunks)\n",
    "\n",
    "# Test different search queries\n",
    "test_queries = [\n",
    "    \"network security\",\n",
    "    \"load balancer\",\n",
    "    \"Azure subnet\",\n",
    "    \"virtual machine traffic\",\n",
    "    \"NSG rules\"\n",
    "]\n",
    "\n",
    "print(f\"\\nüéØ Testing Search Queries:\")\n",
    "for query in test_queries:\n",
    "    results = searcher.search(query, max_results=3)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nüìä Results for '{query}':\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            chunk = result['chunk']\n",
    "            preview = chunk['content'][:100] + \"...\" if len(chunk['content']) > 100 else chunk['content']\n",
    "            print(f\"   {i}. [{result['score']:.3f}] {chunk['source']}\")\n",
    "            print(f\"      üìù {preview}\")\n",
    "            print(f\"      üéØ Matched: {result['matched_terms']}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå No results for '{query}'\")\n",
    "\n",
    "# Show system statistics\n",
    "print(f\"\\nüìä Search System Statistics:\")\n",
    "stats = searcher.get_search_statistics()\n",
    "for key, value in stats.items():\n",
    "    print(f\"   üìà {key}: {value}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Basic Search System Ready!\")\n",
    "print(f\"üéØ Next: Simple Storage System for persistence!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b44cbe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb88468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00a49ff8",
   "metadata": {},
   "source": [
    "**Storage Cleanup and Simple Storage System Implementation**\n",
    "\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "* **`clean_test_storage()` helper**\n",
    "\n",
    "  * Deletes all files matching `session_*` in the configured storage folder to ensure tests start with no leftover data.\n",
    "  * Prints each deleted filename and a confirmation when done.\n",
    "\n",
    "* **`SimpleStorageManager` class**\n",
    "\n",
    "  * **Initialization**:\n",
    "\n",
    "    * Reads `config.PROCESSED_FOLDER`, creates that directory if missing, and logs the storage location.\n",
    "  * **Session ID & Hashing**:\n",
    "\n",
    "    * `_generate_session_id()` ‚Üí timestamp-based IDs (`session_YYYYMMDD_HHMMSS`).\n",
    "    * `_calculate_content_hash()` ‚Üí MD5 hash prefix of content for change detection.\n",
    "  * **Chunk Persistence** (`save_chunks` / `load_chunks`):\n",
    "\n",
    "    * `save_chunks()` serializes chunks + metadata to both JSON (human-readable) and pickle (fast-load), returning the session ID.\n",
    "    * `load_chunks()` prefers pickle over JSON, loads chunks and metadata, and logs counts and timestamps.\n",
    "  * **Search-Index Persistence** (`save_search_index` / `load_search_index`):\n",
    "\n",
    "    * `save_search_index()` pickles the inverted index and stats alongside chunk metadata.\n",
    "    * `load_search_index()` retrieves the index pickle and returns it with its metadata.\n",
    "  * **Session Management** (`list_saved_sessions` / `cleanup_old_sessions`):\n",
    "\n",
    "    * `list_saved_sessions()` scans for `*_chunks.json`, reads metadata, and reports which sessions have pickle/index files.\n",
    "    * `cleanup_old_sessions(keep_latest)` sorts sessions by creation time, deletes all but the most recent N, and logs each removal.\n",
    "  * **Test Harness** at the end exercises storage of test chunks, loading them back, integration with `BasicTextSearcher`, and session listing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "378aed13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Deleted: session_20250629_124156_search_index.pkl\n",
      "üóëÔ∏è Deleted: session_20250629_124156_chunks.pkl\n",
      "üóëÔ∏è Deleted: session_20250629_124156_chunks.json\n",
      "‚úÖ Storage cleaned - ready for fresh test\n",
      "üíæ Building Simple Storage System...\n",
      "\n",
      "üß™ Testing Simple Storage System...\n",
      "üíæ Storage system initialized\n",
      "   üìÅ Storage location: data/processed\n",
      "   üîß Ready to save/load chunks and search indices\n",
      "\n",
      "üíæ Testing chunk storage...\n",
      "‚úÖ Saved 2 chunks:\n",
      "   üìÑ JSON: session_20250629_125835_chunks.json\n",
      "   üöÄ Pickle: session_20250629_125835_chunks.pkl\n",
      "   üîñ Session ID: session_20250629_125835\n",
      "\n",
      "üìö Testing chunk loading...\n",
      "üìö Loading from pickle: session_20250629_125835_chunks.pkl\n",
      "‚úÖ Loaded 2 chunks from Test Azure Documentation\n",
      "   üìÖ Created: 2025-06-29T12:58:35.646276\n",
      "   üìä Total chars: 167\n",
      "   üìÑ Total words: 27\n",
      "‚úÖ Successfully loaded 2 chunks\n",
      "\n",
      "üîç Testing integration with search system...\n",
      "üéØ Search system initialized\n",
      "   üîç Ready to index chunks\n",
      "   üìä Will track search statistics\n",
      "üìö Added 2 chunks to search index\n",
      "   üì¶ Total chunks in system: 2\n",
      "   üîë Index contains 20 unique terms\n",
      "‚úÖ Saved search index:\n",
      "   üîç File: session_20250629_125835_search_index.pkl\n",
      "   üìä Terms indexed: 20\n",
      "   üì¶ Chunks indexed: 2\n",
      "\n",
      "üîç Searching for: 'Azure network'\n",
      "   üìù Query terms: ['azure', 'network']\n",
      "   üì¶ Found 2 candidate chunks\n",
      "   ‚úÖ Returning top 2 results\n",
      "      1. Score: 0.633 | Terms: ['azure', 'network']\n",
      "      2. Score: 0.467 | Terms: ['azure', 'network']\n",
      "‚úÖ Search works with loaded chunks!\n",
      "   üéØ Found 2 results for 'Azure network'\n",
      "\n",
      "üìã Listing all saved sessions...\n",
      "üìö Found 1 saved sessions:\n",
      "   üìÑ session_20250629_125835\n",
      "      üìö Source: Test Azure Documentation\n",
      "      üìÖ Created: 2025-06-29T12:58:35.646276\n",
      "      üì¶ Chunks: 2\n",
      "      üöÄ Pickle: ‚úÖ\n",
      "      üîç Index: ‚úÖ\n",
      "\n",
      "‚úÖ Simple Storage System Complete!\n",
      "üéØ Foundation notebook ready - all components working together!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Add this to the beginning of your storage test\n",
    "def clean_test_storage():\n",
    "    \"\"\"Clean up all test files before creating new ones\"\"\"\n",
    "    storage_path = Path(config.PROCESSED_FOLDER)\n",
    "    test_files = list(storage_path.glob(\"session_*\"))\n",
    "    \n",
    "    for file in test_files:\n",
    "        file.unlink()\n",
    "        print(f\"üóëÔ∏è Deleted: {file.name}\")\n",
    "    \n",
    "    print(f\"‚úÖ Storage cleaned - ready for fresh test\")\n",
    "\n",
    "# Use it like this:\n",
    "clean_test_storage()\n",
    "# Then run your normal storage test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Cell: Simple Storage System\n",
    "print(\"üíæ Building Simple Storage System...\")\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "\n",
    "class SimpleStorageManager:\n",
    "    \"\"\"Simple but robust storage for Azure RAG foundation\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initialize with configuration\"\"\"\n",
    "        self.config = config\n",
    "        self.storage_path = Path(config.PROCESSED_FOLDER)\n",
    "        self.storage_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"üíæ Storage system initialized\")\n",
    "        print(f\"   üìÅ Storage location: {self.storage_path}\")\n",
    "        print(f\"   üîß Ready to save/load chunks and search indices\")\n",
    "    \n",
    "    def _generate_session_id(self):\n",
    "        \"\"\"Generate unique session ID\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        return f\"session_{timestamp}\"\n",
    "    \n",
    "    def _calculate_content_hash(self, content):\n",
    "        \"\"\"Calculate hash of content for change detection\"\"\"\n",
    "        return hashlib.md5(content.encode('utf-8')).hexdigest()[:8]\n",
    "    \n",
    "    def save_chunks(self, chunks, source_name=\"unknown\", session_id=None):\n",
    "        \"\"\"Save chunks to storage with metadata\"\"\"\n",
    "        if not chunks:\n",
    "            print(\"‚ö†Ô∏è  No chunks to save\")\n",
    "            return None\n",
    "        \n",
    "        # Generate session ID if not provided\n",
    "        if session_id is None:\n",
    "            session_id = self._generate_session_id()\n",
    "        \n",
    "        # Prepare storage data\n",
    "        storage_data = {\n",
    "            'metadata': {\n",
    "                'session_id': session_id,\n",
    "                'source_name': source_name,\n",
    "                'created_at': datetime.now().isoformat(),\n",
    "                'total_chunks': len(chunks),\n",
    "                'chunk_size_config': self.config.CHUNK_SIZE,\n",
    "                'chunk_overlap_config': self.config.CHUNK_OVERLAP,\n",
    "                'total_characters': sum(chunk['char_count'] for chunk in chunks),\n",
    "                'total_words': sum(chunk['word_count'] for chunk in chunks)\n",
    "            },\n",
    "            'chunks': chunks\n",
    "        }\n",
    "        \n",
    "        # Save as JSON (human readable)\n",
    "        json_file = self.storage_path / f\"{session_id}_chunks.json\"\n",
    "        with open(json_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(storage_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Save as pickle (faster loading)\n",
    "        pickle_file = self.storage_path / f\"{session_id}_chunks.pkl\"\n",
    "        with open(pickle_file, 'wb') as f:\n",
    "            pickle.dump(storage_data, f)\n",
    "        \n",
    "        print(f\"‚úÖ Saved {len(chunks)} chunks:\")\n",
    "        print(f\"   üìÑ JSON: {json_file.name}\")\n",
    "        print(f\"   üöÄ Pickle: {pickle_file.name}\")\n",
    "        print(f\"   üîñ Session ID: {session_id}\")\n",
    "        \n",
    "        return session_id\n",
    "    \n",
    "    def load_chunks(self, session_id):\n",
    "        \"\"\"Load chunks from storage\"\"\"\n",
    "        pickle_file = self.storage_path / f\"{session_id}_chunks.pkl\"\n",
    "        json_file = self.storage_path / f\"{session_id}_chunks.json\"\n",
    "        \n",
    "        # Try pickle first (faster)\n",
    "        if pickle_file.exists():\n",
    "            print(f\"üìö Loading from pickle: {pickle_file.name}\")\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "        elif json_file.exists():\n",
    "            print(f\"üìö Loading from JSON: {json_file.name}\")\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            print(f\"‚ùå Session not found: {session_id}\")\n",
    "            return None, None\n",
    "        \n",
    "        chunks = data['chunks']\n",
    "        metadata = data['metadata']\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {len(chunks)} chunks from {metadata['source_name']}\")\n",
    "        print(f\"   üìÖ Created: {metadata['created_at']}\")\n",
    "        print(f\"   üìä Total chars: {metadata['total_characters']:,}\")\n",
    "        print(f\"   üìÑ Total words: {metadata['total_words']:,}\")\n",
    "        \n",
    "        return chunks, metadata\n",
    "    \n",
    "    def save_search_index(self, searcher, session_id):\n",
    "        \"\"\"Save search index and statistics\"\"\"\n",
    "        if not hasattr(searcher, 'search_index'):\n",
    "            print(\"‚ö†Ô∏è  No search index to save\")\n",
    "            return\n",
    "        \n",
    "        # Prepare index data\n",
    "        index_data = {\n",
    "            'metadata': {\n",
    "                'session_id': session_id,\n",
    "                'created_at': datetime.now().isoformat(),\n",
    "                'total_chunks_indexed': len(searcher.chunks),\n",
    "                'total_terms': len(searcher.search_index),\n",
    "                'statistics': searcher.get_search_statistics()\n",
    "            },\n",
    "            'search_index': searcher.search_index,\n",
    "            'chunks_metadata': [{\n",
    "                'chunk_id': chunk['chunk_id'],\n",
    "                'source': chunk['source'],\n",
    "                'char_count': chunk['char_count']\n",
    "            } for chunk in searcher.chunks]\n",
    "        }\n",
    "        \n",
    "        # Save index\n",
    "        index_file = self.storage_path / f\"{session_id}_search_index.pkl\"\n",
    "        with open(index_file, 'wb') as f:\n",
    "            pickle.dump(index_data, f)\n",
    "        \n",
    "        print(f\"‚úÖ Saved search index:\")\n",
    "        print(f\"   üîç File: {index_file.name}\")\n",
    "        print(f\"   üìä Terms indexed: {len(searcher.search_index):,}\")\n",
    "        print(f\"   üì¶ Chunks indexed: {len(searcher.chunks)}\")\n",
    "    \n",
    "    def load_search_index(self, session_id):\n",
    "        \"\"\"Load search index\"\"\"\n",
    "        index_file = self.storage_path / f\"{session_id}_search_index.pkl\"\n",
    "        \n",
    "        if not index_file.exists():\n",
    "            print(f\"‚ùå Search index not found: {session_id}\")\n",
    "            return None\n",
    "        \n",
    "        with open(index_file, 'rb') as f:\n",
    "            index_data = pickle.load(f)\n",
    "        \n",
    "        metadata = index_data['metadata']\n",
    "        search_index = index_data['search_index']\n",
    "        \n",
    "        print(f\"‚úÖ Loaded search index:\")\n",
    "        print(f\"   üìÖ Created: {metadata['created_at']}\")\n",
    "        print(f\"   üîç Terms: {len(search_index):,}\")\n",
    "        print(f\"   üì¶ Chunks: {metadata['total_chunks_indexed']}\")\n",
    "        \n",
    "        return search_index, metadata\n",
    "    \n",
    "    def list_saved_sessions(self):\n",
    "        \"\"\"List all saved sessions\"\"\"\n",
    "        json_files = list(self.storage_path.glob(\"*_chunks.json\"))\n",
    "        \n",
    "        if not json_files:\n",
    "            print(\"üì≠ No saved sessions found\")\n",
    "            return []\n",
    "        \n",
    "        sessions = []\n",
    "        print(f\"üìö Found {len(json_files)} saved sessions:\")\n",
    "        \n",
    "        for json_file in sorted(json_files):\n",
    "            try:\n",
    "                with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                metadata = data['metadata']\n",
    "                session_id = metadata['session_id']\n",
    "                \n",
    "                # Check for corresponding files\n",
    "                has_pickle = (self.storage_path / f\"{session_id}_chunks.pkl\").exists()\n",
    "                has_index = (self.storage_path / f\"{session_id}_search_index.pkl\").exists()\n",
    "                \n",
    "                session_info = {\n",
    "                    'session_id': session_id,\n",
    "                    'source_name': metadata['source_name'],\n",
    "                    'created_at': metadata['created_at'],\n",
    "                    'total_chunks': metadata['total_chunks'],\n",
    "                    'has_pickle': has_pickle,\n",
    "                    'has_search_index': has_index\n",
    "                }\n",
    "                \n",
    "                sessions.append(session_info)\n",
    "                \n",
    "                print(f\"   üìÑ {session_id}\")\n",
    "                print(f\"      üìö Source: {metadata['source_name']}\")\n",
    "                print(f\"      üìÖ Created: {metadata['created_at']}\")\n",
    "                print(f\"      üì¶ Chunks: {metadata['total_chunks']}\")\n",
    "                print(f\"      üöÄ Pickle: {'‚úÖ' if has_pickle else '‚ùå'}\")\n",
    "                print(f\"      üîç Index: {'‚úÖ' if has_index else '‚ùå'}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è  Error reading {json_file.name}: {e}\")\n",
    "        \n",
    "        return sessions\n",
    "    \n",
    "    def cleanup_old_sessions(self, keep_latest=5):\n",
    "        \"\"\"Keep only the latest N sessions\"\"\"\n",
    "        sessions = self.list_saved_sessions()\n",
    "        \n",
    "        if len(sessions) <= keep_latest:\n",
    "            print(f\"üìö Only {len(sessions)} sessions found, keeping all\")\n",
    "            return\n",
    "        \n",
    "        # Sort by creation time and keep latest\n",
    "        sessions.sort(key=lambda x: x['created_at'], reverse=True)\n",
    "        sessions_to_delete = sessions[keep_latest:]\n",
    "        \n",
    "        print(f\"üßπ Cleaning up {len(sessions_to_delete)} old sessions...\")\n",
    "        \n",
    "        for session in sessions_to_delete:\n",
    "            session_id = session['session_id']\n",
    "            \n",
    "            # Delete all files for this session\n",
    "            files_to_delete = [\n",
    "                f\"{session_id}_chunks.json\",\n",
    "                f\"{session_id}_chunks.pkl\", \n",
    "                f\"{session_id}_search_index.pkl\"\n",
    "            ]\n",
    "            \n",
    "            for filename in files_to_delete:\n",
    "                file_path = self.storage_path / filename\n",
    "                if file_path.exists():\n",
    "                    file_path.unlink()\n",
    "                    print(f\"   üóëÔ∏è  Deleted: {filename}\")\n",
    "        \n",
    "        print(f\"‚úÖ Cleanup complete, kept {keep_latest} latest sessions\")\n",
    "\n",
    "# Test the storage system\n",
    "print(\"\\nüß™ Testing Simple Storage System...\")\n",
    "\n",
    "# Create storage manager\n",
    "storage = SimpleStorageManager(config)\n",
    "\n",
    "# Test with our previous chunking example - create some test data\n",
    "test_chunks_for_storage = [\n",
    "    {\n",
    "        'content': 'Azure Virtual Networks (VNets) provide the foundation for private networking in Azure.',\n",
    "        'chunk_id': 0,\n",
    "        'source': 'Azure VNet Documentation',\n",
    "        'char_start': 0,\n",
    "        'char_end': 85,\n",
    "        'char_count': 85,\n",
    "        'word_count': 14\n",
    "    },\n",
    "    {\n",
    "        'content': 'Network Security Groups (NSGs) act as virtual firewalls for your Azure resources.',\n",
    "        'chunk_id': 1,\n",
    "        'source': 'Azure Security Documentation',\n",
    "        'char_start': 0,\n",
    "        'char_end': 82,\n",
    "        'char_count': 82,\n",
    "        'word_count': 13\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test saving chunks\n",
    "print(f\"\\nüíæ Testing chunk storage...\")\n",
    "session_id = storage.save_chunks(test_chunks_for_storage, \"Test Azure Documentation\")\n",
    "\n",
    "# Test loading chunks\n",
    "print(f\"\\nüìö Testing chunk loading...\")\n",
    "loaded_chunks, metadata = storage.load_chunks(session_id)\n",
    "\n",
    "if loaded_chunks:\n",
    "    print(f\"‚úÖ Successfully loaded {len(loaded_chunks)} chunks\")\n",
    "    \n",
    "    # Test with search system\n",
    "    print(f\"\\nüîç Testing integration with search system...\")\n",
    "    test_searcher = BasicTextSearcher(config)\n",
    "    test_searcher.add_chunks(loaded_chunks)\n",
    "    \n",
    "    # Save search index\n",
    "    storage.save_search_index(test_searcher, session_id)\n",
    "    \n",
    "    # Test search\n",
    "    results = test_searcher.search(\"Azure network\")\n",
    "    if results:\n",
    "        print(f\"‚úÖ Search works with loaded chunks!\")\n",
    "        print(f\"   üéØ Found {len(results)} results for 'Azure network'\")\n",
    "\n",
    "# List all sessions\n",
    "print(f\"\\nüìã Listing all saved sessions...\")\n",
    "storage.list_saved_sessions()\n",
    "\n",
    "print(f\"\\n‚úÖ Simple Storage System Complete!\")\n",
    "print(f\"üéØ Foundation notebook ready - all components working together!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5e2b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c61cf8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942cb6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9783022d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61442898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a090e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf2c0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1708f84d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc95a567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b85bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
