{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ff2c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "0a00e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "cfaab6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Azure RAG Project: ~/azure-multimodal-rag\n",
      "📍 Current Working Directory: azure-multimodal-rag\n",
      "🎯 Azure RAG Project: ~/projects/azure-multimodal-rag\n",
      "📍 Working in: azure-multimodal-rag\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Get your home directory automatically\n",
    "HOME = Path.home()\n",
    "project_name = \"azure-multimodal-rag\"\n",
    "AZURE_RAG_PROJECT = HOME / \"projects\" / project_name\n",
    "\n",
    "print(f\"🎯 Azure RAG Project: ~/{project_name}\")\n",
    "print(f\"📍 Current Working Directory: {Path.cwd().name}\")\n",
    "\n",
    "# Create and navigate\n",
    "AZURE_RAG_PROJECT.mkdir(parents=True, exist_ok=True)\n",
    "os.chdir(AZURE_RAG_PROJECT)\n",
    "\n",
    "# Safe display\n",
    "print(f\"🎯 Azure RAG Project: ~/projects/azure-multimodal-rag\")\n",
    "print(f\"📍 Working in: {AZURE_RAG_PROJECT.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "74a1e384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Azure RAG Project: ~/projects/azure-multimodal-rag\n",
      "📍 Working directory: azure-multimodal-rag\n",
      "📁 Creating directory structure...\n",
      "   📁 config\n",
      "   📁 models\n",
      "   📁 data/raw/pdfs\n",
      "   📁 data/processed\n",
      "   📁 data/vector_store\n",
      "   📁 src/document_processor\n",
      "   📁 src/vector_store\n",
      "   📁 src/retrieval\n",
      "   📁 src/generation\n",
      "   📁 src/utils\n",
      "   📁 logs\n",
      "   📁 notebooks\n",
      "   📁 scripts\n",
      "   📁 tests\n",
      "✅ Project structure created\n",
      "📍 All files created in: ~/azure-multimodal-rag/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up logging\n",
    "# Create logs directory\n",
    "Path(\"logs\").mkdir(exist_ok=True)\n",
    "\n",
    "# Configure to save to file\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('logs/app.log'),  # Save to file\n",
    "        logging.StreamHandler()               # Also show on screen\n",
    "    ]\n",
    ")\n",
    "\n",
    "# SECURE Project configuration - no personal paths exposed\n",
    "HOME = Path.home()  # Gets your home directory automatically\n",
    "PROJECT_NAME = \"azure-multimodal-rag\"\n",
    "AZURE_RAG_PROJECT = HOME / \"projects\" / PROJECT_NAME\n",
    "\n",
    "# Safe display function\n",
    "def safe_display(path):\n",
    "    \"\"\"Display path without exposing personal directory\"\"\"\n",
    "    return str(path).replace(str(HOME), \"~\")\n",
    "\n",
    "print(f\"🎯 Azure RAG Project: {safe_display(AZURE_RAG_PROJECT)}\")\n",
    "\n",
    "# Create project directory and add to path\n",
    "AZURE_RAG_PROJECT.mkdir(parents=True, exist_ok=True)\n",
    "if str(AZURE_RAG_PROJECT) not in sys.path:\n",
    "    sys.path.append(str(AZURE_RAG_PROJECT))\n",
    "\n",
    "# Change to project directory for file creation\n",
    "os.chdir(AZURE_RAG_PROJECT)\n",
    "print(f\"📍 Working directory: {AZURE_RAG_PROJECT.name}\")\n",
    "\n",
    "# Create complete directory structure\n",
    "directories = [\n",
    "    \"config\", \"models\", \"data/raw/pdfs\", \"data/processed\", \"data/vector_store\",\n",
    "    \"src/document_processor\", \"src/vector_store\", \"src/retrieval\", \n",
    "    \"src/generation\", \"src/utils\", \"logs\", \"notebooks\", \"scripts\", \"tests\"\n",
    "]\n",
    "\n",
    "print(\"📁 Creating directory structure...\")\n",
    "for directory in directories:\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"   📁 {directory}\")\n",
    "\n",
    "# Create __init__.py files\n",
    "init_files = [\n",
    "    \"src/__init__.py\", \"src/document_processor/__init__.py\", \n",
    "    \"src/vector_store/__init__.py\", \"src/retrieval/__init__.py\",\n",
    "    \"src/generation/__init__.py\", \"src/utils/__init__.py\", \"config/__init__.py\"\n",
    "]\n",
    "\n",
    "for init_file in init_files:\n",
    "    Path(init_file).touch()\n",
    "\n",
    "print(\"✅ Project structure created\")\n",
    "print(f\"📍 All files created in: ~/{PROJECT_NAME}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af75c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "526e4c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Creating Configuration System...\n",
      "✅ Configuration System Created!\n",
      "\n",
      "📋 Current Settings:\n",
      "   📁 PDF Folder: data/raw/pdfs\n",
      "   📁 Processed Folder: data/processed\n",
      "   📁 Vector Store: data/vector_store\n",
      "   📝 Chunk Size: 1000 characters\n",
      "   🔄 Chunk Overlap: 200 characters (20% overlap)\n",
      "   🤖 Embedding Model: all-MiniLM-L6-v2\n",
      "   💾 Max Memory: 8 GB\n",
      "   📊 Max Files: 10\n",
      "   📄 Supported Types: .pdf, .txt, .md\n",
      "\n",
      "💾 Configuration saved to: config/settings.py\n"
     ]
    }
   ],
   "source": [
    "# Cell: Configuration System\n",
    "print(\"⚙️ Creating Configuration System...\")\n",
    "\n",
    "class AzureRAGConfig:\n",
    "    \"\"\"Configuration for our Azure RAG system\"\"\"\n",
    "    \n",
    "    # File paths\n",
    "    PDF_FOLDER = \"data/raw/pdfs\"\n",
    "    PROCESSED_FOLDER = \"data/processed\"\n",
    "    VECTOR_STORE_FOLDER = \"data/vector_store\"\n",
    "    LOGS_FOLDER = \"logs\"\n",
    "    \n",
    "    # Text processing settings\n",
    "    CHUNK_SIZE = 1000        # characters - good balance\n",
    "    CHUNK_OVERLAP = 200      # 20% overlap for context\n",
    "    \n",
    "    # AI model settings\n",
    "    EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"  # For creating embeddings\n",
    "    MAX_MEMORY_GB = 8        # Memory limit\n",
    "    \n",
    "    # Processing limits\n",
    "    MAX_FILES_TO_PROCESS = 10  # Don't overwhelm the system\n",
    "    \n",
    "    # Supported file types\n",
    "    SUPPORTED_FILE_TYPES = [\".pdf\", \".txt\", \".md\"]\n",
    "\n",
    "# Create our configuration instance\n",
    "config = AzureRAGConfig()\n",
    "\n",
    "# Test and display our configuration\n",
    "print(\"✅ Configuration System Created!\")\n",
    "print(\"\\n📋 Current Settings:\")\n",
    "print(f\"   📁 PDF Folder: {config.PDF_FOLDER}\")\n",
    "print(f\"   📁 Processed Folder: {config.PROCESSED_FOLDER}\")\n",
    "print(f\"   📁 Vector Store: {config.VECTOR_STORE_FOLDER}\")\n",
    "print(f\"   📝 Chunk Size: {config.CHUNK_SIZE} characters\")\n",
    "print(f\"   🔄 Chunk Overlap: {config.CHUNK_OVERLAP} characters ({config.CHUNK_OVERLAP/config.CHUNK_SIZE*100:.0f}% overlap)\")\n",
    "print(f\"   🤖 Embedding Model: {config.EMBEDDING_MODEL}\")\n",
    "print(f\"   💾 Max Memory: {config.MAX_MEMORY_GB} GB\")\n",
    "print(f\"   📊 Max Files: {config.MAX_FILES_TO_PROCESS}\")\n",
    "print(f\"   📄 Supported Types: {', '.join(config.SUPPORTED_FILE_TYPES)}\")\n",
    "\n",
    "# Save configuration to file for later use\n",
    "config_file_content = f'''\"\"\"Azure RAG Configuration Settings\"\"\"\n",
    "\n",
    "class AzureRAGConfig:\n",
    "    \"\"\"Configuration for our Azure RAG system\"\"\"\n",
    "    \n",
    "    # File paths\n",
    "    PDF_FOLDER = \"{config.PDF_FOLDER}\"\n",
    "    PROCESSED_FOLDER = \"{config.PROCESSED_FOLDER}\"\n",
    "    VECTOR_STORE_FOLDER = \"{config.VECTOR_STORE_FOLDER}\"\n",
    "    LOGS_FOLDER = \"{config.LOGS_FOLDER}\"\n",
    "    \n",
    "    # Text processing settings\n",
    "    CHUNK_SIZE = {config.CHUNK_SIZE}\n",
    "    CHUNK_OVERLAP = {config.CHUNK_OVERLAP}\n",
    "    \n",
    "    # AI model settings\n",
    "    EMBEDDING_MODEL = \"{config.EMBEDDING_MODEL}\"\n",
    "    MAX_MEMORY_GB = {config.MAX_MEMORY_GB}\n",
    "    \n",
    "    # Processing limits\n",
    "    MAX_FILES_TO_PROCESS = {config.MAX_FILES_TO_PROCESS}\n",
    "    \n",
    "    # Supported file types\n",
    "    SUPPORTED_FILE_TYPES = {config.SUPPORTED_FILE_TYPES}\n",
    "\n",
    "# Global config instance\n",
    "config = AzureRAGConfig()\n",
    "'''\n",
    "\n",
    "# Write to config file\n",
    "with open('config/settings.py', 'w') as f:\n",
    "    f.write(config_file_content)\n",
    "\n",
    "print(f\"\\n💾 Configuration saved to: config/settings.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b168109a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24088732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5f9b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "b3e3ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's create a corrected version that works with our config\n",
    "class SimplePDFReader:\n",
    "    \"\"\"Simple PDF reader using PyMuPDF\"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        \"\"\"Initialize with optional config\"\"\"\n",
    "        self.config = config\n",
    "        \n",
    "    def read_pdf(self, pdf_path):\n",
    "        \"\"\"Read a PDF file and return text content\"\"\"\n",
    "        try:\n",
    "            import fitz  # PyMuPDF\n",
    "            \n",
    "            # Open the PDF\n",
    "            doc = fitz.open(pdf_path)\n",
    "            text_content = \"\"\n",
    "            \n",
    "            print(f\"📄 Processing {len(doc)} pages...\")\n",
    "            \n",
    "            # Extract text from each page\n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc[page_num]\n",
    "                page_text = page.get_text()\n",
    "                text_content += page_text\n",
    "                \n",
    "                # Show progress for larger documents\n",
    "                if (page_num + 1) % 5 == 0 or page_num == 0:\n",
    "                    print(f\"   📖 Processed page {page_num + 1}/{len(doc)}\")\n",
    "            \n",
    "            doc.close()\n",
    "            return text_content\n",
    "            \n",
    "        except ImportError:\n",
    "            raise Exception(\"PyMuPDF (fitz) not installed. Run: pip install PyMuPDF\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error reading PDF {pdf_path}: {str(e)}\")\n",
    "\n",
    " \n",
    "       \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7154a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "3f567f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Found 2 PDF files:\n",
      "   1. 01-study-guide-az-vnet.pdf\n",
      "   2. 02-study-guide-az-load-balancer.pdf\n",
      "\n",
      "🔍 Testing with: 01-study-guide-az-vnet.pdf\n",
      "📖 Reading 01-study-guide-az-vnet.pdf...\n",
      "📄 Processing 11 pages...\n",
      "   📖 Processed page 1/11\n",
      "   📖 Processed page 5/11\n",
      "   📖 Processed page 10/11\n",
      "\n",
      "✅ Successfully read PDF!\n",
      "📊 Content Analysis:\n",
      "   📝 Total characters: 25,974\n",
      "   📄 Total words: 3,986\n",
      "   📋 Total lines: 518\n",
      "\n",
      "📋 First 5 meaningful lines:\n",
      "   1. Tuesday, November 2, 2021\n",
      "   2. 1\n",
      "   3. Capabilities of Azure Virtual Networks\n",
      "   4. Azure VNets enable resources in Azure to securely communicate with each other, the internet,\n",
      "   5. and on-premises networks.\n",
      "\n",
      "📄 Sample chunk (first 1000 characters):\n",
      "'Tuesday, November 2, 2021 \n",
      "1 \n",
      " \n",
      " \n",
      "Capabilities of Azure Virtual Networks \n",
      "Azure VNets enable resources in Azure to securely communicate with each other, the internet, \n",
      "and on-premises networks. \n",
      " \n",
      "Communication with the internet. All resources in a VNet can communicate outbound to the \n",
      "internet, by default. You can communicate inbound to a resource by assigning a public IP \n",
      "address or a public Load Balancer. You can also use public IP or public Load Balancer to manage \n",
      "your outbound connections. \n",
      "Communication between Azure resources. There are three key mechanisms through which \n",
      "Azure resource can communicate: VNets, VNet service endpoints, and VNet peering. Virtual \n",
      "Networks can connect not only virtual machines (VMs), but other Azure Resources, such as the \n",
      "App Service Environment, Azure Kubernetes Service, and Azure Virtual Machine Scale Sets. You \n",
      "can use service endpoints to connect to other Azure resource types, such as Azure SQL \n",
      "databases and storage accounts. When you create...'\n",
      "\n",
      "🧮 Chunking Analysis:\n",
      "   📦 Estimated chunks: 32\n",
      "   📏 Chunk size: 1,000 characters\n",
      "   🔄 Overlap: 200 characters (20.0%)\n",
      "   💾 Memory per chunk: ~3.9 KB\n",
      "\n",
      "✅ Content looks good - contains Azure networking terms!\n",
      "\n",
      "🎯 Next step: Text Chunking System!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pdf_files = list(Path(config.PDF_FOLDER).glob(\"*.pdf\"))\n",
    "print(f\"📄 Found {len(pdf_files)} PDF files:\")\n",
    "\n",
    "for i, pdf_file in enumerate(pdf_files, 1):\n",
    "    print(f\"   {i}. {pdf_file.name}\")\n",
    "\n",
    "if pdf_files:\n",
    "    # Test with the first PDF\n",
    "    test_pdf = pdf_files[0]\n",
    "    print(f\"\\n🔍 Testing with: {test_pdf.name}\")\n",
    "    \n",
    "    try:\n",
    "        # Create PDF reader instance with config\n",
    "        pdf_reader = SimplePDFReader(config=config)\n",
    "        \n",
    "        # Read the PDF\n",
    "        print(f\"📖 Reading {test_pdf.name}...\")\n",
    "        text_content = pdf_reader.read_pdf(str(test_pdf))\n",
    "        \n",
    "        # Analyze what we got\n",
    "        print(f\"\\n✅ Successfully read PDF!\")\n",
    "        print(f\"📊 Content Analysis:\")\n",
    "        print(f\"   📝 Total characters: {len(text_content):,}\")\n",
    "        print(f\"   📄 Total words: {len(text_content.split()):,}\")  \n",
    "        print(f\"   📋 Total lines: {len(text_content.splitlines()):,}\")\n",
    "        \n",
    "        # Clean and show first few meaningful lines\n",
    "        lines = [line.strip() for line in text_content.splitlines() if line.strip()]\n",
    "        print(f\"\\n📋 First 5 meaningful lines:\")\n",
    "        for i, line in enumerate(lines[:5], 1):\n",
    "            display_line = line[:100] + \"...\" if len(line) > 100 else line\n",
    "            print(f\"   {i}. {display_line}\")\n",
    "        \n",
    "        # Show a sample chunk\n",
    "        print(f\"\\n📄 Sample chunk (first {config.CHUNK_SIZE} characters):\")\n",
    "        sample_chunk = text_content[:config.CHUNK_SIZE].strip()\n",
    "        print(f\"'{sample_chunk}...'\")\n",
    "        \n",
    "        # Calculate chunking statistics\n",
    "        total_chars = len(text_content)\n",
    "        chunk_size = config.CHUNK_SIZE\n",
    "        chunk_overlap = config.CHUNK_OVERLAP\n",
    "        \n",
    "        # Estimate chunks (accounting for overlap)\n",
    "        estimated_chunks = max(1, (total_chars - chunk_overlap) // (chunk_size - chunk_overlap))\n",
    "        \n",
    "        print(f\"\\n🧮 Chunking Analysis:\")\n",
    "        print(f\"   📦 Estimated chunks: {estimated_chunks}\")\n",
    "        print(f\"   📏 Chunk size: {chunk_size:,} characters\")\n",
    "        print(f\"   🔄 Overlap: {chunk_overlap:,} characters ({chunk_overlap/chunk_size*100:.1f}%)\")\n",
    "        print(f\"   💾 Memory per chunk: ~{chunk_size * 4 / 1024:.1f} KB\")  # Rough estimate\n",
    "        \n",
    "        # Content quality check\n",
    "        if len(text_content.strip()) < 100:\n",
    "            print(f\"\\n⚠️  Warning: Very short content - PDF might be image-based\")\n",
    "        elif \"azure\" in text_content.lower() or \"network\" in text_content.lower():\n",
    "            print(f\"\\n✅ Content looks good - contains Azure networking terms!\")\n",
    "        else:\n",
    "            print(f\"\\n🤔 Content extracted but may need review\")\n",
    "            \n",
    "        print(f\"\\n🎯 Next step: Text Chunking System!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        print(f\"🔧 Debug info:\")\n",
    "        print(f\"   📁 PDF path: {test_pdf}\")\n",
    "        print(f\"   📏 File size: {test_pdf.stat().st_size:,} bytes\")\n",
    "        print(f\"   🔍 File exists: {test_pdf.exists()}\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No PDF files found!\")\n",
    "    print(f\"📁 Looking in: {Path(config.PDF_FOLDER).absolute()}\")\n",
    "    print(\"💡 Add some PDF files to continue testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dcb179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bff63ce",
   "metadata": {},
   "source": [
    "**Smart Text Chunking System Overview**\n",
    "\n",
    "**What This Code Does:**\n",
    "\n",
    "* **Initializes `SmartTextChunker`** with configurable chunk size and overlap, reporting setup details.\n",
    "* **Cleans input text** by normalizing whitespace and paragraph breaks.\n",
    "* **Finds smart split points** near ideal chunk boundaries, preferring sentence endings or paragraph breaks.\n",
    "* **Generates overlapping chunks** of roughly `CHUNK_SIZE` characters with `CHUNK_OVERLAP`, enforcing a minimum forward progress to avoid tiny fragments.\n",
    "* **Assigns metadata** (`chunk_id`, start/end offsets, character and word counts, source) to each chunk.\n",
    "* **Analyzes chunk statistics** (total count, size range, averages) for quality control.\n",
    "* **Includes safety checks** to prevent infinite loops or excessive chunk counts.\n",
    "* **Test harness** demonstrates the fixed algorithm on sample Azure networking text and reports before/after metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "e8a6e7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Building Text Chunking System...\n",
      "\n",
      "🧪 Testing FIXED Text Chunking System...\n",
      "🎯 Chunker initialized:\n",
      "   📏 Chunk size: 1000 characters\n",
      "   🔄 Overlap: 200 characters (20.0%)\n",
      "\n",
      "📝 Sample text length: 1,208 characters\n",
      "📝 Processing text: 1,202 characters\n",
      "📦 Starting chunking process...\n",
      "   📦 Chunk 1: 0-955 (955 chars)\n",
      "   📦 Chunk 2: 755-1202 (447 chars)\n",
      "✅ Chunking complete: 2 chunks created\n",
      "\n",
      "📊 FIXED Chunking Results:\n",
      "   📦 Total chunks created: 2 (was 202!)\n",
      "   📏 Average chunk size: 701 characters\n",
      "   📄 Average words per chunk: 110 words\n",
      "   📐 Size range: 447-955 characters\n",
      "\n",
      "🔍 Generated Chunks:\n",
      "\n",
      "📦 Chunk 0:\n",
      "   📏 Length: 955 chars, 150 words\n",
      "   📍 Position: 0-955\n",
      "   📝 Preview: Azure Virtual Networks (VNets) provide the foundation for your private network in Azure. VNets enable Azure resources, like Azure Virtual Machines (VM...\n",
      "   🔄 Overlap with next chunk: 400 characters\n",
      "\n",
      "📦 Chunk 1:\n",
      "   📏 Length: 447 chars, 71 words\n",
      "   📍 Position: 755-1202\n",
      "   📝 Preview: ups (NSGs) contain security rules that allow or deny inbound or outbound network traffic to several types of Azure resources. For each rule, you can s...\n",
      "\n",
      "🧪 Testing with REAL Azure PDF content...\n",
      "📄 PDF content: 25,974 characters\n",
      "📝 Processing text: 25,370 characters\n",
      "📦 Starting chunking process...\n",
      "   📦 Chunk 1: 0-963 (963 chars)\n",
      "   📦 Chunk 2: 763-1668 (905 chars)\n",
      "   📦 Chunk 3: 1513-2513 (999 chars)\n",
      "✅ Chunking complete: 33 chunks created\n",
      "\n",
      "📊 Real PDF Chunking Results:\n",
      "   📦 Total chunks: 33\n",
      "   📏 Average size: 953 characters\n",
      "   📄 Average words: 149 words\n",
      "   📐 Size range: 685-1000 characters\n",
      "\n",
      "🎯 Chunking Analysis:\n",
      "   📈 Expected chunks: ~25\n",
      "   ✅ Actual chunks: 33\n",
      "   📊 Efficiency: 75.8% of expected\n",
      "\n",
      "📊 BEFORE vs AFTER:\n",
      "   🔴 BROKEN: 202 tiny chunks, avg 106 chars\n",
      "   ✅ FIXED:  2 proper chunks, avg 701 chars\n",
      "\n",
      "✅ Text Chunking System FIXED and Ready!\n",
      "🎯 Next: Test with real PDF content!\n"
     ]
    }
   ],
   "source": [
    "# Cell: Text Chunking System - UPDATED AND FIXED\n",
    "print(\"📦 Building Text Chunking System...\")\n",
    "\n",
    "class SmartTextChunker:\n",
    "    \"\"\"Intelligent text chunking for Azure RAG system\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initialize with configuration settings\"\"\"\n",
    "        self.config = config\n",
    "        self.chunk_size = config.CHUNK_SIZE\n",
    "        self.chunk_overlap = config.CHUNK_OVERLAP\n",
    "        \n",
    "        print(f\"🎯 Chunker initialized:\")\n",
    "        print(f\"   📏 Chunk size: {self.chunk_size} characters\")\n",
    "        print(f\"   🔄 Overlap: {self.chunk_overlap} characters ({self.chunk_overlap/self.chunk_size*100:.1f}%)\")\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text before chunking\"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        import re\n",
    "        \n",
    "        # Replace multiple spaces with single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove excessive line breaks (keep paragraph breaks)\n",
    "        text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)\n",
    "        \n",
    "        # Strip leading/trailing whitespace\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def find_sentence_boundary(self, text, ideal_position):\n",
    "        \"\"\"Find the best place to split text (prefer sentence endings)\"\"\"\n",
    "        # Don't search past the end of text\n",
    "        if ideal_position >= len(text):\n",
    "            return len(text)\n",
    "            \n",
    "        # Look for sentence endings near the ideal position\n",
    "        search_range = min(100, len(text) - ideal_position)  # Don't go past text end\n",
    "        \n",
    "        # Search backwards from ideal position for sentence endings\n",
    "        for i in range(ideal_position, max(0, ideal_position - search_range), -1):\n",
    "            if i < len(text) and text[i] in '.!?':\n",
    "                # Make sure it's not just an abbreviation\n",
    "                if i + 1 < len(text) and text[i + 1] in ' \\n':\n",
    "                    return i + 1\n",
    "        \n",
    "        # If no sentence ending found, look for paragraph breaks\n",
    "        for i in range(ideal_position, max(0, ideal_position - search_range), -1):\n",
    "            if i < len(text) and text[i] == '\\n':\n",
    "                return i + 1\n",
    "        \n",
    "        # If nothing found, use the ideal position\n",
    "        return ideal_position\n",
    "    \n",
    "    def create_chunks(self, text, source_info=\"Unknown\"):\n",
    "        \"\"\"Split text into overlapping chunks with smart boundaries\"\"\"\n",
    "        \n",
    "        # Clean the text first\n",
    "        text = self.clean_text(text)\n",
    "        \n",
    "        print(f\"📝 Processing text: {len(text):,} characters\")\n",
    "        \n",
    "        if len(text) <= self.chunk_size:\n",
    "            # Text is small enough to be one chunk\n",
    "            return [{\n",
    "                'content': text,\n",
    "                'chunk_id': 0,\n",
    "                'source': source_info,\n",
    "                'char_start': 0,\n",
    "                'char_end': len(text),\n",
    "                'char_count': len(text),\n",
    "                'word_count': len(text.split())\n",
    "            }]\n",
    "        \n",
    "        chunks = []\n",
    "        start_pos = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        print(f\"📦 Starting chunking process...\")\n",
    "        \n",
    "        while start_pos < len(text):\n",
    "            # Calculate ideal end position\n",
    "            ideal_end = start_pos + self.chunk_size\n",
    "            \n",
    "            if ideal_end >= len(text):\n",
    "                # Last chunk - take remaining text\n",
    "                end_pos = len(text)\n",
    "            else:\n",
    "                # Find smart boundary\n",
    "                end_pos = self.find_sentence_boundary(text, ideal_end)\n",
    "            \n",
    "            # Extract chunk content\n",
    "            chunk_content = text[start_pos:end_pos].strip()\n",
    "            \n",
    "            if chunk_content:  # Only add non-empty chunks\n",
    "                chunk = {\n",
    "                    'content': chunk_content,\n",
    "                    'chunk_id': chunk_id,\n",
    "                    'source': source_info,\n",
    "                    'char_start': start_pos,\n",
    "                    'char_end': end_pos,\n",
    "                    'char_count': len(chunk_content),\n",
    "                    'word_count': len(chunk_content.split())\n",
    "                }\n",
    "                chunks.append(chunk)\n",
    "                chunk_id += 1\n",
    "                \n",
    "                # Show progress for first few chunks\n",
    "                if chunk_id <= 3:\n",
    "                    print(f\"   📦 Chunk {chunk_id}: {start_pos}-{end_pos} ({len(chunk_content)} chars)\")\n",
    "            \n",
    "            # FIXED: Calculate next starting position with proper overlap\n",
    "            next_start = end_pos - self.chunk_overlap\n",
    "            \n",
    "            # CRITICAL FIX: Ensure meaningful progress to prevent tiny chunks\n",
    "            min_progress = max(self.chunk_size - self.chunk_overlap - 50, 200)  # Minimum meaningful step\n",
    "            \n",
    "            if next_start <= start_pos:\n",
    "                # If overlap is too big, make reasonable progress\n",
    "                next_start = start_pos + min_progress\n",
    "            elif (next_start - start_pos) < min_progress:\n",
    "                # If we're not making enough progress, force a bigger step\n",
    "                next_start = start_pos + min_progress\n",
    "            \n",
    "            # Safety: if we're near the end, just finish\n",
    "            if next_start >= len(text) - 50:\n",
    "                break\n",
    "                \n",
    "            start_pos = next_start\n",
    "            \n",
    "            # Safety check to prevent infinite loops\n",
    "            if chunk_id > 100:  # Reasonable limit for any document\n",
    "                print(f\"⚠️  Safety limit reached at {chunk_id} chunks\")\n",
    "                break\n",
    "        \n",
    "        print(f\"✅ Chunking complete: {len(chunks)} chunks created\")\n",
    "        return chunks\n",
    "    \n",
    "    def analyze_chunks(self, chunks):\n",
    "        \"\"\"Analyze chunk statistics\"\"\"\n",
    "        if not chunks:\n",
    "            return {}\n",
    "        \n",
    "        char_counts = [chunk['char_count'] for chunk in chunks]\n",
    "        word_counts = [chunk['word_count'] for chunk in chunks]\n",
    "        \n",
    "        return {\n",
    "            'total_chunks': len(chunks),\n",
    "            'avg_chars': sum(char_counts) / len(char_counts),\n",
    "            'min_chars': min(char_counts),\n",
    "            'max_chars': max(char_counts),\n",
    "            'avg_words': sum(word_counts) / len(word_counts),\n",
    "            'total_chars': sum(char_counts),\n",
    "            'total_words': sum(word_counts)\n",
    "        }\n",
    "\n",
    "# Test the FIXED chunking system\n",
    "print(\"\\n🧪 Testing FIXED Text Chunking System...\")\n",
    "\n",
    "# Create chunker instance\n",
    "chunker = SmartTextChunker(config)\n",
    "\n",
    "# Test with sample Azure networking text\n",
    "sample_text = \"\"\"\n",
    "Azure Virtual Networks (VNets) provide the foundation for your private network in Azure. VNets enable Azure resources, like Azure Virtual Machines (VMs), to securely communicate with each other, the internet, and on-premises networks.\n",
    "\n",
    "A VNet is similar to a traditional network that you'd operate in your own data center. But it brings with it additional benefits of Azure's infrastructure such as scale, availability, and isolation.\n",
    "\n",
    "Key concepts for VNets include address space, subnets, regions, and subscriptions. The address space is a set of private and public IP addresses that you can use within the VNet. You can divide the address space into multiple subnets and allocate a portion of the VNet's address space to each subnet.\n",
    "\n",
    "Network Security Groups (NSGs) contain security rules that allow or deny inbound or outbound network traffic to several types of Azure resources. For each rule, you can specify source and destination, port, and protocol.\n",
    "\n",
    "Azure Load Balancer operates at layer 4 of the Open Systems Interconnection (OSI) model. It's the single point of contact for clients. Load Balancer distributes inbound flows that arrive at the load balancer's front end to backend pool instances.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\n📝 Sample text length: {len(sample_text):,} characters\")\n",
    "\n",
    "# Create chunks with FIXED algorithm\n",
    "chunks = chunker.create_chunks(sample_text, source_info=\"Azure VNet Study Guide\")\n",
    "\n",
    "# Analyze results\n",
    "stats = chunker.analyze_chunks(chunks)\n",
    "\n",
    "print(f\"\\n📊 FIXED Chunking Results:\")\n",
    "print(f\"   📦 Total chunks created: {stats['total_chunks']} (was 202!)\")\n",
    "print(f\"   📏 Average chunk size: {stats['avg_chars']:.0f} characters\")\n",
    "print(f\"   📄 Average words per chunk: {stats['avg_words']:.0f} words\")\n",
    "print(f\"   📐 Size range: {stats['min_chars']}-{stats['max_chars']} characters\")\n",
    "\n",
    "# Show the chunks\n",
    "print(f\"\\n🔍 Generated Chunks:\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\n📦 Chunk {chunk['chunk_id']}:\")\n",
    "    print(f\"   📏 Length: {chunk['char_count']} chars, {chunk['word_count']} words\")\n",
    "    print(f\"   📍 Position: {chunk['char_start']}-{chunk['char_end']}\")\n",
    "    preview = chunk['content'][:150] + \"...\" if len(chunk['content']) > 150 else chunk['content']\n",
    "    print(f\"   📝 Preview: {preview}\")\n",
    "    \n",
    "    # Show overlap with next chunk\n",
    "    if i < len(chunks) - 1:\n",
    "        next_chunk = chunks[i + 1]\n",
    "        overlap_start = max(chunk['char_start'], next_chunk['char_start'] - config.CHUNK_OVERLAP)\n",
    "        overlap_end = min(chunk['char_end'], next_chunk['char_start'] + config.CHUNK_OVERLAP)\n",
    "        if overlap_end > overlap_start:\n",
    "            print(f\"   🔄 Overlap with next chunk: {overlap_end - overlap_start} characters\")\n",
    "\n",
    "# Test with real PDF content if available\n",
    "if 'text_content' in globals():\n",
    "    print(f\"\\n🧪 Testing with REAL Azure PDF content...\")\n",
    "    print(f\"📄 PDF content: {len(text_content):,} characters\")\n",
    "    \n",
    "    # Create chunks from real PDF\n",
    "    real_chunks = chunker.create_chunks(text_content, source_info=\"Azure VNet Study Guide PDF\")\n",
    "    real_stats = chunker.analyze_chunks(real_chunks)\n",
    "    \n",
    "    print(f\"\\n📊 Real PDF Chunking Results:\")\n",
    "    print(f\"   📦 Total chunks: {real_stats['total_chunks']}\")\n",
    "    print(f\"   📏 Average size: {real_stats['avg_chars']:.0f} characters\")\n",
    "    print(f\"   📄 Average words: {real_stats['avg_words']:.0f} words\")\n",
    "    print(f\"   📐 Size range: {real_stats['min_chars']}-{real_stats['max_chars']} characters\")\n",
    "    \n",
    "    # Expected vs actual\n",
    "    expected_chunks = len(text_content) // config.CHUNK_SIZE\n",
    "    print(f\"\\n🎯 Chunking Analysis:\")\n",
    "    print(f\"   📈 Expected chunks: ~{expected_chunks}\")\n",
    "    print(f\"   ✅ Actual chunks: {real_stats['total_chunks']}\")\n",
    "    print(f\"   📊 Efficiency: {expected_chunks/real_stats['total_chunks']*100:.1f}% of expected\")\n",
    "\n",
    "print(f\"\\n📊 BEFORE vs AFTER:\")\n",
    "print(f\"   🔴 BROKEN: 202 tiny chunks, avg 106 chars\")\n",
    "print(f\"   ✅ FIXED:  {stats['total_chunks']} proper chunks, avg {stats['avg_chars']:.0f} chars\")\n",
    "\n",
    "print(f\"\\n✅ Text Chunking System FIXED and Ready!\")\n",
    "print(f\"🎯 Next: Test with real PDF content!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c796f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74b6445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e0c9a1b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d80a147",
   "metadata": {},
   "source": [
    "**Basic Text Search System Overview**\n",
    "\n",
    "* **Defines a `BasicTextSearcher` class** that ingests text “chunks” and builds an inverted index mapping keywords to their source passages.\n",
    "* **Tokenizes content** using regular expressions and avoids duplicate entries in the index for each chunk.\n",
    "* **Implements a relevance scoring** method combining term frequency, exact-phrase matches, and domain-specific bonuses for Azure-related terms.\n",
    "* **Provides a `search()` method** that retrieves, scores, and ranks matching chunks based on the query, returning the top results.\n",
    "* **Includes methods for adding new chunks** (`add_chunks`) and rebuilding the index incrementally.\n",
    "* **Offers a `get_search_statistics()` method** to report total chunks, indexed terms, average/min/max chunk sizes, source distribution, and a rough memory estimate.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "013cb47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Building Basic Text Search System...\n",
      "\n",
      "🧪 Testing Basic Search System...\n",
      "🎯 Search system initialized\n",
      "   🔍 Ready to index chunks\n",
      "   📊 Will track search statistics\n",
      "📚 Added 4 chunks to search index\n",
      "   📦 Total chunks in system: 4\n",
      "   🔑 Index contains 68 unique terms\n",
      "\n",
      "🎯 Testing Search Queries:\n",
      "\n",
      "🔍 Searching for: 'network security'\n",
      "   📝 Query terms: ['network', 'security']\n",
      "   📦 Found 4 candidate chunks\n",
      "   ✅ Returning top 3 results\n",
      "      1. Score: 1.600 | Terms: ['network', 'security']\n",
      "      2. Score: 1.430 | Terms: ['network', 'security']\n",
      "      3. Score: 0.483 | Terms: ['network']\n",
      "\n",
      "📊 Results for 'network security':\n",
      "   1. [1.600] Azure VNet Guide\n",
      "      📝 Azure Virtual Networks (VNets) provide isolated networking environments in Azure. You can configure ...\n",
      "      🎯 Matched: ['network', 'security']\n",
      "   2. [1.430] Azure Security Guide\n",
      "      📝 Network Security Groups (NSGs) contain security rules that control inbound and outbound traffic to A...\n",
      "      🎯 Matched: ['network', 'security']\n",
      "   3. [0.483] Azure VNet Guide\n",
      "      📝 Subnets allow you to segment your Azure virtual network into smaller networks. Each subnet must be a...\n",
      "      🎯 Matched: ['network']\n",
      "\n",
      "🔍 Searching for: 'load balancer'\n",
      "   📝 Query terms: ['load', 'balancer']\n",
      "   📦 Found 1 candidate chunks\n",
      "   ✅ Returning top 1 results\n",
      "      1. Score: 1.387 | Terms: ['load', 'balancer']\n",
      "\n",
      "📊 Results for 'load balancer':\n",
      "   1. [1.387] Azure Load Balancer Guide\n",
      "      📝 Azure Load Balancer distributes incoming network traffic across multiple virtual machines. It operat...\n",
      "      🎯 Matched: ['load', 'balancer']\n",
      "\n",
      "🔍 Searching for: 'Azure subnet'\n",
      "   📝 Query terms: ['azure', 'subnet']\n",
      "   📦 Found 4 candidate chunks\n",
      "   ✅ Returning top 3 results\n",
      "      1. Score: 0.550 | Terms: ['azure', 'subnet']\n",
      "      2. Score: 0.525 | Terms: ['azure', 'subnet']\n",
      "      3. Score: 0.343 | Terms: ['azure']\n",
      "\n",
      "📊 Results for 'Azure subnet':\n",
      "   1. [0.550] Azure VNet Guide\n",
      "      📝 Azure Virtual Networks (VNets) provide isolated networking environments in Azure. You can configure ...\n",
      "      🎯 Matched: ['azure', 'subnet']\n",
      "   2. [0.525] Azure VNet Guide\n",
      "      📝 Subnets allow you to segment your Azure virtual network into smaller networks. Each subnet must be a...\n",
      "      🎯 Matched: ['azure', 'subnet']\n",
      "   3. [0.343] Azure Security Guide\n",
      "      📝 Network Security Groups (NSGs) contain security rules that control inbound and outbound traffic to A...\n",
      "      🎯 Matched: ['azure']\n",
      "\n",
      "🔍 Searching for: 'virtual machine traffic'\n",
      "   📝 Query terms: ['virtual', 'machine', 'traffic']\n",
      "   📦 Found 4 candidate chunks\n",
      "   ✅ Returning top 3 results\n",
      "      1. Score: 0.450 | Terms: ['virtual']\n",
      "      2. Score: 0.442 | Terms: ['virtual']\n",
      "      3. Score: 0.430 | Terms: ['virtual', 'machine', 'traffic']\n",
      "\n",
      "📊 Results for 'virtual machine traffic':\n",
      "   1. [0.450] Azure VNet Guide\n",
      "      📝 Azure Virtual Networks (VNets) provide isolated networking environments in Azure. You can configure ...\n",
      "      🎯 Matched: ['virtual']\n",
      "   2. [0.442] Azure VNet Guide\n",
      "      📝 Subnets allow you to segment your Azure virtual network into smaller networks. Each subnet must be a...\n",
      "      🎯 Matched: ['virtual']\n",
      "   3. [0.430] Azure Load Balancer Guide\n",
      "      📝 Azure Load Balancer distributes incoming network traffic across multiple virtual machines. It operat...\n",
      "      🎯 Matched: ['virtual', 'machine', 'traffic']\n",
      "\n",
      "🔍 Searching for: 'NSG rules'\n",
      "   📝 Query terms: ['nsg', 'rules']\n",
      "   📦 Found 1 candidate chunks\n",
      "   ✅ Returning top 1 results\n",
      "      1. Score: 0.387 | Terms: ['nsg', 'rules']\n",
      "\n",
      "📊 Results for 'NSG rules':\n",
      "   1. [0.387] Azure Security Guide\n",
      "      📝 Network Security Groups (NSGs) contain security rules that control inbound and outbound traffic to A...\n",
      "      🎯 Matched: ['nsg', 'rules']\n",
      "\n",
      "📊 Search System Statistics:\n",
      "   📈 total_chunks: 4\n",
      "   📈 total_terms_indexed: 68\n",
      "   📈 avg_chunk_size: 154.25\n",
      "   📈 min_chunk_size: 144\n",
      "   📈 max_chunk_size: 166\n",
      "   📈 sources: {'Azure VNet Guide': 2, 'Azure Security Guide': 1, 'Azure Load Balancer Guide': 1}\n",
      "   📈 memory_estimate_mb: 0.002353668212890625\n",
      "\n",
      "✅ Basic Search System Ready!\n",
      "🎯 Next: Simple Storage System for persistence!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Cell: Basic Text Search System\n",
    "print(\"🔍 Building Basic Text Search System...\")\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class BasicTextSearcher:\n",
    "    \"\"\"Simple but effective text search for Azure RAG foundation\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initialize with configuration\"\"\"\n",
    "        self.config = config\n",
    "        self.chunks = []  # Will store our text chunks\n",
    "        self.search_index = {}  # Simple keyword index\n",
    "        \n",
    "        print(f\"🎯 Search system initialized\")\n",
    "        print(f\"   🔍 Ready to index chunks\")\n",
    "        print(f\"   📊 Will track search statistics\")\n",
    "    \n",
    "    def add_chunks(self, chunks: List[Dict]):\n",
    "        \"\"\"Add chunks to our search system\"\"\"\n",
    "        self.chunks.extend(chunks)\n",
    "        self._build_search_index(chunks)\n",
    "        \n",
    "        print(f\"📚 Added {len(chunks)} chunks to search index\")\n",
    "        print(f\"   📦 Total chunks in system: {len(self.chunks)}\")\n",
    "        print(f\"   🔑 Index contains {len(self.search_index)} unique terms\")\n",
    "    \n",
    "    def _build_search_index(self, chunks: List[Dict]):\n",
    "        \"\"\"Build a simple keyword index for fast searching\"\"\"\n",
    "        for chunk in chunks:\n",
    "            chunk_id = chunk['chunk_id']\n",
    "            text = chunk['content'].lower()\n",
    "            \n",
    "            # Extract words (simple tokenization)\n",
    "            words = re.findall(r'\\b\\w+\\b', text)\n",
    "            \n",
    "            # Add to inverted index\n",
    "            for word in words:\n",
    "                if word not in self.search_index:\n",
    "                    self.search_index[word] = []\n",
    "                \n",
    "                # Only add if not already there (avoid duplicates)\n",
    "                if chunk_id not in [item['chunk_id'] for item in self.search_index[word]]:\n",
    "                    self.search_index[word].append({\n",
    "                        'chunk_id': chunk_id,\n",
    "                        'source': chunk['source']\n",
    "                    })\n",
    "    \n",
    "    def _calculate_relevance_score(self, chunk: Dict, query_terms: List[str]) -> float:\n",
    "        \"\"\"Calculate how relevant a chunk is to the query\"\"\"\n",
    "        content = chunk['content'].lower()\n",
    "        score = 0.0\n",
    "        \n",
    "        # Count term matches\n",
    "        for term in query_terms:\n",
    "            term_count = content.count(term.lower())\n",
    "            if term_count > 0:\n",
    "                # Term frequency score (more mentions = higher score)\n",
    "                tf_score = term_count / len(content.split())\n",
    "                score += tf_score\n",
    "                \n",
    "                # Bonus for exact phrase matches\n",
    "                if len(query_terms) > 1 and ' '.join(query_terms).lower() in content:\n",
    "                    score += 0.5\n",
    "        \n",
    "        # Bonus for Azure-specific terms (domain relevance)\n",
    "        azure_terms = ['azure', 'vnet', 'subnet', 'nsg', 'load balancer', 'network']\n",
    "        azure_bonus = sum(1 for term in azure_terms if term in content) * 0.1\n",
    "        score += azure_bonus\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def search(self, query: str, max_results: int = 5) -> List[Dict]:\n",
    "        \"\"\"Search for chunks matching the query\"\"\"\n",
    "        if not query.strip():\n",
    "            return []\n",
    "        \n",
    "        print(f\"\\n🔍 Searching for: '{query}'\")\n",
    "        \n",
    "        # Prepare query terms\n",
    "        query_terms = re.findall(r'\\b\\w+\\b', query.lower())\n",
    "        print(f\"   📝 Query terms: {query_terms}\")\n",
    "        \n",
    "        # Find candidate chunks\n",
    "        candidate_chunks = set()\n",
    "        \n",
    "        for term in query_terms:\n",
    "            if term in self.search_index:\n",
    "                for item in self.search_index[term]:\n",
    "                    candidate_chunks.add(item['chunk_id'])\n",
    "        \n",
    "        print(f\"   📦 Found {len(candidate_chunks)} candidate chunks\")\n",
    "        \n",
    "        if not candidate_chunks:\n",
    "            print(f\"   ❌ No matches found\")\n",
    "            return []\n",
    "        \n",
    "        # Score and rank chunks\n",
    "        scored_chunks = []\n",
    "        \n",
    "        for chunk_id in candidate_chunks:\n",
    "            # Find the actual chunk\n",
    "            chunk = next((c for c in self.chunks if c['chunk_id'] == chunk_id), None)\n",
    "            if chunk:\n",
    "                score = self._calculate_relevance_score(chunk, query_terms)\n",
    "                if score > 0:\n",
    "                    scored_chunks.append({\n",
    "                        'chunk': chunk,\n",
    "                        'score': score,\n",
    "                        'matched_terms': [term for term in query_terms \n",
    "                                        if term in chunk['content'].lower()]\n",
    "                    })\n",
    "        \n",
    "        # Sort by relevance score (highest first)\n",
    "        scored_chunks.sort(key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        # Return top results\n",
    "        results = scored_chunks[:max_results]\n",
    "        \n",
    "        print(f\"   ✅ Returning top {len(results)} results\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"      {i}. Score: {result['score']:.3f} | Terms: {result['matched_terms']}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_search_statistics(self) -> Dict:\n",
    "        \"\"\"Get statistics about the search system\"\"\"\n",
    "        if not self.chunks:\n",
    "            return {'status': 'empty'}\n",
    "        \n",
    "        # Calculate statistics\n",
    "        chunk_sizes = [len(chunk['content']) for chunk in self.chunks]\n",
    "        sources = [chunk['source'] for chunk in self.chunks]\n",
    "        source_counts = Counter(sources)\n",
    "        \n",
    "        return {\n",
    "            'total_chunks': len(self.chunks),\n",
    "            'total_terms_indexed': len(self.search_index),\n",
    "            'avg_chunk_size': sum(chunk_sizes) / len(chunk_sizes),\n",
    "            'min_chunk_size': min(chunk_sizes),\n",
    "            'max_chunk_size': max(chunk_sizes),\n",
    "            'sources': dict(source_counts),\n",
    "            'memory_estimate_mb': (sum(chunk_sizes) * 4) / (1024 * 1024)  # Rough estimate\n",
    "        }\n",
    "\n",
    "# Test the search system\n",
    "print(\"\\n🧪 Testing Basic Search System...\")\n",
    "\n",
    "# Create search system\n",
    "searcher = BasicTextSearcher(config)\n",
    "\n",
    "# Create some test chunks (simulate chunked Azure content)\n",
    "test_chunks = [\n",
    "    {\n",
    "        'content': 'Azure Virtual Networks (VNets) provide isolated networking environments in Azure. You can configure subnets, network security groups, and routing tables.',\n",
    "        'chunk_id': 0,\n",
    "        'source': 'Azure VNet Guide',\n",
    "        'char_start': 0,\n",
    "        'char_end': 150,\n",
    "        'char_count': 150,\n",
    "        'word_count': 25\n",
    "    },\n",
    "    {\n",
    "        'content': 'Network Security Groups (NSGs) contain security rules that control inbound and outbound traffic to Azure resources. Each rule specifies protocol, port, and direction.',\n",
    "        'chunk_id': 1,\n",
    "        'source': 'Azure Security Guide', \n",
    "        'char_start': 0,\n",
    "        'char_end': 160,\n",
    "        'char_count': 160,\n",
    "        'word_count': 27\n",
    "    },\n",
    "    {\n",
    "        'content': 'Azure Load Balancer distributes incoming network traffic across multiple virtual machines. It operates at Layer 4 and supports both TCP and UDP protocols.',\n",
    "        'chunk_id': 2,\n",
    "        'source': 'Azure Load Balancer Guide',\n",
    "        'char_start': 0,\n",
    "        'char_end': 155,\n",
    "        'char_count': 155,\n",
    "        'word_count': 26\n",
    "    },\n",
    "    {\n",
    "        'content': 'Subnets allow you to segment your Azure virtual network into smaller networks. Each subnet must be assigned a portion of the VNet address space.',\n",
    "        'chunk_id': 3,\n",
    "        'source': 'Azure VNet Guide',\n",
    "        'char_start': 151,\n",
    "        'char_end': 280,\n",
    "        'char_count': 129,\n",
    "        'word_count': 24\n",
    "    }\n",
    "]\n",
    "\n",
    "# Add chunks to searcher\n",
    "searcher.add_chunks(test_chunks)\n",
    "\n",
    "# Test different search queries\n",
    "test_queries = [\n",
    "    \"network security\",\n",
    "    \"load balancer\",\n",
    "    \"Azure subnet\",\n",
    "    \"virtual machine traffic\",\n",
    "    \"NSG rules\"\n",
    "]\n",
    "\n",
    "print(f\"\\n🎯 Testing Search Queries:\")\n",
    "for query in test_queries:\n",
    "    results = searcher.search(query, max_results=3)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\n📊 Results for '{query}':\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            chunk = result['chunk']\n",
    "            preview = chunk['content'][:100] + \"...\" if len(chunk['content']) > 100 else chunk['content']\n",
    "            print(f\"   {i}. [{result['score']:.3f}] {chunk['source']}\")\n",
    "            print(f\"      📝 {preview}\")\n",
    "            print(f\"      🎯 Matched: {result['matched_terms']}\")\n",
    "    else:\n",
    "        print(f\"\\n❌ No results for '{query}'\")\n",
    "\n",
    "# Show system statistics\n",
    "print(f\"\\n📊 Search System Statistics:\")\n",
    "stats = searcher.get_search_statistics()\n",
    "for key, value in stats.items():\n",
    "    print(f\"   📈 {key}: {value}\")\n",
    "\n",
    "print(f\"\\n✅ Basic Search System Ready!\")\n",
    "print(f\"🎯 Next: Simple Storage System for persistence!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b44cbe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb88468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00a49ff8",
   "metadata": {},
   "source": [
    "**Storage Cleanup and Simple Storage System Implementation**\n",
    "\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "* **`clean_test_storage()` helper**\n",
    "\n",
    "  * Deletes all files matching `session_*` in the configured storage folder to ensure tests start with no leftover data.\n",
    "  * Prints each deleted filename and a confirmation when done.\n",
    "\n",
    "* **`SimpleStorageManager` class**\n",
    "\n",
    "  * **Initialization**:\n",
    "\n",
    "    * Reads `config.PROCESSED_FOLDER`, creates that directory if missing, and logs the storage location.\n",
    "  * **Session ID & Hashing**:\n",
    "\n",
    "    * `_generate_session_id()` → timestamp-based IDs (`session_YYYYMMDD_HHMMSS`).\n",
    "    * `_calculate_content_hash()` → MD5 hash prefix of content for change detection.\n",
    "  * **Chunk Persistence** (`save_chunks` / `load_chunks`):\n",
    "\n",
    "    * `save_chunks()` serializes chunks + metadata to both JSON (human-readable) and pickle (fast-load), returning the session ID.\n",
    "    * `load_chunks()` prefers pickle over JSON, loads chunks and metadata, and logs counts and timestamps.\n",
    "  * **Search-Index Persistence** (`save_search_index` / `load_search_index`):\n",
    "\n",
    "    * `save_search_index()` pickles the inverted index and stats alongside chunk metadata.\n",
    "    * `load_search_index()` retrieves the index pickle and returns it with its metadata.\n",
    "  * **Session Management** (`list_saved_sessions` / `cleanup_old_sessions`):\n",
    "\n",
    "    * `list_saved_sessions()` scans for `*_chunks.json`, reads metadata, and reports which sessions have pickle/index files.\n",
    "    * `cleanup_old_sessions(keep_latest)` sorts sessions by creation time, deletes all but the most recent N, and logs each removal.\n",
    "  * **Test Harness** at the end exercises storage of test chunks, loading them back, integration with `BasicTextSearcher`, and session listing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "378aed13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️ Deleted: session_20250629_124156_search_index.pkl\n",
      "🗑️ Deleted: session_20250629_124156_chunks.pkl\n",
      "🗑️ Deleted: session_20250629_124156_chunks.json\n",
      "✅ Storage cleaned - ready for fresh test\n",
      "💾 Building Simple Storage System...\n",
      "\n",
      "🧪 Testing Simple Storage System...\n",
      "💾 Storage system initialized\n",
      "   📁 Storage location: data/processed\n",
      "   🔧 Ready to save/load chunks and search indices\n",
      "\n",
      "💾 Testing chunk storage...\n",
      "✅ Saved 2 chunks:\n",
      "   📄 JSON: session_20250629_125835_chunks.json\n",
      "   🚀 Pickle: session_20250629_125835_chunks.pkl\n",
      "   🔖 Session ID: session_20250629_125835\n",
      "\n",
      "📚 Testing chunk loading...\n",
      "📚 Loading from pickle: session_20250629_125835_chunks.pkl\n",
      "✅ Loaded 2 chunks from Test Azure Documentation\n",
      "   📅 Created: 2025-06-29T12:58:35.646276\n",
      "   📊 Total chars: 167\n",
      "   📄 Total words: 27\n",
      "✅ Successfully loaded 2 chunks\n",
      "\n",
      "🔍 Testing integration with search system...\n",
      "🎯 Search system initialized\n",
      "   🔍 Ready to index chunks\n",
      "   📊 Will track search statistics\n",
      "📚 Added 2 chunks to search index\n",
      "   📦 Total chunks in system: 2\n",
      "   🔑 Index contains 20 unique terms\n",
      "✅ Saved search index:\n",
      "   🔍 File: session_20250629_125835_search_index.pkl\n",
      "   📊 Terms indexed: 20\n",
      "   📦 Chunks indexed: 2\n",
      "\n",
      "🔍 Searching for: 'Azure network'\n",
      "   📝 Query terms: ['azure', 'network']\n",
      "   📦 Found 2 candidate chunks\n",
      "   ✅ Returning top 2 results\n",
      "      1. Score: 0.633 | Terms: ['azure', 'network']\n",
      "      2. Score: 0.467 | Terms: ['azure', 'network']\n",
      "✅ Search works with loaded chunks!\n",
      "   🎯 Found 2 results for 'Azure network'\n",
      "\n",
      "📋 Listing all saved sessions...\n",
      "📚 Found 1 saved sessions:\n",
      "   📄 session_20250629_125835\n",
      "      📚 Source: Test Azure Documentation\n",
      "      📅 Created: 2025-06-29T12:58:35.646276\n",
      "      📦 Chunks: 2\n",
      "      🚀 Pickle: ✅\n",
      "      🔍 Index: ✅\n",
      "\n",
      "✅ Simple Storage System Complete!\n",
      "🎯 Foundation notebook ready - all components working together!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Add this to the beginning of your storage test\n",
    "def clean_test_storage():\n",
    "    \"\"\"Clean up all test files before creating new ones\"\"\"\n",
    "    storage_path = Path(config.PROCESSED_FOLDER)\n",
    "    test_files = list(storage_path.glob(\"session_*\"))\n",
    "    \n",
    "    for file in test_files:\n",
    "        file.unlink()\n",
    "        print(f\"🗑️ Deleted: {file.name}\")\n",
    "    \n",
    "    print(f\"✅ Storage cleaned - ready for fresh test\")\n",
    "\n",
    "# Use it like this:\n",
    "clean_test_storage()\n",
    "# Then run your normal storage test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Cell: Simple Storage System\n",
    "print(\"💾 Building Simple Storage System...\")\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "\n",
    "class SimpleStorageManager:\n",
    "    \"\"\"Simple but robust storage for Azure RAG foundation\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initialize with configuration\"\"\"\n",
    "        self.config = config\n",
    "        self.storage_path = Path(config.PROCESSED_FOLDER)\n",
    "        self.storage_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"💾 Storage system initialized\")\n",
    "        print(f\"   📁 Storage location: {self.storage_path}\")\n",
    "        print(f\"   🔧 Ready to save/load chunks and search indices\")\n",
    "    \n",
    "    def _generate_session_id(self):\n",
    "        \"\"\"Generate unique session ID\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        return f\"session_{timestamp}\"\n",
    "    \n",
    "    def _calculate_content_hash(self, content):\n",
    "        \"\"\"Calculate hash of content for change detection\"\"\"\n",
    "        return hashlib.md5(content.encode('utf-8')).hexdigest()[:8]\n",
    "    \n",
    "    def save_chunks(self, chunks, source_name=\"unknown\", session_id=None):\n",
    "        \"\"\"Save chunks to storage with metadata\"\"\"\n",
    "        if not chunks:\n",
    "            print(\"⚠️  No chunks to save\")\n",
    "            return None\n",
    "        \n",
    "        # Generate session ID if not provided\n",
    "        if session_id is None:\n",
    "            session_id = self._generate_session_id()\n",
    "        \n",
    "        # Prepare storage data\n",
    "        storage_data = {\n",
    "            'metadata': {\n",
    "                'session_id': session_id,\n",
    "                'source_name': source_name,\n",
    "                'created_at': datetime.now().isoformat(),\n",
    "                'total_chunks': len(chunks),\n",
    "                'chunk_size_config': self.config.CHUNK_SIZE,\n",
    "                'chunk_overlap_config': self.config.CHUNK_OVERLAP,\n",
    "                'total_characters': sum(chunk['char_count'] for chunk in chunks),\n",
    "                'total_words': sum(chunk['word_count'] for chunk in chunks)\n",
    "            },\n",
    "            'chunks': chunks\n",
    "        }\n",
    "        \n",
    "        # Save as JSON (human readable)\n",
    "        json_file = self.storage_path / f\"{session_id}_chunks.json\"\n",
    "        with open(json_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(storage_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Save as pickle (faster loading)\n",
    "        pickle_file = self.storage_path / f\"{session_id}_chunks.pkl\"\n",
    "        with open(pickle_file, 'wb') as f:\n",
    "            pickle.dump(storage_data, f)\n",
    "        \n",
    "        print(f\"✅ Saved {len(chunks)} chunks:\")\n",
    "        print(f\"   📄 JSON: {json_file.name}\")\n",
    "        print(f\"   🚀 Pickle: {pickle_file.name}\")\n",
    "        print(f\"   🔖 Session ID: {session_id}\")\n",
    "        \n",
    "        return session_id\n",
    "    \n",
    "    def load_chunks(self, session_id):\n",
    "        \"\"\"Load chunks from storage\"\"\"\n",
    "        pickle_file = self.storage_path / f\"{session_id}_chunks.pkl\"\n",
    "        json_file = self.storage_path / f\"{session_id}_chunks.json\"\n",
    "        \n",
    "        # Try pickle first (faster)\n",
    "        if pickle_file.exists():\n",
    "            print(f\"📚 Loading from pickle: {pickle_file.name}\")\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "        elif json_file.exists():\n",
    "            print(f\"📚 Loading from JSON: {json_file.name}\")\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            print(f\"❌ Session not found: {session_id}\")\n",
    "            return None, None\n",
    "        \n",
    "        chunks = data['chunks']\n",
    "        metadata = data['metadata']\n",
    "        \n",
    "        print(f\"✅ Loaded {len(chunks)} chunks from {metadata['source_name']}\")\n",
    "        print(f\"   📅 Created: {metadata['created_at']}\")\n",
    "        print(f\"   📊 Total chars: {metadata['total_characters']:,}\")\n",
    "        print(f\"   📄 Total words: {metadata['total_words']:,}\")\n",
    "        \n",
    "        return chunks, metadata\n",
    "    \n",
    "    def save_search_index(self, searcher, session_id):\n",
    "        \"\"\"Save search index and statistics\"\"\"\n",
    "        if not hasattr(searcher, 'search_index'):\n",
    "            print(\"⚠️  No search index to save\")\n",
    "            return\n",
    "        \n",
    "        # Prepare index data\n",
    "        index_data = {\n",
    "            'metadata': {\n",
    "                'session_id': session_id,\n",
    "                'created_at': datetime.now().isoformat(),\n",
    "                'total_chunks_indexed': len(searcher.chunks),\n",
    "                'total_terms': len(searcher.search_index),\n",
    "                'statistics': searcher.get_search_statistics()\n",
    "            },\n",
    "            'search_index': searcher.search_index,\n",
    "            'chunks_metadata': [{\n",
    "                'chunk_id': chunk['chunk_id'],\n",
    "                'source': chunk['source'],\n",
    "                'char_count': chunk['char_count']\n",
    "            } for chunk in searcher.chunks]\n",
    "        }\n",
    "        \n",
    "        # Save index\n",
    "        index_file = self.storage_path / f\"{session_id}_search_index.pkl\"\n",
    "        with open(index_file, 'wb') as f:\n",
    "            pickle.dump(index_data, f)\n",
    "        \n",
    "        print(f\"✅ Saved search index:\")\n",
    "        print(f\"   🔍 File: {index_file.name}\")\n",
    "        print(f\"   📊 Terms indexed: {len(searcher.search_index):,}\")\n",
    "        print(f\"   📦 Chunks indexed: {len(searcher.chunks)}\")\n",
    "    \n",
    "    def load_search_index(self, session_id):\n",
    "        \"\"\"Load search index\"\"\"\n",
    "        index_file = self.storage_path / f\"{session_id}_search_index.pkl\"\n",
    "        \n",
    "        if not index_file.exists():\n",
    "            print(f\"❌ Search index not found: {session_id}\")\n",
    "            return None\n",
    "        \n",
    "        with open(index_file, 'rb') as f:\n",
    "            index_data = pickle.load(f)\n",
    "        \n",
    "        metadata = index_data['metadata']\n",
    "        search_index = index_data['search_index']\n",
    "        \n",
    "        print(f\"✅ Loaded search index:\")\n",
    "        print(f\"   📅 Created: {metadata['created_at']}\")\n",
    "        print(f\"   🔍 Terms: {len(search_index):,}\")\n",
    "        print(f\"   📦 Chunks: {metadata['total_chunks_indexed']}\")\n",
    "        \n",
    "        return search_index, metadata\n",
    "    \n",
    "    def list_saved_sessions(self):\n",
    "        \"\"\"List all saved sessions\"\"\"\n",
    "        json_files = list(self.storage_path.glob(\"*_chunks.json\"))\n",
    "        \n",
    "        if not json_files:\n",
    "            print(\"📭 No saved sessions found\")\n",
    "            return []\n",
    "        \n",
    "        sessions = []\n",
    "        print(f\"📚 Found {len(json_files)} saved sessions:\")\n",
    "        \n",
    "        for json_file in sorted(json_files):\n",
    "            try:\n",
    "                with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                metadata = data['metadata']\n",
    "                session_id = metadata['session_id']\n",
    "                \n",
    "                # Check for corresponding files\n",
    "                has_pickle = (self.storage_path / f\"{session_id}_chunks.pkl\").exists()\n",
    "                has_index = (self.storage_path / f\"{session_id}_search_index.pkl\").exists()\n",
    "                \n",
    "                session_info = {\n",
    "                    'session_id': session_id,\n",
    "                    'source_name': metadata['source_name'],\n",
    "                    'created_at': metadata['created_at'],\n",
    "                    'total_chunks': metadata['total_chunks'],\n",
    "                    'has_pickle': has_pickle,\n",
    "                    'has_search_index': has_index\n",
    "                }\n",
    "                \n",
    "                sessions.append(session_info)\n",
    "                \n",
    "                print(f\"   📄 {session_id}\")\n",
    "                print(f\"      📚 Source: {metadata['source_name']}\")\n",
    "                print(f\"      📅 Created: {metadata['created_at']}\")\n",
    "                print(f\"      📦 Chunks: {metadata['total_chunks']}\")\n",
    "                print(f\"      🚀 Pickle: {'✅' if has_pickle else '❌'}\")\n",
    "                print(f\"      🔍 Index: {'✅' if has_index else '❌'}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️  Error reading {json_file.name}: {e}\")\n",
    "        \n",
    "        return sessions\n",
    "    \n",
    "    def cleanup_old_sessions(self, keep_latest=5):\n",
    "        \"\"\"Keep only the latest N sessions\"\"\"\n",
    "        sessions = self.list_saved_sessions()\n",
    "        \n",
    "        if len(sessions) <= keep_latest:\n",
    "            print(f\"📚 Only {len(sessions)} sessions found, keeping all\")\n",
    "            return\n",
    "        \n",
    "        # Sort by creation time and keep latest\n",
    "        sessions.sort(key=lambda x: x['created_at'], reverse=True)\n",
    "        sessions_to_delete = sessions[keep_latest:]\n",
    "        \n",
    "        print(f\"🧹 Cleaning up {len(sessions_to_delete)} old sessions...\")\n",
    "        \n",
    "        for session in sessions_to_delete:\n",
    "            session_id = session['session_id']\n",
    "            \n",
    "            # Delete all files for this session\n",
    "            files_to_delete = [\n",
    "                f\"{session_id}_chunks.json\",\n",
    "                f\"{session_id}_chunks.pkl\", \n",
    "                f\"{session_id}_search_index.pkl\"\n",
    "            ]\n",
    "            \n",
    "            for filename in files_to_delete:\n",
    "                file_path = self.storage_path / filename\n",
    "                if file_path.exists():\n",
    "                    file_path.unlink()\n",
    "                    print(f\"   🗑️  Deleted: {filename}\")\n",
    "        \n",
    "        print(f\"✅ Cleanup complete, kept {keep_latest} latest sessions\")\n",
    "\n",
    "# Test the storage system\n",
    "print(\"\\n🧪 Testing Simple Storage System...\")\n",
    "\n",
    "# Create storage manager\n",
    "storage = SimpleStorageManager(config)\n",
    "\n",
    "# Test with our previous chunking example - create some test data\n",
    "test_chunks_for_storage = [\n",
    "    {\n",
    "        'content': 'Azure Virtual Networks (VNets) provide the foundation for private networking in Azure.',\n",
    "        'chunk_id': 0,\n",
    "        'source': 'Azure VNet Documentation',\n",
    "        'char_start': 0,\n",
    "        'char_end': 85,\n",
    "        'char_count': 85,\n",
    "        'word_count': 14\n",
    "    },\n",
    "    {\n",
    "        'content': 'Network Security Groups (NSGs) act as virtual firewalls for your Azure resources.',\n",
    "        'chunk_id': 1,\n",
    "        'source': 'Azure Security Documentation',\n",
    "        'char_start': 0,\n",
    "        'char_end': 82,\n",
    "        'char_count': 82,\n",
    "        'word_count': 13\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test saving chunks\n",
    "print(f\"\\n💾 Testing chunk storage...\")\n",
    "session_id = storage.save_chunks(test_chunks_for_storage, \"Test Azure Documentation\")\n",
    "\n",
    "# Test loading chunks\n",
    "print(f\"\\n📚 Testing chunk loading...\")\n",
    "loaded_chunks, metadata = storage.load_chunks(session_id)\n",
    "\n",
    "if loaded_chunks:\n",
    "    print(f\"✅ Successfully loaded {len(loaded_chunks)} chunks\")\n",
    "    \n",
    "    # Test with search system\n",
    "    print(f\"\\n🔍 Testing integration with search system...\")\n",
    "    test_searcher = BasicTextSearcher(config)\n",
    "    test_searcher.add_chunks(loaded_chunks)\n",
    "    \n",
    "    # Save search index\n",
    "    storage.save_search_index(test_searcher, session_id)\n",
    "    \n",
    "    # Test search\n",
    "    results = test_searcher.search(\"Azure network\")\n",
    "    if results:\n",
    "        print(f\"✅ Search works with loaded chunks!\")\n",
    "        print(f\"   🎯 Found {len(results)} results for 'Azure network'\")\n",
    "\n",
    "# List all sessions\n",
    "print(f\"\\n📋 Listing all saved sessions...\")\n",
    "storage.list_saved_sessions()\n",
    "\n",
    "print(f\"\\n✅ Simple Storage System Complete!\")\n",
    "print(f\"🎯 Foundation notebook ready - all components working together!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5e2b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c61cf8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942cb6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9783022d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61442898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a090e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf2c0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1708f84d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc95a567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b85bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
