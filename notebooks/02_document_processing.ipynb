{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ff2c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a00e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c61cf8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2991d51",
   "metadata": {},
   "source": [
    "\n",
    "**Session Initialization & Configuration**\n",
    "\n",
    "**What This Code Does:**\n",
    " \n",
    "\n",
    "* **Navigate to project directory**\n",
    "\n",
    "  * Compute  home folder (`HOME`) and define the project path (`AZURE_RAG_PROJECT`)\n",
    "  * Change the current working directory to the project root\n",
    "  * Print the active directory name\n",
    "\n",
    "* **Ensure project is importable**\n",
    "\n",
    "  * Check if the project path is already in `sys.path`\n",
    "  * If not, append it so you can import modules from `azure-multimodal-rag`\n",
    "\n",
    "* **Set up logging**\n",
    "\n",
    "  * Build a timestamped log-file path under `logs/`\n",
    "  * Configure the `logging` module to write INFO-level messages both to file and to the console\n",
    "  * Print the log-file name and confirm the environment is ready\n",
    "\n",
    "* **Load (or create) configuration**\n",
    "\n",
    "  * Attempt to import `config` from your settings module\n",
    "  * If that fails, define a fallback `AzureRAGConfig` class with sane defaults:\n",
    "\n",
    "    * Raw PDF folder, processed data folder\n",
    "    * Chunk size and overlap for text splitting\n",
    "    * Supported file extensions\n",
    "  * Instantiate `config` and confirm\n",
    "\n",
    "* **Display current configuration**\n",
    "\n",
    "  * Print out the active chunk size, overlap, and PDF folder path\n",
    "\n",
    "* **Discover available PDFs**\n",
    "\n",
    "  * Glob all `.pdf` files in `config.PDF_FOLDER`\n",
    "  * Print the number of PDFs found and list each with its file size (in MB)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "942cb6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Working Directory: azure-multimodal-rag\n",
      "📋 Session logged to: doc_processing_20250629_143210.log\n",
      "✅ Environment ready!\n",
      "✅ Configuration loaded from file\n",
      "\n",
      "📊 Current Config:\n",
      "   📏 Chunk Size: 1000\n",
      "   🔄 Overlap: 200\n",
      "   📁 PDF Folder: data/raw/pdfs\n",
      "\n",
      "📄 Available PDFs: 2\n",
      "   📄 01-study-guide-az-vnet.pdf (0.1 MB)\n",
      "   📄 02-study-guide-az-load-balancer.pdf (0.2 MB)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Navigate to project directory\n",
    "HOME = Path.home()\n",
    "project_name = \"azure-multimodal-rag\"\n",
    "AZURE_RAG_PROJECT = HOME / \"projects\" / project_name\n",
    "os.chdir(AZURE_RAG_PROJECT)\n",
    "\n",
    "print(f\"📁 Working Directory: {AZURE_RAG_PROJECT.name}\")\n",
    "\n",
    "# Add to Python path\n",
    "if str(AZURE_RAG_PROJECT) not in sys.path:\n",
    "    sys.path.append(str(AZURE_RAG_PROJECT))\n",
    "\n",
    "# Setup logging for this session\n",
    "log_file = Path(\"logs\") / f\"doc_processing_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"📋 Session logged to: {log_file.name}\")\n",
    "print(\"✅ Environment ready!\")\n",
    "\n",
    "# Load config (recreate if needed)\n",
    "try:\n",
    "    from config.settings import config\n",
    "    print(f\"✅ Configuration loaded from file\")\n",
    "except ImportError:\n",
    "    print(\"⚠️  Creating config...\")\n",
    "    class AzureRAGConfig:\n",
    "        PDF_FOLDER = \"data/raw/pdfs\"\n",
    "        PROCESSED_FOLDER = \"data/processed\"\n",
    "        CHUNK_SIZE = 1000\n",
    "        CHUNK_OVERLAP = 200\n",
    "        SUPPORTED_FILE_TYPES = [\".pdf\", \".txt\", \".md\"]\n",
    "    \n",
    "    config = AzureRAGConfig()\n",
    "    print(f\"✅ Configuration ready\")\n",
    "\n",
    "print(f\"\\n📊 Current Config:\")\n",
    "print(f\"   📏 Chunk Size: {config.CHUNK_SIZE}\")\n",
    "print(f\"   🔄 Overlap: {config.CHUNK_OVERLAP}\")\n",
    "print(f\"   📁 PDF Folder: {config.PDF_FOLDER}\")\n",
    "\n",
    "# Check what PDFs we have\n",
    "pdf_files = list(Path(config.PDF_FOLDER).glob(\"*.pdf\"))\n",
    "print(f\"\\n📄 Available PDFs: {len(pdf_files)}\")\n",
    "for pdf in pdf_files:\n",
    "    size_mb = pdf.stat().st_size / (1024 * 1024)\n",
    "    print(f\"   📄 {pdf.name} ({size_mb:.1f} MB)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9783022d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f86659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4037ce3",
   "metadata": {},
   "source": [
    "\n",
    "**PDF Extraction with Metadata & Structure Analysis**\n",
    "\n",
    "**What This Code Does:**\n",
    "  \n",
    "* **Initialization (`__init__`)**\n",
    "\n",
    "  * Stores the passed-in `config` object for folder paths and settings.\n",
    "  * Configures a class-specific logger (e.g. `__name__.EnhancedPDFReader`) at INFO level.\n",
    "\n",
    "* **`read_pdf_with_metadata(pdf_path)`**\n",
    "\n",
    "  1. **Imports & Validation**:\n",
    "\n",
    "     * Tries to import PyMuPDF (`fitz`) and raises a clear error if missing.\n",
    "  2. **Timing & Logging**:\n",
    "\n",
    "     * Records start time and logs the PDF filename being processed.\n",
    "  3. **Metadata Extraction**:\n",
    "\n",
    "     * Calls `_extract_document_metadata` to pull file stats (size, timestamps) and embedded PDF metadata (title, author, etc.).\n",
    "  4. **Page-Level Content**:\n",
    "\n",
    "     * Invokes `_extract_pages_content` to loop through each page—capturing text, char/word/line counts, densities, and logs progress every 10 pages.\n",
    "  5. **Aggregate Statistics**:\n",
    "\n",
    "     * Merges all page text, then uses `_calculate_content_statistics` to derive totals (chars, words, unique terms), distributions (alphabetic, numeric, punctuation), density ratios, and a simple readability score.\n",
    "  6. **Structure Detection**:\n",
    "\n",
    "     * Runs `_detect_structure_patterns` to spot headings, bullets, code blocks, tables, URLs, and Azure-specific naming patterns, yielding an estimated section count and a “structure confidence” score.\n",
    "  7. **Result Assembly**:\n",
    "\n",
    "     * Closes the document, measures elapsed time, logs a summary line, and returns a dictionary containing:\n",
    "\n",
    "       * `content` (full text)\n",
    "       * `metadata`, `pages`, `content_stats`, `structure_hints`\n",
    "       * `processing_info` (timing, timestamp, version)\n",
    "\n",
    "* **Helper Methods**\n",
    "\n",
    "  * **`_extract_document_metadata`**: Gathers filesystem info (size, modified time) and PDF properties (creator, producer, dates).\n",
    "  * **`_extract_pages_content`**: Iterates pages to get raw text, counts, densities, and logs progress.\n",
    "  * **`_calculate_content_statistics`**: Computes global counts, char-type breakdown, page-level aggregates, density ratios, and calls `_estimate_readability`.\n",
    "  * **`_detect_structure_patterns`**: Uses regex to count section numbers, bullets, ALL-CAP lines, title-case lines, code/tables/URLs, and Azure tags.\n",
    "  * **`_estimate_readability`**: Approximates Flesch Reading Ease via syllable counting and sentence lengths.\n",
    "  * **`_calculate_structure_confidence`**: Scores the presence of structural cues (numbered sections, bullets, tables, code) to a 0–1 confidence value.\n",
    "\n",
    "* **Test Block**\n",
    "\n",
    "  * Locates the first PDF in `config.PDF_FOLDER`.\n",
    "  * Instantiates `EnhancedPDFReader` and calls `read_pdf_with_metadata`.\n",
    "  * Prints out key results: title, pages, file size, total chars/words/unique words, readability score, structure confidence, and processing time.\n",
    "  * Summarizes structure hints (estimated sections, bullets, URLs, Azure services).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03fce793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:32:10,860 - INFO - Document Quality Assessor initialized\n",
      "2025-06-29 14:32:10,892 - INFO - Assessing quality for: 01-study-guide-az-vnet.pdf\n",
      "2025-06-29 14:32:10,896 - INFO - Quality assessment complete: 0.83 -> premium\n",
      "2025-06-29 14:32:10,898 - INFO - Enhanced PDF Reader initialized\n",
      "2025-06-29 14:32:10,899 - INFO - Document Quality Assessor initialized\n",
      "2025-06-29 14:32:10,900 - INFO - Batch Document Processor initialized\n",
      "2025-06-29 14:32:10,921 - INFO - Processing: 01-study-guide-az-vnet.pdf\n",
      "2025-06-29 14:32:10,922 - INFO - Processing PDF: 01-study-guide-az-vnet.pdf\n",
      "2025-06-29 14:32:10,943 - INFO - Processed 10 pages...\n",
      "2025-06-29 14:32:10,964 - INFO - PDF processed successfully: 25,984 chars, 11 pages, 0.04s\n",
      "2025-06-29 14:32:10,975 - INFO - Assessing quality for: 01-study-guide-az-vnet.pdf\n",
      "2025-06-29 14:32:10,978 - INFO - Quality assessment complete: 0.83 -> premium\n",
      "2025-06-29 14:32:10,978 - INFO - Processing: 02-study-guide-az-load-balancer.pdf\n",
      "2025-06-29 14:32:10,979 - INFO - Processing PDF: 02-study-guide-az-load-balancer.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Building Document Quality Assessor...\n",
      "\n",
      "🧪 Testing Document Quality Assessor...\n",
      "\n",
      "📊 Quality Assessment Results:\n",
      "   🎯 Overall Score: 0.83/1.0\n",
      "   🏆 Processing Tier: PREMIUM\n",
      "   👥 Review Type: None\n",
      "   🧩 Chunking Strategy: semantic_section_based\n",
      "   💰 Estimated Cost: 10.0 units\n",
      "   ✅ Should Process: False\n",
      "   📈 Priority Score: 100/100\n",
      "\n",
      "🔍 Quality Breakdown:\n",
      "   📊 Structure Quality: 1.00\n",
      "   📊 Content Quality: 0.61\n",
      "   📊 Technical Quality: 0.90\n",
      "   📊 Ocr Quality: 0.95\n",
      "   📊 Business Value: 0.59\n",
      "\n",
      "💡 Recommendations:\n",
      "   1. High Azure service density - consider specialized Azure chunking\n",
      "\n",
      "✅ Document Quality Assessor working perfectly!\n",
      "\n",
      "🎯 Ready for adaptive chunking strategies!\n",
      "🏭 Building Batch Document Processing Pipeline...\n",
      "\n",
      "🧪 Testing Batch Document Processing...\n",
      "\n",
      "📄 Found 2 PDF files to process:\n",
      "   1. 01-study-guide-az-vnet.pdf (0.1 MB)\n",
      "   2. 02-study-guide-az-load-balancer.pdf (0.2 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:32:11,004 - INFO - Processed 10 pages...\n",
      "2025-06-29 14:32:11,034 - INFO - Processed 20 pages...\n",
      "2025-06-29 14:32:11,068 - INFO - PDF processed successfully: 55,835 chars, 25 pages, 0.09s\n",
      "2025-06-29 14:32:11,069 - INFO - Assessing quality for: 02-study-guide-az-load-balancer.pdf\n",
      "2025-06-29 14:32:11,074 - INFO - Quality assessment complete: 0.83 -> premium\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Batch Processing Summary:\n",
      "   📄 Total Documents: 2\n",
      "   ✅ Successfully Processed: 2\n",
      "   ❌ Processing Errors: 0\n",
      "\n",
      "🏆 Processing Tier Distribution:\n",
      "   🥇 Premium: 0 (0.0%)\n",
      "   🥈 Standard: 0 (0.0%)\n",
      "   🥉 Basic: 0 (0.0%)\n",
      "   👥 Manual Review: 0 (0.0%)\n",
      "   ⏭️ Skipped: 0 (0.0%)\n",
      "\n",
      "📋 Document Details:\n",
      "   1. 01-study-guide-az-vnet.pdf\n",
      "      🎯 Score: 0.83 | Tier: PREMIUM\n",
      "      🧩 Strategy: semantic_section_based\n",
      "      💰 Cost: 10.0 units\n",
      "      💡 Rec: High Azure service density - consider specialized Azure chunking\n",
      "   2. 02-study-guide-az-load-balancer.pdf\n",
      "      🎯 Score: 0.83 | Tier: PREMIUM\n",
      "      🧩 Strategy: semantic_section_based\n",
      "      💰 Cost: 10.0 units\n",
      "      💡 Rec: High Azure service density - consider specialized Azure chunking\n",
      "\n",
      "💰 Cost Analysis:\n",
      "   💸 Total Estimated Cost: 20.0 processing units\n",
      "   📈 Average Cost per Document: 10.0 units\n",
      "\n",
      "🔍 Most Common Recommendations:\n",
      "   • High Azure service density - consider specialized Azure chunking (2 documents)\n",
      "\n",
      "✅ Batch processing complete!\n",
      "🎯 Ready for adaptive chunking implementation!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Document Quality Assessor\n",
    "print(\"🔍 Building Document Quality Assessor...\")\n",
    "\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import re\n",
    "\n",
    "class ProcessingTier(Enum):\n",
    "    \"\"\"Processing tiers based on document quality\"\"\"\n",
    "    PREMIUM = \"premium\"\n",
    "    STANDARD = \"standard\" \n",
    "    BASIC = \"basic\"\n",
    "    MANUAL_REVIEW = \"manual_review\"\n",
    "    SKIP = \"skip\"\n",
    "\n",
    "class ReviewType(Enum):\n",
    "    \"\"\"Types of human review needed\"\"\"\n",
    "    NONE = \"none\"\n",
    "    SPOT_CHECK = \"spot_check\"\n",
    "    CHUNKING_VALIDATION = \"chunking_validation\"\n",
    "    FULL_STRUCTURAL_REVIEW = \"full_structural_review\"\n",
    "    OCR_CLEANUP = \"ocr_cleanup\"\n",
    "\n",
    "@dataclass\n",
    "class QualityAssessment:\n",
    "    \"\"\"Comprehensive document quality assessment result\"\"\"\n",
    "    overall_score: float\n",
    "    processing_tier: ProcessingTier\n",
    "    review_type: ReviewType\n",
    "    chunking_strategy: str\n",
    "    estimated_processing_cost: float\n",
    "    confidence_factors: Dict\n",
    "    recommendations: List[str]\n",
    "    should_process: bool\n",
    "    priority_score: int\n",
    "\n",
    "class DocumentQualityAssessor:\n",
    "    \"\"\"Production-ready document quality assessment system\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n",
    "        \n",
    "        # Quality thresholds for different processing decisions\n",
    "        self.thresholds = {\n",
    "            'premium_processing': 0.8,      # Top-tier processing\n",
    "            'standard_processing': 0.6,     # Standard automation\n",
    "            'basic_processing': 0.4,        # Minimal processing\n",
    "            'manual_review': 0.2,           # Human review required\n",
    "            'skip_processing': 0.1          # Don't process\n",
    "        }\n",
    "        \n",
    "        # Cost estimates (in processing units)\n",
    "        self.processing_costs = {\n",
    "            ProcessingTier.PREMIUM: 10.0,\n",
    "            ProcessingTier.STANDARD: 5.0,\n",
    "            ProcessingTier.BASIC: 2.0,\n",
    "            ProcessingTier.MANUAL_REVIEW: 15.0,  # Includes human time\n",
    "            ProcessingTier.SKIP: 0.1\n",
    "        }\n",
    "        \n",
    "        self.logger.info(\"Document Quality Assessor initialized\")\n",
    "    \n",
    "    def assess_document_quality(self, pdf_data: Dict) -> QualityAssessment:\n",
    "        \"\"\"Comprehensive quality assessment with processing recommendations\"\"\"\n",
    "        \n",
    "        self.logger.info(f\"Assessing quality for: {pdf_data['metadata']['filename']}\")\n",
    "        \n",
    "        # Individual quality components\n",
    "        structure_quality = self._assess_structure_quality(pdf_data)\n",
    "        content_quality = self._assess_content_quality(pdf_data)\n",
    "        technical_quality = self._assess_technical_quality(pdf_data)\n",
    "        ocr_quality = self._assess_ocr_quality(pdf_data)\n",
    "        business_value = self._assess_business_value(pdf_data)\n",
    "        \n",
    "        # Confidence factors for transparency\n",
    "        confidence_factors = {\n",
    "            'structure_quality': structure_quality,\n",
    "            'content_quality': content_quality,\n",
    "            'technical_quality': technical_quality,\n",
    "            'ocr_quality': ocr_quality,\n",
    "            'business_value': business_value\n",
    "        }\n",
    "        \n",
    "        # Calculate weighted overall score\n",
    "        overall_score = self._calculate_overall_score(confidence_factors)\n",
    "        \n",
    "        # Determine processing strategy\n",
    "        processing_tier = self._determine_processing_tier(overall_score, confidence_factors)\n",
    "        review_type = self._determine_review_type(overall_score, confidence_factors)\n",
    "        chunking_strategy = self._recommend_chunking_strategy(pdf_data, overall_score)\n",
    "        \n",
    "        # Cost-benefit analysis\n",
    "        estimated_cost = self.processing_costs[processing_tier]\n",
    "        should_process = self._should_process_document(overall_score, estimated_cost, business_value)\n",
    "        \n",
    "        # Generate recommendations\n",
    "        recommendations = self._generate_recommendations(pdf_data, confidence_factors)\n",
    "        \n",
    "        # Calculate priority for processing queue\n",
    "        priority_score = self._calculate_priority_score(overall_score, business_value, estimated_cost)\n",
    "        \n",
    "        assessment = QualityAssessment(\n",
    "            overall_score=overall_score,\n",
    "            processing_tier=processing_tier,\n",
    "            review_type=review_type,\n",
    "            chunking_strategy=chunking_strategy,\n",
    "            estimated_processing_cost=estimated_cost,\n",
    "            confidence_factors=confidence_factors,\n",
    "            recommendations=recommendations,\n",
    "            should_process=should_process,\n",
    "            priority_score=priority_score\n",
    "        )\n",
    "        \n",
    "        self.logger.info(f\"Quality assessment complete: {overall_score:.2f} -> {processing_tier.value}\")\n",
    "        return assessment\n",
    "    \n",
    "    def _assess_structure_quality(self, pdf_data: Dict) -> float:\n",
    "        \"\"\"Assess document structure quality\"\"\"\n",
    "        \n",
    "        structure_hints = pdf_data['structure_hints']\n",
    "        \n",
    "        # Base score from structure confidence\n",
    "        base_score = structure_hints['structure_confidence']\n",
    "        \n",
    "        # Bonus points for clear organization\n",
    "        heading_score = 0.0\n",
    "        headings = structure_hints['heading_patterns']\n",
    "        \n",
    "        if headings['numbered_sections'] > 0:\n",
    "            heading_score += 0.3\n",
    "        if headings['all_caps_lines'] > 0:\n",
    "            heading_score += 0.2\n",
    "        if headings['bullet_points'] > 0:\n",
    "            heading_score += 0.1\n",
    "        \n",
    "        # Penalty for inconsistent structure\n",
    "        if headings['numbered_sections'] > 0 and headings['bullet_points'] > headings['numbered_sections'] * 3:\n",
    "            heading_score -= 0.1  # Too many bullets vs sections\n",
    "        \n",
    "        final_score = min(1.0, base_score + heading_score)\n",
    "        \n",
    "        self.logger.debug(f\"Structure quality: {final_score:.2f} (base: {base_score:.2f}, heading: {heading_score:.2f})\")\n",
    "        return final_score\n",
    "    \n",
    "    def _assess_content_quality(self, pdf_data: Dict) -> float:\n",
    "        \"\"\"Assess content quality and richness\"\"\"\n",
    "        \n",
    "        content_stats = pdf_data['content_stats']\n",
    "        \n",
    "        # Content richness indicators\n",
    "        word_diversity = content_stats['unique_words'] / max(1, content_stats['total_words'])\n",
    "        content_density = content_stats['content_density']\n",
    "        \n",
    "        # Check for meaningful content\n",
    "        if content_stats['total_words'] < 100:\n",
    "            return 0.1  # Too little content\n",
    "        \n",
    "        # Readability assessment (context-aware)\n",
    "        readability = content_stats['readability_estimate']\n",
    "        \n",
    "        # For technical documents, adjust readability expectations\n",
    "        if self._is_technical_document(pdf_data):\n",
    "            # Technical docs can have lower readability\n",
    "            readability_score = max(0.5, readability / 100)\n",
    "        else:\n",
    "            readability_score = readability / 100\n",
    "        \n",
    "        # Combine factors\n",
    "        quality_score = (\n",
    "            word_diversity * 0.3 +\n",
    "            content_density * 0.2 +\n",
    "            readability_score * 0.3 +\n",
    "            min(1.0, content_stats['total_words'] / 1000) * 0.2  # Length bonus up to 1000 words\n",
    "        )\n",
    "        \n",
    "        self.logger.debug(f\"Content quality: {quality_score:.2f} (diversity: {word_diversity:.2f}, density: {content_density:.2f})\")\n",
    "        return min(1.0, quality_score)\n",
    "    \n",
    "    def _assess_technical_quality(self, pdf_data: Dict) -> float:\n",
    "        \"\"\"Assess technical content quality for Azure documents\"\"\"\n",
    "        \n",
    "        structure_hints = pdf_data['structure_hints']\n",
    "        content = pdf_data['content'].lower()\n",
    "        \n",
    "        # Azure-specific quality indicators\n",
    "        azure_mentions = structure_hints['azure_patterns']['azure_services']\n",
    "        \n",
    "        # Technical terminology density\n",
    "        technical_terms = [\n",
    "            'configuration', 'deployment', 'management', 'security',\n",
    "            'network', 'virtual', 'resource', 'service', 'endpoint',\n",
    "            'policy', 'rule', 'group', 'account', 'subscription'\n",
    "        ]\n",
    "        \n",
    "        technical_density = sum(content.count(term) for term in technical_terms) / max(1, len(content.split()))\n",
    "        \n",
    "        # Code and configuration indicators\n",
    "        has_code = structure_hints['content_indicators']['has_code_blocks']\n",
    "        has_urls = structure_hints['content_indicators']['has_urls'] > 0\n",
    "        \n",
    "        # Calculate technical quality\n",
    "        technical_score = 0.0\n",
    "        \n",
    "        if azure_mentions > 10:  # Good Azure coverage\n",
    "            technical_score += 0.4\n",
    "        elif azure_mentions > 5:\n",
    "            technical_score += 0.2\n",
    "        \n",
    "        if technical_density > 0.05:  # 5% technical terms\n",
    "            technical_score += 0.3\n",
    "        elif technical_density > 0.02:\n",
    "            technical_score += 0.15\n",
    "        \n",
    "        if has_code:\n",
    "            technical_score += 0.2\n",
    "        \n",
    "        if has_urls:\n",
    "            technical_score += 0.1\n",
    "        \n",
    "        self.logger.debug(f\"Technical quality: {technical_score:.2f} (Azure mentions: {azure_mentions}, tech density: {technical_density:.3f})\")\n",
    "        return min(1.0, technical_score)\n",
    "    \n",
    "    def _assess_ocr_quality(self, pdf_data: Dict) -> float:\n",
    "        \"\"\"Assess OCR quality and text extraction reliability\"\"\"\n",
    "        \n",
    "        content = pdf_data['content']\n",
    "        \n",
    "        if not content.strip():\n",
    "            return 0.0\n",
    "        \n",
    "        # OCR quality indicators\n",
    "        total_chars = len(content)\n",
    "        \n",
    "        # Check for OCR artifacts\n",
    "        random_chars = len(re.findall(r'[^\\w\\s\\.\\,\\!\\?\\-\\(\\)\\:\\;]', content))\n",
    "        random_char_ratio = random_chars / max(1, total_chars)\n",
    "        \n",
    "        # Check for broken words (common OCR issue)\n",
    "        words = content.split()\n",
    "        short_fragments = len([w for w in words if len(w) == 1 and w.isalpha()])\n",
    "        fragment_ratio = short_fragments / max(1, len(words))\n",
    "        \n",
    "        # Check for missing spaces (words run together)\n",
    "        long_words = len([w for w in words if len(w) > 20])\n",
    "        long_word_ratio = long_words / max(1, len(words))\n",
    "        \n",
    "        # Calculate OCR quality score\n",
    "        ocr_score = 1.0\n",
    "        ocr_score -= random_char_ratio * 2.0    # Penalize random characters\n",
    "        ocr_score -= fragment_ratio * 1.5       # Penalize fragmentation\n",
    "        ocr_score -= long_word_ratio * 1.0      # Penalize missing spaces\n",
    "        \n",
    "        ocr_score = max(0.0, ocr_score)\n",
    "        \n",
    "        self.logger.debug(f\"OCR quality: {ocr_score:.2f} (random chars: {random_char_ratio:.3f}, fragments: {fragment_ratio:.3f})\")\n",
    "        return ocr_score\n",
    "    \n",
    "    def _assess_business_value(self, pdf_data: Dict) -> float:\n",
    "        \"\"\"Assess business value and priority of the document\"\"\"\n",
    "        \n",
    "        content = pdf_data['content'].lower()\n",
    "        metadata = pdf_data['metadata']\n",
    "        \n",
    "        # Document freshness\n",
    "        file_age_days = 0  # Could calculate from file metadata\n",
    "        freshness_score = max(0.5, 1.0 - (file_age_days / 365))  # Decay over a year\n",
    "        \n",
    "        # Content value indicators\n",
    "        value_keywords = [\n",
    "            'guide', 'tutorial', 'documentation', 'best practices',\n",
    "            'architecture', 'deployment', 'configuration', 'troubleshooting'\n",
    "        ]\n",
    "        \n",
    "        value_score = sum(content.count(keyword) for keyword in value_keywords) / max(1, len(content.split()))\n",
    "        value_score = min(1.0, value_score * 100)  # Scale up\n",
    "        \n",
    "        # Document size (comprehensive content is valuable)\n",
    "        size_score = min(1.0, metadata['page_count'] / 20)  # Up to 20 pages gets full points\n",
    "        \n",
    "        # Combine factors\n",
    "        business_value = (\n",
    "            freshness_score * 0.3 +\n",
    "            value_score * 0.4 +\n",
    "            size_score * 0.3\n",
    "        )\n",
    "        \n",
    "        self.logger.debug(f\"Business value: {business_value:.2f} (freshness: {freshness_score:.2f}, value: {value_score:.2f})\")\n",
    "        return business_value\n",
    "    \n",
    "    def _calculate_overall_score(self, confidence_factors: Dict) -> float:\n",
    "        \"\"\"Calculate weighted overall quality score\"\"\"\n",
    "        \n",
    "        weights = {\n",
    "            'structure_quality': 0.25,\n",
    "            'content_quality': 0.25,\n",
    "            'technical_quality': 0.20,\n",
    "            'ocr_quality': 0.20,\n",
    "            'business_value': 0.10\n",
    "        }\n",
    "        \n",
    "        overall_score = sum(\n",
    "            confidence_factors[factor] * weight \n",
    "            for factor, weight in weights.items()\n",
    "        )\n",
    "        \n",
    "        return min(1.0, overall_score)\n",
    "    \n",
    "    def _determine_processing_tier(self, overall_score: float, confidence_factors: Dict) -> ProcessingTier:\n",
    "        \"\"\"Determine appropriate processing tier\"\"\"\n",
    "        \n",
    "        # Check for specific issues that override score\n",
    "        if confidence_factors['ocr_quality'] < 0.3:\n",
    "            return ProcessingTier.MANUAL_REVIEW\n",
    "        \n",
    "        # Score-based tiers\n",
    "        if overall_score >= self.thresholds['premium_processing']:\n",
    "            return ProcessingTier.PREMIUM\n",
    "        elif overall_score >= self.thresholds['standard_processing']:\n",
    "            return ProcessingTier.STANDARD\n",
    "        elif overall_score >= self.thresholds['basic_processing']:\n",
    "            return ProcessingTier.BASIC\n",
    "        elif overall_score >= self.thresholds['manual_review']:\n",
    "            return ProcessingTier.MANUAL_REVIEW\n",
    "        else:\n",
    "            return ProcessingTier.SKIP\n",
    "    \n",
    "    def _determine_review_type(self, overall_score: float, confidence_factors: Dict) -> ReviewType:\n",
    "        \"\"\"Determine type of human review needed\"\"\"\n",
    "        \n",
    "        if confidence_factors['ocr_quality'] < 0.5:\n",
    "            return ReviewType.OCR_CLEANUP\n",
    "        elif confidence_factors['structure_quality'] < 0.3:\n",
    "            return ReviewType.FULL_STRUCTURAL_REVIEW\n",
    "        elif overall_score < 0.6:\n",
    "            return ReviewType.CHUNKING_VALIDATION\n",
    "        elif overall_score >= 0.8:\n",
    "            return ReviewType.NONE\n",
    "        else:\n",
    "            return ReviewType.SPOT_CHECK\n",
    "    \n",
    "    def _recommend_chunking_strategy(self, pdf_data: Dict, overall_score: float) -> str:\n",
    "        \"\"\"Recommend chunking strategy based on quality assessment\"\"\"\n",
    "        \n",
    "        structure_confidence = pdf_data['structure_hints']['structure_confidence']\n",
    "        \n",
    "        if overall_score >= 0.8 and structure_confidence >= 0.7:\n",
    "            return \"semantic_section_based\"\n",
    "        elif overall_score >= 0.6 and structure_confidence >= 0.5:\n",
    "            return \"smart_boundary_chunking\"\n",
    "        elif overall_score >= 0.4:\n",
    "            return \"sentence_boundary_chunking\"\n",
    "        else:\n",
    "            return \"fixed_size_chunking\"\n",
    "    \n",
    "    def _should_process_document(self, overall_score: float, estimated_cost: float, business_value: float) -> bool:\n",
    "        \"\"\"Cost-benefit analysis for processing decision\"\"\"\n",
    "        \n",
    "        # Simple ROI calculation\n",
    "        expected_benefit = overall_score * business_value * 10  # Scale factor\n",
    "        roi = expected_benefit / max(0.1, estimated_cost)\n",
    "        \n",
    "        # Process if ROI > 1.0 and minimum quality threshold met\n",
    "        return roi > 1.0 and overall_score > 0.2\n",
    "    \n",
    "    def _generate_recommendations(self, pdf_data: Dict, confidence_factors: Dict) -> List[str]:\n",
    "        \"\"\"Generate actionable recommendations\"\"\"\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        if confidence_factors['structure_quality'] < 0.5:\n",
    "            recommendations.append(\"Consider manual section marking for better structure detection\")\n",
    "        \n",
    "        if confidence_factors['ocr_quality'] < 0.7:\n",
    "            recommendations.append(\"OCR quality issues detected - consider reprocessing with better OCR\")\n",
    "        \n",
    "        if confidence_factors['technical_quality'] < 0.3:\n",
    "            recommendations.append(\"Low technical content density - verify document relevance\")\n",
    "        \n",
    "        if pdf_data['content_stats']['total_words'] < 500:\n",
    "            recommendations.append(\"Short document - consider combining with related documents\")\n",
    "        \n",
    "        azure_services = pdf_data['structure_hints']['azure_patterns']['azure_services']\n",
    "        if azure_services > 50:\n",
    "            recommendations.append(\"High Azure service density - consider specialized Azure chunking\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _calculate_priority_score(self, overall_score: float, business_value: float, estimated_cost: float) -> int:\n",
    "        \"\"\"Calculate priority score for processing queue (0-100)\"\"\"\n",
    "        \n",
    "        # Higher score = higher priority\n",
    "        priority = (overall_score * 40 + business_value * 40 + (10 / max(1, estimated_cost)) * 20)\n",
    "        return min(100, int(priority * 100))\n",
    "    \n",
    "    def _is_technical_document(self, pdf_data: Dict) -> bool:\n",
    "        \"\"\"Detect if document is technical in nature\"\"\"\n",
    "        \n",
    "        azure_mentions = pdf_data['structure_hints']['azure_patterns']['azure_services']\n",
    "        technical_indicators = pdf_data['structure_hints']['content_indicators']\n",
    "        \n",
    "        return (\n",
    "            azure_mentions > 5 or\n",
    "            technical_indicators['has_code_blocks'] or\n",
    "            technical_indicators['has_urls'] > 2\n",
    "        )\n",
    "\n",
    "\n",
    "# Test the Document Quality Assessor\n",
    "print(\"\\n🧪 Testing Document Quality Assessor...\")\n",
    "\n",
    "if 'pdf_data' in globals() and pdf_data:\n",
    "    # Create quality assessor\n",
    "    quality_assessor = DocumentQualityAssessor(config)\n",
    "    \n",
    "    # Assess our Azure document\n",
    "    assessment = quality_assessor.assess_document_quality(pdf_data)\n",
    "    \n",
    "    print(f\"\\n📊 Quality Assessment Results:\")\n",
    "    print(f\"   🎯 Overall Score: {assessment.overall_score:.2f}/1.0\")\n",
    "    print(f\"   🏆 Processing Tier: {assessment.processing_tier.value.upper()}\")\n",
    "    print(f\"   👥 Review Type: {assessment.review_type.value.replace('_', ' ').title()}\")\n",
    "    print(f\"   🧩 Chunking Strategy: {assessment.chunking_strategy}\")\n",
    "    print(f\"   💰 Estimated Cost: {assessment.estimated_processing_cost:.1f} units\")\n",
    "    print(f\"   ✅ Should Process: {assessment.should_process}\")\n",
    "    print(f\"   📈 Priority Score: {assessment.priority_score}/100\")\n",
    "    \n",
    "    print(f\"\\n🔍 Quality Breakdown:\")\n",
    "    for factor, score in assessment.confidence_factors.items():\n",
    "        print(f\"   📊 {factor.replace('_', ' ').title()}: {score:.2f}\")\n",
    "    \n",
    "    print(f\"\\n💡 Recommendations:\")\n",
    "    for i, rec in enumerate(assessment.recommendations, 1):\n",
    "        print(f\"   {i}. {rec}\")\n",
    "    \n",
    "    print(f\"\\n✅ Document Quality Assessor working perfectly!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No PDF data available for assessment\")\n",
    "\n",
    "print(f\"\\n🎯 Ready for adaptive chunking strategies!\")\n",
    "\n",
    "\n",
    "# Cell 4: Batch Document Processing Pipeline\n",
    "print(\"🏭 Building Batch Document Processing Pipeline...\")\n",
    "\n",
    "class BatchDocumentProcessor:\n",
    "    \"\"\"Process multiple documents with quality assessment and routing\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n",
    "        \n",
    "        # Initialize components\n",
    "        self.pdf_reader = EnhancedPDFReader(config)\n",
    "        self.quality_assessor = DocumentQualityAssessor(config)\n",
    "        \n",
    "        # Processing statistics\n",
    "        self.stats = {\n",
    "            'total_processed': 0,\n",
    "            'premium_tier': 0,\n",
    "            'standard_tier': 0,\n",
    "            'basic_tier': 0,\n",
    "            'manual_review': 0,\n",
    "            'skipped': 0,\n",
    "            'processing_errors': 0\n",
    "        }\n",
    "        \n",
    "        self.logger.info(\"Batch Document Processor initialized\")\n",
    "    \n",
    "    def process_all_pdfs(self, pdf_folder: str = None) -> List[Dict]:\n",
    "        \"\"\"Process all PDFs in the folder with quality assessment\"\"\"\n",
    "        \n",
    "        if pdf_folder is None:\n",
    "            pdf_folder = self.config.PDF_FOLDER\n",
    "        \n",
    "        pdf_files = list(Path(pdf_folder).glob(\"*.pdf\"))\n",
    "        \n",
    "        if not pdf_files:\n",
    "            self.logger.warning(f\"No PDF files found in {pdf_folder}\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"\\n📄 Found {len(pdf_files)} PDF files to process:\")\n",
    "        for i, pdf_file in enumerate(pdf_files, 1):\n",
    "            size_mb = pdf_file.stat().st_size / (1024 * 1024)\n",
    "            print(f\"   {i}. {pdf_file.name} ({size_mb:.1f} MB)\")\n",
    "        \n",
    "        # Process each PDF\n",
    "        results = []\n",
    "        \n",
    "        for pdf_file in pdf_files:\n",
    "            try:\n",
    "                result = self.process_single_pdf(pdf_file)\n",
    "                results.append(result)\n",
    "                self.stats['total_processed'] += 1\n",
    "                \n",
    "                # Update tier statistics\n",
    "                tier = result['quality_assessment'].processing_tier.value\n",
    "                if tier in self.stats:\n",
    "                    self.stats[tier] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error processing {pdf_file.name}: {str(e)}\")\n",
    "                self.stats['processing_errors'] += 1\n",
    "                \n",
    "                # Add error result\n",
    "                results.append({\n",
    "                    'filename': pdf_file.name,\n",
    "                    'status': 'error',\n",
    "                    'error': str(e),\n",
    "                    'quality_assessment': None,\n",
    "                    'processing_recommendation': 'manual_investigation'\n",
    "                })\n",
    "        \n",
    "        # Print summary\n",
    "        self._print_processing_summary(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def process_single_pdf(self, pdf_path: Path) -> Dict:\n",
    "        \"\"\"Process a single PDF with quality assessment\"\"\"\n",
    "        \n",
    "        self.logger.info(f\"Processing: {pdf_path.name}\")\n",
    "        \n",
    "        # Extract PDF content and metadata\n",
    "        pdf_data = self.pdf_reader.read_pdf_with_metadata(pdf_path)\n",
    "        \n",
    "        # Assess document quality\n",
    "        assessment = self.quality_assessor.assess_document_quality(pdf_data)\n",
    "        \n",
    "        # Create processing result\n",
    "        result = {\n",
    "            'filename': pdf_path.name,\n",
    "            'status': 'processed',\n",
    "            'pdf_data': pdf_data,\n",
    "            'quality_assessment': assessment,\n",
    "            'processing_recommendation': self._get_processing_recommendation(assessment)\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _get_processing_recommendation(self, assessment: QualityAssessment) -> Dict:\n",
    "        \"\"\"Generate detailed processing recommendation\"\"\"\n",
    "        \n",
    "        recommendation = {\n",
    "            'action': assessment.processing_tier.value,\n",
    "            'priority': assessment.priority_score,\n",
    "            'chunking_strategy': assessment.chunking_strategy,\n",
    "            'estimated_cost': assessment.estimated_processing_cost,\n",
    "            'should_process_immediately': assessment.should_process and assessment.processing_tier != ProcessingTier.MANUAL_REVIEW,\n",
    "            'human_review_required': assessment.review_type != ReviewType.NONE,\n",
    "            'review_type': assessment.review_type.value,\n",
    "            'recommendations': assessment.recommendations\n",
    "        }\n",
    "        \n",
    "        return recommendation\n",
    "    \n",
    "    def _print_processing_summary(self, results: List[Dict]) -> None:\n",
    "        \"\"\"Print comprehensive processing summary\"\"\"\n",
    "        \n",
    "        print(f\"\\n📊 Batch Processing Summary:\")\n",
    "        print(f\"   📄 Total Documents: {len(results)}\")\n",
    "        print(f\"   ✅ Successfully Processed: {self.stats['total_processed']}\")\n",
    "        print(f\"   ❌ Processing Errors: {self.stats['processing_errors']}\")\n",
    "        \n",
    "        print(f\"\\n🏆 Processing Tier Distribution:\")\n",
    "        total_successful = self.stats['total_processed']\n",
    "        if total_successful > 0:\n",
    "            print(f\"   🥇 Premium: {self.stats.get('premium', 0)} ({self.stats.get('premium', 0)/total_successful*100:.1f}%)\")\n",
    "            print(f\"   🥈 Standard: {self.stats.get('standard', 0)} ({self.stats.get('standard', 0)/total_successful*100:.1f}%)\")\n",
    "            print(f\"   🥉 Basic: {self.stats.get('basic', 0)} ({self.stats.get('basic', 0)/total_successful*100:.1f}%)\")\n",
    "            print(f\"   👥 Manual Review: {self.stats.get('manual_review', 0)} ({self.stats.get('manual_review', 0)/total_successful*100:.1f}%)\")\n",
    "            print(f\"   ⏭️ Skipped: {self.stats.get('skip', 0)} ({self.stats.get('skip', 0)/total_successful*100:.1f}%)\")\n",
    "        \n",
    "        # Document-level details\n",
    "        print(f\"\\n📋 Document Details:\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            if result['status'] == 'processed':\n",
    "                assessment = result['quality_assessment']\n",
    "                print(f\"   {i}. {result['filename']}\")\n",
    "                print(f\"      🎯 Score: {assessment.overall_score:.2f} | Tier: {assessment.processing_tier.value.upper()}\")\n",
    "                print(f\"      🧩 Strategy: {assessment.chunking_strategy}\")\n",
    "                print(f\"      💰 Cost: {assessment.estimated_processing_cost:.1f} units\")\n",
    "                if assessment.recommendations:\n",
    "                    print(f\"      💡 Rec: {assessment.recommendations[0]}\")\n",
    "            else:\n",
    "                print(f\"   {i}. {result['filename']} - ERROR: {result['error']}\")\n",
    "        \n",
    "        # Cost analysis\n",
    "        total_cost = sum(r['quality_assessment'].estimated_processing_cost \n",
    "                        for r in results if r['status'] == 'processed')\n",
    "        \n",
    "        print(f\"\\n💰 Cost Analysis:\")\n",
    "        print(f\"   💸 Total Estimated Cost: {total_cost:.1f} processing units\")\n",
    "        print(f\"   📈 Average Cost per Document: {total_cost/max(1, total_successful):.1f} units\")\n",
    "        \n",
    "        # Recommendations summary\n",
    "        all_recommendations = []\n",
    "        for result in results:\n",
    "            if result['status'] == 'processed':\n",
    "                all_recommendations.extend(result['quality_assessment'].recommendations)\n",
    "        \n",
    "        if all_recommendations:\n",
    "            from collections import Counter\n",
    "            common_recs = Counter(all_recommendations).most_common(3)\n",
    "            print(f\"\\n🔍 Most Common Recommendations:\")\n",
    "            for rec, count in common_recs:\n",
    "                print(f\"   • {rec} ({count} documents)\")\n",
    "\n",
    "\n",
    "# Test Batch Processing\n",
    "print(\"\\n🧪 Testing Batch Document Processing...\")\n",
    "\n",
    "# Create batch processor\n",
    "batch_processor = BatchDocumentProcessor(config)\n",
    "\n",
    "# Process all PDFs in the folder\n",
    "batch_results = batch_processor.process_all_pdfs()\n",
    "\n",
    "print(f\"\\n✅ Batch processing complete!\")\n",
    "print(f\"🎯 Ready for adaptive chunking implementation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9b9df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca82a6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199851a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c699786e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ced76ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-29 14:32:11,214 - INFO - Document Quality Assessor initialized\n",
      "2025-06-29 14:32:11,215 - INFO - Assessing quality for: 01-study-guide-az-vnet.pdf\n",
      "2025-06-29 14:32:11,219 - INFO - Quality assessment complete: 0.83 -> premium\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Building Document Quality Assessor...\n",
      "\n",
      "🧪 Testing Document Quality Assessor...\n",
      "\n",
      "📊 Quality Assessment Results:\n",
      "   🎯 Overall Score: 0.83/1.0\n",
      "   🏆 Processing Tier: PREMIUM\n",
      "   👥 Review Type: None\n",
      "   🧩 Chunking Strategy: semantic_section_based\n",
      "   💰 Estimated Cost: 10.0 units\n",
      "   ✅ Should Process: False\n",
      "   📈 Priority Score: 100/100\n",
      "\n",
      "🔍 Quality Breakdown:\n",
      "   📊 Structure Quality: 1.00\n",
      "   📊 Content Quality: 0.61\n",
      "   📊 Technical Quality: 0.90\n",
      "   📊 Ocr Quality: 0.95\n",
      "   📊 Business Value: 0.59\n",
      "\n",
      "💡 Recommendations:\n",
      "   1. High Azure service density - consider specialized Azure chunking\n",
      "\n",
      "✅ Document Quality Assessor working perfectly!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Document Quality Assessor\n",
    "print(\"🔍 Building Document Quality Assessor...\")\n",
    "\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import re\n",
    "\n",
    "class ProcessingTier(Enum):\n",
    "    \"\"\"Processing tiers based on document quality\"\"\"\n",
    "    PREMIUM = \"premium\"\n",
    "    STANDARD = \"standard\" \n",
    "    BASIC = \"basic\"\n",
    "    MANUAL_REVIEW = \"manual_review\"\n",
    "    SKIP = \"skip\"\n",
    "\n",
    "class ReviewType(Enum):\n",
    "    \"\"\"Types of human review needed\"\"\"\n",
    "    NONE = \"none\"\n",
    "    SPOT_CHECK = \"spot_check\"\n",
    "    CHUNKING_VALIDATION = \"chunking_validation\"\n",
    "    FULL_STRUCTURAL_REVIEW = \"full_structural_review\"\n",
    "    OCR_CLEANUP = \"ocr_cleanup\"\n",
    "\n",
    "@dataclass\n",
    "class QualityAssessment:\n",
    "    \"\"\"Comprehensive document quality assessment result\"\"\"\n",
    "    overall_score: float\n",
    "    processing_tier: ProcessingTier\n",
    "    review_type: ReviewType\n",
    "    chunking_strategy: str\n",
    "    estimated_processing_cost: float\n",
    "    confidence_factors: Dict\n",
    "    recommendations: List[str]\n",
    "    should_process: bool\n",
    "    priority_score: int\n",
    "\n",
    "class DocumentQualityAssessor:\n",
    "    \"\"\"Production-ready document quality assessment system\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n",
    "        \n",
    "        # Quality thresholds for different processing decisions\n",
    "        self.thresholds = {\n",
    "            'premium_processing': 0.8,      # Top-tier processing\n",
    "            'standard_processing': 0.6,     # Standard automation\n",
    "            'basic_processing': 0.4,        # Minimal processing\n",
    "            'manual_review': 0.2,           # Human review required\n",
    "            'skip_processing': 0.1          # Don't process\n",
    "        }\n",
    "        \n",
    "        # Cost estimates (in processing units)\n",
    "        self.processing_costs = {\n",
    "            ProcessingTier.PREMIUM: 10.0,\n",
    "            ProcessingTier.STANDARD: 5.0,\n",
    "            ProcessingTier.BASIC: 2.0,\n",
    "            ProcessingTier.MANUAL_REVIEW: 15.0,  # Includes human time\n",
    "            ProcessingTier.SKIP: 0.1\n",
    "        }\n",
    "        \n",
    "        self.logger.info(\"Document Quality Assessor initialized\")\n",
    "    \n",
    "    def assess_document_quality(self, pdf_data: Dict) -> QualityAssessment:\n",
    "        \"\"\"Comprehensive quality assessment with processing recommendations\"\"\"\n",
    "        \n",
    "        self.logger.info(f\"Assessing quality for: {pdf_data['metadata']['filename']}\")\n",
    "        \n",
    "        # Individual quality components\n",
    "        structure_quality = self._assess_structure_quality(pdf_data)\n",
    "        content_quality = self._assess_content_quality(pdf_data)\n",
    "        technical_quality = self._assess_technical_quality(pdf_data)\n",
    "        ocr_quality = self._assess_ocr_quality(pdf_data)\n",
    "        business_value = self._assess_business_value(pdf_data)\n",
    "        \n",
    "        # Confidence factors for transparency\n",
    "        confidence_factors = {\n",
    "            'structure_quality': structure_quality,\n",
    "            'content_quality': content_quality,\n",
    "            'technical_quality': technical_quality,\n",
    "            'ocr_quality': ocr_quality,\n",
    "            'business_value': business_value\n",
    "        }\n",
    "        \n",
    "        # Calculate weighted overall score\n",
    "        overall_score = self._calculate_overall_score(confidence_factors)\n",
    "        \n",
    "        # Determine processing strategy\n",
    "        processing_tier = self._determine_processing_tier(overall_score, confidence_factors)\n",
    "        review_type = self._determine_review_type(overall_score, confidence_factors)\n",
    "        chunking_strategy = self._recommend_chunking_strategy(pdf_data, overall_score)\n",
    "        \n",
    "        # Cost-benefit analysis\n",
    "        estimated_cost = self.processing_costs[processing_tier]\n",
    "        should_process = self._should_process_document(overall_score, estimated_cost, business_value)\n",
    "        \n",
    "        # Generate recommendations\n",
    "        recommendations = self._generate_recommendations(pdf_data, confidence_factors)\n",
    "        \n",
    "        # Calculate priority for processing queue\n",
    "        priority_score = self._calculate_priority_score(overall_score, business_value, estimated_cost)\n",
    "        \n",
    "        assessment = QualityAssessment(\n",
    "            overall_score=overall_score,\n",
    "            processing_tier=processing_tier,\n",
    "            review_type=review_type,\n",
    "            chunking_strategy=chunking_strategy,\n",
    "            estimated_processing_cost=estimated_cost,\n",
    "            confidence_factors=confidence_factors,\n",
    "            recommendations=recommendations,\n",
    "            should_process=should_process,\n",
    "            priority_score=priority_score\n",
    "        )\n",
    "        \n",
    "        self.logger.info(f\"Quality assessment complete: {overall_score:.2f} -> {processing_tier.value}\")\n",
    "        return assessment\n",
    "    \n",
    "    def _assess_structure_quality(self, pdf_data: Dict) -> float:\n",
    "        \"\"\"Assess document structure quality\"\"\"\n",
    "        \n",
    "        structure_hints = pdf_data['structure_hints']\n",
    "        \n",
    "        # Base score from structure confidence\n",
    "        base_score = structure_hints['structure_confidence']\n",
    "        \n",
    "        # Bonus points for clear organization\n",
    "        heading_score = 0.0\n",
    "        headings = structure_hints['heading_patterns']\n",
    "        \n",
    "        if headings['numbered_sections'] > 0:\n",
    "            heading_score += 0.3\n",
    "        if headings['all_caps_lines'] > 0:\n",
    "            heading_score += 0.2\n",
    "        if headings['bullet_points'] > 0:\n",
    "            heading_score += 0.1\n",
    "        \n",
    "        # Penalty for inconsistent structure\n",
    "        if headings['numbered_sections'] > 0 and headings['bullet_points'] > headings['numbered_sections'] * 3:\n",
    "            heading_score -= 0.1  # Too many bullets vs sections\n",
    "        \n",
    "        final_score = min(1.0, base_score + heading_score)\n",
    "        \n",
    "        self.logger.debug(f\"Structure quality: {final_score:.2f} (base: {base_score:.2f}, heading: {heading_score:.2f})\")\n",
    "        return final_score\n",
    "    \n",
    "    def _assess_content_quality(self, pdf_data: Dict) -> float:\n",
    "        \"\"\"Assess content quality and richness\"\"\"\n",
    "        \n",
    "        content_stats = pdf_data['content_stats']\n",
    "        \n",
    "        # Content richness indicators\n",
    "        word_diversity = content_stats['unique_words'] / max(1, content_stats['total_words'])\n",
    "        content_density = content_stats['content_density']\n",
    "        \n",
    "        # Check for meaningful content\n",
    "        if content_stats['total_words'] < 100:\n",
    "            return 0.1  # Too little content\n",
    "        \n",
    "        # Readability assessment (context-aware)\n",
    "        readability = content_stats['readability_estimate']\n",
    "        \n",
    "        # For technical documents, adjust readability expectations\n",
    "        if self._is_technical_document(pdf_data):\n",
    "            # Technical docs can have lower readability\n",
    "            readability_score = max(0.5, readability / 100)\n",
    "        else:\n",
    "            readability_score = readability / 100\n",
    "        \n",
    "        # Combine factors\n",
    "        quality_score = (\n",
    "            word_diversity * 0.3 +\n",
    "            content_density * 0.2 +\n",
    "            readability_score * 0.3 +\n",
    "            min(1.0, content_stats['total_words'] / 1000) * 0.2  # Length bonus up to 1000 words\n",
    "        )\n",
    "        \n",
    "        self.logger.debug(f\"Content quality: {quality_score:.2f} (diversity: {word_diversity:.2f}, density: {content_density:.2f})\")\n",
    "        return min(1.0, quality_score)\n",
    "    \n",
    "    def _assess_technical_quality(self, pdf_data: Dict) -> float:\n",
    "        \"\"\"Assess technical content quality for Azure documents\"\"\"\n",
    "        \n",
    "        structure_hints = pdf_data['structure_hints']\n",
    "        content = pdf_data['content'].lower()\n",
    "        \n",
    "        # Azure-specific quality indicators\n",
    "        azure_mentions = structure_hints['azure_patterns']['azure_services']\n",
    "        \n",
    "        # Technical terminology density\n",
    "        technical_terms = [\n",
    "            'configuration', 'deployment', 'management', 'security',\n",
    "            'network', 'virtual', 'resource', 'service', 'endpoint',\n",
    "            'policy', 'rule', 'group', 'account', 'subscription'\n",
    "        ]\n",
    "        \n",
    "        technical_density = sum(content.count(term) for term in technical_terms) / max(1, len(content.split()))\n",
    "        \n",
    "        # Code and configuration indicators\n",
    "        has_code = structure_hints['content_indicators']['has_code_blocks']\n",
    "        has_urls = structure_hints['content_indicators']['has_urls'] > 0\n",
    "        \n",
    "        # Calculate technical quality\n",
    "        technical_score = 0.0\n",
    "        \n",
    "        if azure_mentions > 10:  # Good Azure coverage\n",
    "            technical_score += 0.4\n",
    "        elif azure_mentions > 5:\n",
    "            technical_score += 0.2\n",
    "        \n",
    "        if technical_density > 0.05:  # 5% technical terms\n",
    "            technical_score += 0.3\n",
    "        elif technical_density > 0.02:\n",
    "            technical_score += 0.15\n",
    "        \n",
    "        if has_code:\n",
    "            technical_score += 0.2\n",
    "        \n",
    "        if has_urls:\n",
    "            technical_score += 0.1\n",
    "        \n",
    "        self.logger.debug(f\"Technical quality: {technical_score:.2f} (Azure mentions: {azure_mentions}, tech density: {technical_density:.3f})\")\n",
    "        return min(1.0, technical_score)\n",
    "    \n",
    "    def _assess_ocr_quality(self, pdf_data: Dict) -> float:\n",
    "        \"\"\"Assess OCR quality and text extraction reliability\"\"\"\n",
    "        \n",
    "        content = pdf_data['content']\n",
    "        \n",
    "        if not content.strip():\n",
    "            return 0.0\n",
    "        \n",
    "        # OCR quality indicators\n",
    "        total_chars = len(content)\n",
    "        \n",
    "        # Check for OCR artifacts\n",
    "        random_chars = len(re.findall(r'[^\\w\\s\\.\\,\\!\\?\\-\\(\\)\\:\\;]', content))\n",
    "        random_char_ratio = random_chars / max(1, total_chars)\n",
    "        \n",
    "        # Check for broken words (common OCR issue)\n",
    "        words = content.split()\n",
    "        short_fragments = len([w for w in words if len(w) == 1 and w.isalpha()])\n",
    "        fragment_ratio = short_fragments / max(1, len(words))\n",
    "        \n",
    "        # Check for missing spaces (words run together)\n",
    "        long_words = len([w for w in words if len(w) > 20])\n",
    "        long_word_ratio = long_words / max(1, len(words))\n",
    "        \n",
    "        # Calculate OCR quality score\n",
    "        ocr_score = 1.0\n",
    "        ocr_score -= random_char_ratio * 2.0    # Penalize random characters\n",
    "        ocr_score -= fragment_ratio * 1.5       # Penalize fragmentation\n",
    "        ocr_score -= long_word_ratio * 1.0      # Penalize missing spaces\n",
    "        \n",
    "        ocr_score = max(0.0, ocr_score)\n",
    "        \n",
    "        self.logger.debug(f\"OCR quality: {ocr_score:.2f} (random chars: {random_char_ratio:.3f}, fragments: {fragment_ratio:.3f})\")\n",
    "        return ocr_score\n",
    "    \n",
    "    def _assess_business_value(self, pdf_data: Dict) -> float:\n",
    "        \"\"\"Assess business value and priority of the document\"\"\"\n",
    "        \n",
    "        content = pdf_data['content'].lower()\n",
    "        metadata = pdf_data['metadata']\n",
    "        \n",
    "        # Document freshness\n",
    "        file_age_days = 0  # Could calculate from file metadata\n",
    "        freshness_score = max(0.5, 1.0 - (file_age_days / 365))  # Decay over a year\n",
    "        \n",
    "        # Content value indicators\n",
    "        value_keywords = [\n",
    "            'guide', 'tutorial', 'documentation', 'best practices',\n",
    "            'architecture', 'deployment', 'configuration', 'troubleshooting'\n",
    "        ]\n",
    "        \n",
    "        value_score = sum(content.count(keyword) for keyword in value_keywords) / max(1, len(content.split()))\n",
    "        value_score = min(1.0, value_score * 100)  # Scale up\n",
    "        \n",
    "        # Document size (comprehensive content is valuable)\n",
    "        size_score = min(1.0, metadata['page_count'] / 20)  # Up to 20 pages gets full points\n",
    "        \n",
    "        # Combine factors\n",
    "        business_value = (\n",
    "            freshness_score * 0.3 +\n",
    "            value_score * 0.4 +\n",
    "            size_score * 0.3\n",
    "        )\n",
    "        \n",
    "        self.logger.debug(f\"Business value: {business_value:.2f} (freshness: {freshness_score:.2f}, value: {value_score:.2f})\")\n",
    "        return business_value\n",
    "    \n",
    "    def _calculate_overall_score(self, confidence_factors: Dict) -> float:\n",
    "        \"\"\"Calculate weighted overall quality score\"\"\"\n",
    "        \n",
    "        weights = {\n",
    "            'structure_quality': 0.25,\n",
    "            'content_quality': 0.25,\n",
    "            'technical_quality': 0.20,\n",
    "            'ocr_quality': 0.20,\n",
    "            'business_value': 0.10\n",
    "        }\n",
    "        \n",
    "        overall_score = sum(\n",
    "            confidence_factors[factor] * weight \n",
    "            for factor, weight in weights.items()\n",
    "        )\n",
    "        \n",
    "        return min(1.0, overall_score)\n",
    "    \n",
    "    def _determine_processing_tier(self, overall_score: float, confidence_factors: Dict) -> ProcessingTier:\n",
    "        \"\"\"Determine appropriate processing tier\"\"\"\n",
    "        \n",
    "        # Check for specific issues that override score\n",
    "        if confidence_factors['ocr_quality'] < 0.3:\n",
    "            return ProcessingTier.MANUAL_REVIEW\n",
    "        \n",
    "        # Score-based tiers\n",
    "        if overall_score >= self.thresholds['premium_processing']:\n",
    "            return ProcessingTier.PREMIUM\n",
    "        elif overall_score >= self.thresholds['standard_processing']:\n",
    "            return ProcessingTier.STANDARD\n",
    "        elif overall_score >= self.thresholds['basic_processing']:\n",
    "            return ProcessingTier.BASIC\n",
    "        elif overall_score >= self.thresholds['manual_review']:\n",
    "            return ProcessingTier.MANUAL_REVIEW\n",
    "        else:\n",
    "            return ProcessingTier.SKIP\n",
    "    \n",
    "    def _determine_review_type(self, overall_score: float, confidence_factors: Dict) -> ReviewType:\n",
    "        \"\"\"Determine type of human review needed\"\"\"\n",
    "        \n",
    "        if confidence_factors['ocr_quality'] < 0.5:\n",
    "            return ReviewType.OCR_CLEANUP\n",
    "        elif confidence_factors['structure_quality'] < 0.3:\n",
    "            return ReviewType.FULL_STRUCTURAL_REVIEW\n",
    "        elif overall_score < 0.6:\n",
    "            return ReviewType.CHUNKING_VALIDATION\n",
    "        elif overall_score >= 0.8:\n",
    "            return ReviewType.NONE\n",
    "        else:\n",
    "            return ReviewType.SPOT_CHECK\n",
    "    \n",
    "    def _recommend_chunking_strategy(self, pdf_data: Dict, overall_score: float) -> str:\n",
    "        \"\"\"Recommend chunking strategy based on quality assessment\"\"\"\n",
    "        \n",
    "        structure_confidence = pdf_data['structure_hints']['structure_confidence']\n",
    "        \n",
    "        if overall_score >= 0.8 and structure_confidence >= 0.7:\n",
    "            return \"semantic_section_based\"\n",
    "        elif overall_score >= 0.6 and structure_confidence >= 0.5:\n",
    "            return \"smart_boundary_chunking\"\n",
    "        elif overall_score >= 0.4:\n",
    "            return \"sentence_boundary_chunking\"\n",
    "        else:\n",
    "            return \"fixed_size_chunking\"\n",
    "    \n",
    "    def _should_process_document(self, overall_score: float, estimated_cost: float, business_value: float) -> bool:\n",
    "        \"\"\"Cost-benefit analysis for processing decision\"\"\"\n",
    "        \n",
    "        # Simple ROI calculation\n",
    "        expected_benefit = overall_score * business_value * 10  # Scale factor\n",
    "        roi = expected_benefit / max(0.1, estimated_cost)\n",
    "        \n",
    "        # Process if ROI > 1.0 and minimum quality threshold met\n",
    "        return roi > 1.0 and overall_score > 0.2\n",
    "    \n",
    "    def _generate_recommendations(self, pdf_data: Dict, confidence_factors: Dict) -> List[str]:\n",
    "        \"\"\"Generate actionable recommendations\"\"\"\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        if confidence_factors['structure_quality'] < 0.5:\n",
    "            recommendations.append(\"Consider manual section marking for better structure detection\")\n",
    "        \n",
    "        if confidence_factors['ocr_quality'] < 0.7:\n",
    "            recommendations.append(\"OCR quality issues detected - consider reprocessing with better OCR\")\n",
    "        \n",
    "        if confidence_factors['technical_quality'] < 0.3:\n",
    "            recommendations.append(\"Low technical content density - verify document relevance\")\n",
    "        \n",
    "        if pdf_data['content_stats']['total_words'] < 500:\n",
    "            recommendations.append(\"Short document - consider combining with related documents\")\n",
    "        \n",
    "        azure_services = pdf_data['structure_hints']['azure_patterns']['azure_services']\n",
    "        if azure_services > 50:\n",
    "            recommendations.append(\"High Azure service density - consider specialized Azure chunking\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _calculate_priority_score(self, overall_score: float, business_value: float, estimated_cost: float) -> int:\n",
    "        \"\"\"Calculate priority score for processing queue (0-100)\"\"\"\n",
    "        \n",
    "        # Higher score = higher priority\n",
    "        priority = (overall_score * 40 + business_value * 40 + (10 / max(1, estimated_cost)) * 20)\n",
    "        return min(100, int(priority * 100))\n",
    "    \n",
    "    def _is_technical_document(self, pdf_data: Dict) -> bool:\n",
    "        \"\"\"Detect if document is technical in nature\"\"\"\n",
    "        \n",
    "        azure_mentions = pdf_data['structure_hints']['azure_patterns']['azure_services']\n",
    "        technical_indicators = pdf_data['structure_hints']['content_indicators']\n",
    "        \n",
    "        return (\n",
    "            azure_mentions > 5 or\n",
    "            technical_indicators['has_code_blocks'] or\n",
    "            technical_indicators['has_urls'] > 2\n",
    "        )\n",
    "\n",
    "\n",
    "# Test the Document Quality Assessor\n",
    "print(\"\\n🧪 Testing Document Quality Assessor...\")\n",
    "\n",
    "if 'pdf_data' in globals() and pdf_data:\n",
    "    # Create quality assessor\n",
    "    quality_assessor = DocumentQualityAssessor(config)\n",
    "    \n",
    "    # Assess our Azure document\n",
    "    assessment = quality_assessor.assess_document_quality(pdf_data)\n",
    "    \n",
    "    print(f\"\\n📊 Quality Assessment Results:\")\n",
    "    print(f\"   🎯 Overall Score: {assessment.overall_score:.2f}/1.0\")\n",
    "    print(f\"   🏆 Processing Tier: {assessment.processing_tier.value.upper()}\")\n",
    "    print(f\"   👥 Review Type: {assessment.review_type.value.replace('_', ' ').title()}\")\n",
    "    print(f\"   🧩 Chunking Strategy: {assessment.chunking_strategy}\")\n",
    "    print(f\"   💰 Estimated Cost: {assessment.estimated_processing_cost:.1f} units\")\n",
    "    print(f\"   ✅ Should Process: {assessment.should_process}\")\n",
    "    print(f\"   📈 Priority Score: {assessment.priority_score}/100\")\n",
    "    \n",
    "    print(f\"\\n🔍 Quality Breakdown:\")\n",
    "    for factor, score in assessment.confidence_factors.items():\n",
    "        print(f\"   📊 {factor.replace('_', ' ').title()}: {score:.2f}\")\n",
    "    \n",
    "    print(f\"\\n💡 Recommendations:\")\n",
    "    for i, rec in enumerate(assessment.recommendations, 1):\n",
    "        print(f\"   {i}. {rec}\")\n",
    "    \n",
    "    print(f\"\\n✅ Document Quality Assessor working perfectly!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No PDF data available for assessment\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9636273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d979ee59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bc5487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc0791a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d1e9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f485d955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f99cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028ca29f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339454ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f10dc81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d2003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7f597a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd2ba1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84304752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722bb54c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecaadba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc2c44e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7533629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8095151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4e4e75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585f513a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61442898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a090e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf2c0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1708f84d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc95a567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b85bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
